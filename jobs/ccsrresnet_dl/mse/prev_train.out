Tue Jun 30 16:25:35 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:1A:00.0 Off |                  Off |
| 33%   33C    P8    15W / 260W |      1MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Quadro RTX 6000     On   | 00000000:1B:00.0 Off |                  Off |
| 33%   35C    P8     4W / 260W |      1MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Quadro RTX 6000     On   | 00000000:60:00.0 Off |                  Off |
| 33%   38C    P8    10W / 260W |      1MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Quadro RTX 6000     On   | 00000000:61:00.0 Off |                  Off |
| 33%   48C    P2    86W / 260W |  11912MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    3   N/A  N/A     56463      C   python                          11623MiB |
+-----------------------------------------------------------------------------+
Running python script now
Beginning training for CCSRResNet model...
Using the following hyperparemters:
Data:                 /data/pacole2/DeepLesionPreprocessed/miniStudies/
Dataset:              DeepLesion
Upscale:              1
Learning rate:        0.0001
Number of epochs:     300
Prints per epoch:     10
Start decay:          250
Batch size:           32
Content loss:         mse
CLambda:              1.0
WLambda:              0.001
Check Sample:         True
Checkpoint directory: checkpoints/ccsrresnet_dl/mse/
Load:                 True
Prefix:               last
Cuda:                 2

GPU devices being used:  [0, 1]
Epoch [179 / 300]
===> Epoch [179 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0354, D Fake Loss: 94.1011, D Real Loss: 94.7175, Wasserstein Loss: -94.0709, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [179 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0357, D Fake Loss: 94.5285, D Real Loss: 95.1391, Wasserstein Loss: -94.4869, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [179 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 94.4333, D Real Loss: 95.0526, Wasserstein Loss: -94.3974, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [179 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 94.4303, D Real Loss: 95.0482, Wasserstein Loss: -94.3837, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [179 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 94.2780, D Real Loss: 94.8950, Wasserstein Loss: -94.2354, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [179 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 94.0450, D Real Loss: 94.6607, Wasserstein Loss: -94.0074, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [179 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 94.0792, D Real Loss: 94.6941, Wasserstein Loss: -94.0458, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [179 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 93.9812, D Real Loss: 94.5958, Wasserstein Loss: -93.9431, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [179 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 93.9679, D Real Loss: 94.5832, Wasserstein Loss: -93.9351, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [179 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 93.9459, D Real Loss: 94.5609, Wasserstein Loss: -93.9147, Content Loss: 0.0002, PSNR: 37.85
Epoch [179 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 93.9504, D Real Loss: 94.5658, Wasserstein Loss: -93.9207, Content Loss: 0.0002, PSNR: 37.85
Epoch [180 / 300]
===> Epoch [180 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0418, D Fake Loss: 95.5376, D Real Loss: 96.1798, Wasserstein Loss: -95.4803, Content Loss: 0.0002, PSNR: 37.61
===> Epoch [180 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0403, D Fake Loss: 95.1721, D Real Loss: 95.8033, Wasserstein Loss: -95.1278, Content Loss: 0.0002, PSNR: 37.68
===> Epoch [180 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 94.9236, D Real Loss: 95.5578, Wasserstein Loss: -94.8990, Content Loss: 0.0002, PSNR: 37.73
===> Epoch [180 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 94.8640, D Real Loss: 95.4933, Wasserstein Loss: -94.8442, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [180 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 94.6652, D Real Loss: 95.2911, Wasserstein Loss: -94.6291, Content Loss: 0.0002, PSNR: 37.81
===> Epoch [180 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 94.4245, D Real Loss: 95.0465, Wasserstein Loss: -94.3931, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [180 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 94.3126, D Real Loss: 94.9269, Wasserstein Loss: -94.2811, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [180 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 94.3907, D Real Loss: 95.0028, Wasserstein Loss: -94.3663, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [180 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 94.3725, D Real Loss: 94.9839, Wasserstein Loss: -94.3464, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [180 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 94.4512, D Real Loss: 95.0612, Wasserstein Loss: -94.4246, Content Loss: 0.0002, PSNR: 37.92
Epoch [180 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 94.4423, D Real Loss: 95.0517, Wasserstein Loss: -94.4159, Content Loss: 0.0002, PSNR: 37.92
Epoch [181 / 300]
===> Epoch [181 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 93.8569, D Real Loss: 94.4894, Wasserstein Loss: -93.8290, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [181 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 94.3832, D Real Loss: 95.0252, Wasserstein Loss: -94.3457, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [181 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 94.6818, D Real Loss: 95.3206, Wasserstein Loss: -94.6572, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [181 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 94.5440, D Real Loss: 95.1778, Wasserstein Loss: -94.5068, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [181 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 94.3107, D Real Loss: 94.9340, Wasserstein Loss: -94.2820, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [181 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 94.2807, D Real Loss: 94.9006, Wasserstein Loss: -94.2559, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [181 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 94.2641, D Real Loss: 94.8854, Wasserstein Loss: -94.2376, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [181 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 94.2402, D Real Loss: 94.8609, Wasserstein Loss: -94.2178, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [181 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 94.1507, D Real Loss: 94.7712, Wasserstein Loss: -94.1270, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [181 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 94.1316, D Real Loss: 94.7484, Wasserstein Loss: -94.1067, Content Loss: 0.0002, PSNR: 37.84
Epoch [181 / 300]: Gradient Penalty: 0.0377, D Fake Loss: 94.1324, D Real Loss: 94.7497, Wasserstein Loss: -94.1079, Content Loss: 0.0002, PSNR: 37.84
Epoch [182 / 300]
===> Epoch [182 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 94.0366, D Real Loss: 94.6655, Wasserstein Loss: -94.0629, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [182 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 94.0152, D Real Loss: 94.6466, Wasserstein Loss: -93.9788, Content Loss: 0.0002, PSNR: 37.69
===> Epoch [182 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 94.5209, D Real Loss: 95.1521, Wasserstein Loss: -94.4888, Content Loss: 0.0002, PSNR: 37.53
===> Epoch [182 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0412, D Fake Loss: 94.2774, D Real Loss: 94.9118, Wasserstein Loss: -94.2565, Content Loss: 0.0002, PSNR: 37.53
===> Epoch [182 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 94.7980, D Real Loss: 95.4313, Wasserstein Loss: -94.7697, Content Loss: 0.0002, PSNR: 37.58
===> Epoch [182 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 94.6810, D Real Loss: 95.3163, Wasserstein Loss: -94.6515, Content Loss: 0.0002, PSNR: 37.59
===> Epoch [182 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 94.7603, D Real Loss: 95.3919, Wasserstein Loss: -94.7277, Content Loss: 0.0002, PSNR: 37.63
===> Epoch [182 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 94.7429, D Real Loss: 95.3731, Wasserstein Loss: -94.7100, Content Loss: 0.0002, PSNR: 37.66
===> Epoch [182 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 94.8374, D Real Loss: 95.4644, Wasserstein Loss: -94.8089, Content Loss: 0.0002, PSNR: 37.72
===> Epoch [182 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 94.7454, D Real Loss: 95.3692, Wasserstein Loss: -94.7165, Content Loss: 0.0002, PSNR: 37.73
Epoch [182 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 94.7648, D Real Loss: 95.3886, Wasserstein Loss: -94.7386, Content Loss: 0.0002, PSNR: 37.73
Epoch [183 / 300]
===> Epoch [183 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0430, D Fake Loss: 94.8563, D Real Loss: 95.5226, Wasserstein Loss: -94.8261, Content Loss: 0.0002, PSNR: 37.51
===> Epoch [183 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 95.3737, D Real Loss: 96.0163, Wasserstein Loss: -95.3132, Content Loss: 0.0002, PSNR: 37.71
===> Epoch [183 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 95.3307, D Real Loss: 95.9648, Wasserstein Loss: -95.2816, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [183 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 95.4882, D Real Loss: 96.1261, Wasserstein Loss: -95.4385, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [183 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 95.2366, D Real Loss: 95.8692, Wasserstein Loss: -95.2034, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [183 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 95.0450, D Real Loss: 95.6769, Wasserstein Loss: -95.0113, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [183 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 94.9121, D Real Loss: 95.5409, Wasserstein Loss: -94.8791, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [183 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 94.9137, D Real Loss: 95.5430, Wasserstein Loss: -94.8820, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [183 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 94.8213, D Real Loss: 95.4443, Wasserstein Loss: -94.7923, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [183 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 94.7828, D Real Loss: 95.4065, Wasserstein Loss: -94.7523, Content Loss: 0.0002, PSNR: 37.86
Epoch [183 / 300]: Gradient Penalty: 0.0389, D Fake Loss: 94.7705, D Real Loss: 95.3938, Wasserstein Loss: -94.7388, Content Loss: 0.0002, PSNR: 37.86
Epoch [184 / 300]
===> Epoch [184 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 93.9819, D Real Loss: 94.6119, Wasserstein Loss: -93.9471, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [184 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 94.3439, D Real Loss: 94.9637, Wasserstein Loss: -94.3185, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [184 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 95.0904, D Real Loss: 95.7136, Wasserstein Loss: -95.0799, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [184 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 95.0276, D Real Loss: 95.6550, Wasserstein Loss: -94.9979, Content Loss: 0.0002, PSNR: 37.76
===> Epoch [184 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 94.6842, D Real Loss: 95.3081, Wasserstein Loss: -94.6576, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [184 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 94.7661, D Real Loss: 95.3858, Wasserstein Loss: -94.7345, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [184 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 94.8840, D Real Loss: 95.5052, Wasserstein Loss: -94.8637, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [184 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 95.0412, D Real Loss: 95.6604, Wasserstein Loss: -95.0205, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [184 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 94.9816, D Real Loss: 95.5971, Wasserstein Loss: -94.9561, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [184 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 95.0619, D Real Loss: 95.6801, Wasserstein Loss: -95.0381, Content Loss: 0.0002, PSNR: 37.85
Epoch [184 / 300]: Gradient Penalty: 0.0382, D Fake Loss: 95.0608, D Real Loss: 95.6786, Wasserstein Loss: -95.0377, Content Loss: 0.0002, PSNR: 37.85
Epoch [185 / 300]
===> Epoch [185 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 93.9706, D Real Loss: 94.5908, Wasserstein Loss: -93.9538, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [185 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 94.8905, D Real Loss: 95.5128, Wasserstein Loss: -94.8769, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [185 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 95.1268, D Real Loss: 95.7506, Wasserstein Loss: -95.0833, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [185 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 94.9818, D Real Loss: 95.5988, Wasserstein Loss: -94.9513, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [185 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 94.5485, D Real Loss: 95.1784, Wasserstein Loss: -94.5269, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [185 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 94.7395, D Real Loss: 95.3684, Wasserstein Loss: -94.7124, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [185 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 94.8447, D Real Loss: 95.4772, Wasserstein Loss: -94.8148, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [185 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 94.8165, D Real Loss: 95.4466, Wasserstein Loss: -94.7848, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [185 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 94.7631, D Real Loss: 95.3870, Wasserstein Loss: -94.7351, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [185 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 94.8059, D Real Loss: 95.4280, Wasserstein Loss: -94.7820, Content Loss: 0.0002, PSNR: 37.80
Epoch [185 / 300]: Gradient Penalty: 0.0392, D Fake Loss: 94.8445, D Real Loss: 95.4664, Wasserstein Loss: -94.8195, Content Loss: 0.0002, PSNR: 37.80
Epoch [186 / 300]
===> Epoch [186 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 94.8896, D Real Loss: 95.5092, Wasserstein Loss: -94.8406, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [186 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 94.6427, D Real Loss: 95.2485, Wasserstein Loss: -94.5951, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [186 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 94.9681, D Real Loss: 95.5864, Wasserstein Loss: -94.9296, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [186 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 94.8625, D Real Loss: 95.4756, Wasserstein Loss: -94.8307, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [186 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 94.8067, D Real Loss: 95.4180, Wasserstein Loss: -94.7641, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [186 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 94.8673, D Real Loss: 95.4765, Wasserstein Loss: -94.8296, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [186 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0360, D Fake Loss: 94.6873, D Real Loss: 95.2935, Wasserstein Loss: -94.6589, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [186 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 94.7733, D Real Loss: 95.3855, Wasserstein Loss: -94.7403, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [186 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 94.8966, D Real Loss: 95.5140, Wasserstein Loss: -94.8694, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [186 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 94.8147, D Real Loss: 95.4309, Wasserstein Loss: -94.7854, Content Loss: 0.0002, PSNR: 37.85
Epoch [186 / 300]: Gradient Penalty: 0.0382, D Fake Loss: 94.8214, D Real Loss: 95.4377, Wasserstein Loss: -94.7913, Content Loss: 0.0002, PSNR: 37.86
Epoch [187 / 300]
===> Epoch [187 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 94.3118, D Real Loss: 94.9189, Wasserstein Loss: -94.2873, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [187 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 95.2808, D Real Loss: 95.9000, Wasserstein Loss: -95.2609, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [187 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 95.7846, D Real Loss: 96.4220, Wasserstein Loss: -95.7599, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [187 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 95.2626, D Real Loss: 95.8956, Wasserstein Loss: -95.2217, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [187 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 95.2581, D Real Loss: 95.8862, Wasserstein Loss: -95.2334, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [187 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 95.0906, D Real Loss: 95.7147, Wasserstein Loss: -95.0602, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [187 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 95.2657, D Real Loss: 95.8891, Wasserstein Loss: -95.2347, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [187 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 95.1600, D Real Loss: 95.7846, Wasserstein Loss: -95.1312, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [187 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 94.9244, D Real Loss: 95.5491, Wasserstein Loss: -94.8997, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [187 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 95.0398, D Real Loss: 95.6620, Wasserstein Loss: -95.0173, Content Loss: 0.0002, PSNR: 37.86
Epoch [187 / 300]: Gradient Penalty: 0.0392, D Fake Loss: 95.0711, D Real Loss: 95.6932, Wasserstein Loss: -95.0504, Content Loss: 0.0002, PSNR: 37.87
Epoch [188 / 300]
===> Epoch [188 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 97.0079, D Real Loss: 97.6295, Wasserstein Loss: -96.9295, Content Loss: 0.0002, PSNR: 37.81
===> Epoch [188 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 96.3687, D Real Loss: 96.9827, Wasserstein Loss: -96.3383, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [188 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 96.4538, D Real Loss: 97.0657, Wasserstein Loss: -96.4272, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [188 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 96.1526, D Real Loss: 96.7641, Wasserstein Loss: -96.1240, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [188 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 96.3391, D Real Loss: 96.9483, Wasserstein Loss: -96.3031, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [188 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 96.1981, D Real Loss: 96.8141, Wasserstein Loss: -96.1714, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [188 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 96.1539, D Real Loss: 96.7766, Wasserstein Loss: -96.1204, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [188 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 96.0682, D Real Loss: 96.6857, Wasserstein Loss: -96.0394, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [188 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 95.9347, D Real Loss: 96.5528, Wasserstein Loss: -95.9048, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [188 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 95.8620, D Real Loss: 96.4788, Wasserstein Loss: -95.8326, Content Loss: 0.0002, PSNR: 37.90
Epoch [188 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 95.8607, D Real Loss: 96.4772, Wasserstein Loss: -95.8307, Content Loss: 0.0002, PSNR: 37.90
Epoch [189 / 300]
===> Epoch [189 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 94.9186, D Real Loss: 95.5353, Wasserstein Loss: -94.9175, Content Loss: 0.0002, PSNR: 37.58
===> Epoch [189 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 95.3813, D Real Loss: 96.0006, Wasserstein Loss: -95.3351, Content Loss: 0.0002, PSNR: 37.67
===> Epoch [189 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 95.8551, D Real Loss: 96.4847, Wasserstein Loss: -95.8156, Content Loss: 0.0002, PSNR: 37.76
===> Epoch [189 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 96.1006, D Real Loss: 96.7282, Wasserstein Loss: -96.0728, Content Loss: 0.0002, PSNR: 37.73
===> Epoch [189 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 96.1717, D Real Loss: 96.8071, Wasserstein Loss: -96.1409, Content Loss: 0.0002, PSNR: 37.72
===> Epoch [189 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 96.1211, D Real Loss: 96.7505, Wasserstein Loss: -96.0897, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [189 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 96.3317, D Real Loss: 96.9620, Wasserstein Loss: -96.3041, Content Loss: 0.0002, PSNR: 37.73
===> Epoch [189 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 96.1258, D Real Loss: 96.7533, Wasserstein Loss: -96.0985, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [189 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 96.1291, D Real Loss: 96.7571, Wasserstein Loss: -96.1020, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [189 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 96.1880, D Real Loss: 96.8148, Wasserstein Loss: -96.1614, Content Loss: 0.0002, PSNR: 37.82
Epoch [189 / 300]: Gradient Penalty: 0.0400, D Fake Loss: 96.1939, D Real Loss: 96.8208, Wasserstein Loss: -96.1674, Content Loss: 0.0002, PSNR: 37.82
Epoch [190 / 300]
===> Epoch [190 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 96.4914, D Real Loss: 97.1396, Wasserstein Loss: -96.4525, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [190 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 96.5004, D Real Loss: 97.1393, Wasserstein Loss: -96.4537, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [190 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 96.1070, D Real Loss: 96.7385, Wasserstein Loss: -96.0823, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [190 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.0472, D Real Loss: 96.6776, Wasserstein Loss: -96.0224, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [190 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.3903, D Real Loss: 97.0184, Wasserstein Loss: -96.3672, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [190 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 96.3408, D Real Loss: 96.9682, Wasserstein Loss: -96.3249, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [190 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 96.1280, D Real Loss: 96.7506, Wasserstein Loss: -96.1064, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [190 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 96.1033, D Real Loss: 96.7230, Wasserstein Loss: -96.0769, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [190 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 96.0006, D Real Loss: 96.6198, Wasserstein Loss: -95.9810, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [190 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 95.8271, D Real Loss: 96.4459, Wasserstein Loss: -95.8012, Content Loss: 0.0002, PSNR: 37.95
Epoch [190 / 300]: Gradient Penalty: 0.0378, D Fake Loss: 95.8234, D Real Loss: 96.4421, Wasserstein Loss: -95.7998, Content Loss: 0.0002, PSNR: 37.96
Epoch [191 / 300]
===> Epoch [191 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 96.4835, D Real Loss: 97.1423, Wasserstein Loss: -96.4599, Content Loss: 0.0002, PSNR: 37.73
===> Epoch [191 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 96.6762, D Real Loss: 97.3122, Wasserstein Loss: -96.6456, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [191 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 96.2648, D Real Loss: 96.8943, Wasserstein Loss: -96.2289, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [191 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 96.1218, D Real Loss: 96.7521, Wasserstein Loss: -96.0890, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [191 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 95.9862, D Real Loss: 96.6160, Wasserstein Loss: -95.9511, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [191 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 96.1457, D Real Loss: 96.7769, Wasserstein Loss: -96.1137, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [191 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 96.2805, D Real Loss: 96.9141, Wasserstein Loss: -96.2539, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [191 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 96.0731, D Real Loss: 96.7058, Wasserstein Loss: -96.0518, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [191 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.0434, D Real Loss: 96.6737, Wasserstein Loss: -96.0145, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [191 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 96.1861, D Real Loss: 96.8146, Wasserstein Loss: -96.1584, Content Loss: 0.0002, PSNR: 37.93
Epoch [191 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 96.1648, D Real Loss: 96.7929, Wasserstein Loss: -96.1333, Content Loss: 0.0002, PSNR: 37.92
Epoch [192 / 300]
===> Epoch [192 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0342, D Fake Loss: 93.1278, D Real Loss: 93.7332, Wasserstein Loss: -93.1731, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [192 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 95.4383, D Real Loss: 96.0696, Wasserstein Loss: -95.4352, Content Loss: 0.0002, PSNR: 37.81
===> Epoch [192 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 95.0108, D Real Loss: 95.6414, Wasserstein Loss: -95.0002, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [192 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 95.7603, D Real Loss: 96.3895, Wasserstein Loss: -95.7466, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [192 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 95.8228, D Real Loss: 96.4447, Wasserstein Loss: -95.7998, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [192 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 95.8010, D Real Loss: 96.4265, Wasserstein Loss: -95.7810, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [192 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 95.8971, D Real Loss: 96.5239, Wasserstein Loss: -95.8720, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [192 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 95.8768, D Real Loss: 96.5016, Wasserstein Loss: -95.8559, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [192 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 96.0398, D Real Loss: 96.6640, Wasserstein Loss: -96.0182, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [192 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 96.1343, D Real Loss: 96.7547, Wasserstein Loss: -96.1088, Content Loss: 0.0002, PSNR: 37.85
Epoch [192 / 300]: Gradient Penalty: 0.0384, D Fake Loss: 96.1142, D Real Loss: 96.7349, Wasserstein Loss: -96.0904, Content Loss: 0.0002, PSNR: 37.85
Epoch [193 / 300]
===> Epoch [193 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0424, D Fake Loss: 97.8843, D Real Loss: 98.5578, Wasserstein Loss: -97.8064, Content Loss: 0.0002, PSNR: 37.40
===> Epoch [193 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0425, D Fake Loss: 96.4639, D Real Loss: 97.1106, Wasserstein Loss: -96.4140, Content Loss: 0.0002, PSNR: 37.64
===> Epoch [193 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 96.4661, D Real Loss: 97.1050, Wasserstein Loss: -96.4253, Content Loss: 0.0002, PSNR: 37.68
===> Epoch [193 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0412, D Fake Loss: 95.9790, D Real Loss: 96.6176, Wasserstein Loss: -95.9415, Content Loss: 0.0002, PSNR: 37.66
===> Epoch [193 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 96.1365, D Real Loss: 96.7682, Wasserstein Loss: -96.1045, Content Loss: 0.0002, PSNR: 37.69
===> Epoch [193 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 96.2273, D Real Loss: 96.8510, Wasserstein Loss: -96.1920, Content Loss: 0.0002, PSNR: 37.72
===> Epoch [193 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 96.2702, D Real Loss: 96.8924, Wasserstein Loss: -96.2398, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [193 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 96.2480, D Real Loss: 96.8674, Wasserstein Loss: -96.2179, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [193 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 96.3694, D Real Loss: 96.9870, Wasserstein Loss: -96.3429, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [193 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 96.4379, D Real Loss: 97.0586, Wasserstein Loss: -96.4110, Content Loss: 0.0002, PSNR: 37.81
Epoch [193 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 96.4378, D Real Loss: 97.0578, Wasserstein Loss: -96.4092, Content Loss: 0.0002, PSNR: 37.81
Epoch [194 / 300]
===> Epoch [194 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 95.5870, D Real Loss: 96.2099, Wasserstein Loss: -95.5791, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [194 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 96.7265, D Real Loss: 97.3443, Wasserstein Loss: -96.7135, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [194 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 96.5488, D Real Loss: 97.1633, Wasserstein Loss: -96.5248, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [194 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 97.1400, D Real Loss: 97.7564, Wasserstein Loss: -97.1180, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [194 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 96.9917, D Real Loss: 97.6094, Wasserstein Loss: -96.9698, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [194 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 97.1879, D Real Loss: 97.8079, Wasserstein Loss: -97.1696, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [194 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 96.9829, D Real Loss: 97.6024, Wasserstein Loss: -96.9566, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [194 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 96.9329, D Real Loss: 97.5488, Wasserstein Loss: -96.9052, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [194 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 96.8192, D Real Loss: 97.4336, Wasserstein Loss: -96.7940, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [194 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 96.7320, D Real Loss: 97.3457, Wasserstein Loss: -96.7083, Content Loss: 0.0002, PSNR: 38.00
Epoch [194 / 300]: Gradient Penalty: 0.0371, D Fake Loss: 96.7191, D Real Loss: 97.3324, Wasserstein Loss: -96.6941, Content Loss: 0.0002, PSNR: 38.00
Epoch [195 / 300]
===> Epoch [195 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 95.8262, D Real Loss: 96.4359, Wasserstein Loss: -95.7734, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [195 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 96.6269, D Real Loss: 97.2577, Wasserstein Loss: -96.5893, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [195 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 96.1067, D Real Loss: 96.7359, Wasserstein Loss: -96.0877, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [195 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 96.2600, D Real Loss: 96.8889, Wasserstein Loss: -96.2414, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [195 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 96.0051, D Real Loss: 96.6276, Wasserstein Loss: -95.9852, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [195 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 95.8893, D Real Loss: 96.5132, Wasserstein Loss: -95.8646, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [195 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 95.8596, D Real Loss: 96.4824, Wasserstein Loss: -95.8386, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [195 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 95.8336, D Real Loss: 96.4566, Wasserstein Loss: -95.8026, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [195 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 95.8852, D Real Loss: 96.5119, Wasserstein Loss: -95.8565, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [195 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 95.9474, D Real Loss: 96.5730, Wasserstein Loss: -95.9234, Content Loss: 0.0002, PSNR: 37.80
Epoch [195 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 95.9769, D Real Loss: 96.6019, Wasserstein Loss: -95.9528, Content Loss: 0.0002, PSNR: 37.80
Epoch [196 / 300]
===> Epoch [196 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 97.1374, D Real Loss: 97.7678, Wasserstein Loss: -97.0325, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [196 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 96.7511, D Real Loss: 97.3740, Wasserstein Loss: -96.7032, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [196 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.3962, D Real Loss: 98.0298, Wasserstein Loss: -97.3612, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [196 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.2708, D Real Loss: 97.9020, Wasserstein Loss: -97.2310, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [196 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.0371, D Real Loss: 97.6641, Wasserstein Loss: -97.0098, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [196 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.0128, D Real Loss: 97.6369, Wasserstein Loss: -96.9767, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [196 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.1530, D Real Loss: 97.7800, Wasserstein Loss: -97.1184, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [196 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 96.9302, D Real Loss: 97.5545, Wasserstein Loss: -96.8917, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [196 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 96.7528, D Real Loss: 97.3730, Wasserstein Loss: -96.7178, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [196 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 96.8314, D Real Loss: 97.4574, Wasserstein Loss: -96.7967, Content Loss: 0.0002, PSNR: 37.88
Epoch [196 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 96.8011, D Real Loss: 97.4272, Wasserstein Loss: -96.7689, Content Loss: 0.0002, PSNR: 37.88
Epoch [197 / 300]
===> Epoch [197 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 97.2264, D Real Loss: 97.8753, Wasserstein Loss: -97.2156, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [197 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.0488, D Real Loss: 97.6797, Wasserstein Loss: -97.0139, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [197 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 97.1541, D Real Loss: 97.7892, Wasserstein Loss: -97.1273, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [197 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.2900, D Real Loss: 97.9218, Wasserstein Loss: -97.2734, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [197 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.0839, D Real Loss: 97.7120, Wasserstein Loss: -97.0566, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [197 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 97.0387, D Real Loss: 97.6628, Wasserstein Loss: -97.0037, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [197 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 96.7980, D Real Loss: 97.4166, Wasserstein Loss: -96.7787, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [197 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 96.7888, D Real Loss: 97.4100, Wasserstein Loss: -96.7647, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [197 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 96.7269, D Real Loss: 97.3466, Wasserstein Loss: -96.7003, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [197 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 96.6065, D Real Loss: 97.2258, Wasserstein Loss: -96.5784, Content Loss: 0.0002, PSNR: 37.94
Epoch [197 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 96.5755, D Real Loss: 97.1948, Wasserstein Loss: -96.5498, Content Loss: 0.0002, PSNR: 37.94
Epoch [198 / 300]
===> Epoch [198 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 97.3704, D Real Loss: 98.0114, Wasserstein Loss: -97.4270, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [198 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.7901, D Real Loss: 98.4200, Wasserstein Loss: -97.7628, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [198 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.2187, D Real Loss: 97.8488, Wasserstein Loss: -97.2071, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [198 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 97.0823, D Real Loss: 97.7142, Wasserstein Loss: -97.0704, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [198 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 96.8846, D Real Loss: 97.5145, Wasserstein Loss: -96.8606, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [198 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 96.8686, D Real Loss: 97.4929, Wasserstein Loss: -96.8392, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [198 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 96.9478, D Real Loss: 97.5718, Wasserstein Loss: -96.9248, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [198 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 96.7424, D Real Loss: 97.3634, Wasserstein Loss: -96.7143, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [198 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 96.6205, D Real Loss: 97.2449, Wasserstein Loss: -96.5995, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [198 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 96.6734, D Real Loss: 97.2983, Wasserstein Loss: -96.6620, Content Loss: 0.0002, PSNR: 37.85
Epoch [198 / 300]: Gradient Penalty: 0.0387, D Fake Loss: 96.7772, D Real Loss: 97.4015, Wasserstein Loss: -96.7621, Content Loss: 0.0002, PSNR: 37.84
Epoch [199 / 300]
===> Epoch [199 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0429, D Fake Loss: 95.9035, D Real Loss: 96.5540, Wasserstein Loss: -95.7478, Content Loss: 0.0002, PSNR: 37.34
===> Epoch [199 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0420, D Fake Loss: 96.7285, D Real Loss: 97.3723, Wasserstein Loss: -96.6355, Content Loss: 0.0002, PSNR: 37.51
===> Epoch [199 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0414, D Fake Loss: 96.8436, D Real Loss: 97.4740, Wasserstein Loss: -96.7756, Content Loss: 0.0002, PSNR: 37.67
===> Epoch [199 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 97.0633, D Real Loss: 97.6875, Wasserstein Loss: -97.0061, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [199 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0405, D Fake Loss: 97.2760, D Real Loss: 97.9064, Wasserstein Loss: -97.2315, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [199 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 97.3231, D Real Loss: 97.9524, Wasserstein Loss: -97.2841, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [199 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.2557, D Real Loss: 97.8844, Wasserstein Loss: -97.2110, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [199 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.3189, D Real Loss: 97.9441, Wasserstein Loss: -97.2785, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [199 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 97.2791, D Real Loss: 97.9042, Wasserstein Loss: -97.2391, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [199 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 97.1102, D Real Loss: 97.7327, Wasserstein Loss: -97.0709, Content Loss: 0.0002, PSNR: 37.86
Epoch [199 / 300]: Gradient Penalty: 0.0395, D Fake Loss: 97.1028, D Real Loss: 97.7257, Wasserstein Loss: -97.0622, Content Loss: 0.0002, PSNR: 37.86
Epoch [200 / 300]
===> Epoch [200 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 97.4127, D Real Loss: 98.0417, Wasserstein Loss: -97.4035, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [200 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 97.5426, D Real Loss: 98.1663, Wasserstein Loss: -97.5348, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [200 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.5518, D Real Loss: 98.1789, Wasserstein Loss: -97.5363, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [200 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 97.4488, D Real Loss: 98.0721, Wasserstein Loss: -97.4376, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [200 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 97.5575, D Real Loss: 98.1818, Wasserstein Loss: -97.5373, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [200 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 97.3879, D Real Loss: 98.0111, Wasserstein Loss: -97.3726, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [200 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 97.3396, D Real Loss: 97.9575, Wasserstein Loss: -97.3203, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [200 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 97.2811, D Real Loss: 97.8982, Wasserstein Loss: -97.2618, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [200 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 97.4417, D Real Loss: 98.0571, Wasserstein Loss: -97.4234, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [200 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 97.2913, D Real Loss: 97.9058, Wasserstein Loss: -97.2684, Content Loss: 0.0002, PSNR: 37.98
Epoch [200 / 300]: Gradient Penalty: 0.0379, D Fake Loss: 97.2782, D Real Loss: 97.8930, Wasserstein Loss: -97.2548, Content Loss: 0.0002, PSNR: 37.98
Epoch [201 / 300]
===> Epoch [201 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 97.5491, D Real Loss: 98.1943, Wasserstein Loss: -97.5923, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [201 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.2917, D Real Loss: 97.9246, Wasserstein Loss: -97.2906, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [201 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 97.1933, D Real Loss: 97.8251, Wasserstein Loss: -97.1715, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [201 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 97.2169, D Real Loss: 97.8476, Wasserstein Loss: -97.2007, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [201 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.2724, D Real Loss: 97.8996, Wasserstein Loss: -97.2561, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [201 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 97.3482, D Real Loss: 97.9729, Wasserstein Loss: -97.3247, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [201 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 97.2383, D Real Loss: 97.8608, Wasserstein Loss: -97.2139, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [201 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 97.1045, D Real Loss: 97.7283, Wasserstein Loss: -97.0792, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [201 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 97.0431, D Real Loss: 97.6649, Wasserstein Loss: -97.0238, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [201 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 97.1137, D Real Loss: 97.7358, Wasserstein Loss: -97.0920, Content Loss: 0.0002, PSNR: 37.98
Epoch [201 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 97.1179, D Real Loss: 97.7398, Wasserstein Loss: -97.0948, Content Loss: 0.0002, PSNR: 37.99
Epoch [202 / 300]
===> Epoch [202 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0416, D Fake Loss: 97.2048, D Real Loss: 97.8693, Wasserstein Loss: -97.1677, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [202 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 97.1442, D Real Loss: 97.7851, Wasserstein Loss: -97.1111, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [202 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.3957, D Real Loss: 98.0293, Wasserstein Loss: -97.3693, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [202 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.9333, D Real Loss: 97.5628, Wasserstein Loss: -96.9144, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [202 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.1205, D Real Loss: 97.7496, Wasserstein Loss: -97.0914, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [202 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.1972, D Real Loss: 97.8240, Wasserstein Loss: -97.1771, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [202 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.1973, D Real Loss: 97.8239, Wasserstein Loss: -97.1783, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [202 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 97.2715, D Real Loss: 97.8985, Wasserstein Loss: -97.2500, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [202 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 97.2957, D Real Loss: 97.9231, Wasserstein Loss: -97.2745, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [202 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 97.4357, D Real Loss: 98.0606, Wasserstein Loss: -97.4135, Content Loss: 0.0002, PSNR: 37.96
Epoch [202 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 97.4497, D Real Loss: 98.0742, Wasserstein Loss: -97.4268, Content Loss: 0.0002, PSNR: 37.96
Epoch [203 / 300]
===> Epoch [203 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.1257, D Real Loss: 97.7703, Wasserstein Loss: -97.0511, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [203 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.9207, D Real Loss: 97.5464, Wasserstein Loss: -96.8730, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [203 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 96.5329, D Real Loss: 97.1637, Wasserstein Loss: -96.4999, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [203 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 97.0065, D Real Loss: 97.6370, Wasserstein Loss: -96.9748, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [203 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 96.9644, D Real Loss: 97.5925, Wasserstein Loss: -96.9369, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [203 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 97.1514, D Real Loss: 97.7751, Wasserstein Loss: -97.1283, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [203 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 97.1552, D Real Loss: 97.7811, Wasserstein Loss: -97.1180, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [203 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 96.9772, D Real Loss: 97.6015, Wasserstein Loss: -96.9462, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [203 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 96.9157, D Real Loss: 97.5395, Wasserstein Loss: -96.8863, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [203 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 96.8770, D Real Loss: 97.4977, Wasserstein Loss: -96.8483, Content Loss: 0.0002, PSNR: 37.91
Epoch [203 / 300]: Gradient Penalty: 0.0386, D Fake Loss: 96.8788, D Real Loss: 97.4991, Wasserstein Loss: -96.8517, Content Loss: 0.0002, PSNR: 37.92
Epoch [204 / 300]
===> Epoch [204 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 98.3224, D Real Loss: 98.9494, Wasserstein Loss: -98.3228, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [204 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.4139, D Real Loss: 99.0480, Wasserstein Loss: -98.3779, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [204 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.9474, D Real Loss: 98.5862, Wasserstein Loss: -97.9171, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [204 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 97.6797, D Real Loss: 98.3157, Wasserstein Loss: -97.6504, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [204 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.0297, D Real Loss: 98.6669, Wasserstein Loss: -98.0078, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [204 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.8825, D Real Loss: 98.5212, Wasserstein Loss: -97.8550, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [204 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.8373, D Real Loss: 98.4712, Wasserstein Loss: -97.8124, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [204 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.8071, D Real Loss: 98.4405, Wasserstein Loss: -97.7799, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [204 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 97.5477, D Real Loss: 98.1772, Wasserstein Loss: -97.5247, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [204 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.4438, D Real Loss: 98.0682, Wasserstein Loss: -97.4221, Content Loss: 0.0002, PSNR: 37.99
Epoch [204 / 300]: Gradient Penalty: 0.0384, D Fake Loss: 97.4759, D Real Loss: 98.0999, Wasserstein Loss: -97.4569, Content Loss: 0.0002, PSNR: 37.99
Epoch [205 / 300]
===> Epoch [205 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 97.0427, D Real Loss: 97.6965, Wasserstein Loss: -97.0074, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [205 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 96.9732, D Real Loss: 97.6248, Wasserstein Loss: -96.9223, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [205 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 96.6008, D Real Loss: 97.2501, Wasserstein Loss: -96.5656, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [205 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0409, D Fake Loss: 96.7282, D Real Loss: 97.3719, Wasserstein Loss: -96.6930, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [205 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.1336, D Real Loss: 97.7699, Wasserstein Loss: -97.1030, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [205 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.3674, D Real Loss: 98.0008, Wasserstein Loss: -97.3444, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [205 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 97.3882, D Real Loss: 98.0199, Wasserstein Loss: -97.3541, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [205 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.2179, D Real Loss: 97.8448, Wasserstein Loss: -97.1823, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [205 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.1696, D Real Loss: 97.7964, Wasserstein Loss: -97.1382, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [205 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.0357, D Real Loss: 97.6611, Wasserstein Loss: -97.0045, Content Loss: 0.0002, PSNR: 37.96
Epoch [205 / 300]: Gradient Penalty: 0.0385, D Fake Loss: 97.0518, D Real Loss: 97.6773, Wasserstein Loss: -97.0244, Content Loss: 0.0002, PSNR: 37.96
Epoch [206 / 300]
===> Epoch [206 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0417, D Fake Loss: 99.0982, D Real Loss: 99.7490, Wasserstein Loss: -99.0917, Content Loss: 0.0002, PSNR: 37.77
===> Epoch [206 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 98.6735, D Real Loss: 99.3126, Wasserstein Loss: -98.6532, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [206 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.8980, D Real Loss: 98.5317, Wasserstein Loss: -97.8432, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [206 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.9289, D Real Loss: 98.5600, Wasserstein Loss: -97.8993, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [206 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 98.1244, D Real Loss: 98.7618, Wasserstein Loss: -98.0901, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [206 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 98.0011, D Real Loss: 98.6340, Wasserstein Loss: -97.9707, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [206 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 98.0240, D Real Loss: 98.6582, Wasserstein Loss: -97.9980, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [206 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 97.9507, D Real Loss: 98.5856, Wasserstein Loss: -97.9237, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [206 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 97.7084, D Real Loss: 98.3409, Wasserstein Loss: -97.6798, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [206 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.5296, D Real Loss: 98.1585, Wasserstein Loss: -97.5040, Content Loss: 0.0002, PSNR: 37.98
Epoch [206 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 97.5514, D Real Loss: 98.1795, Wasserstein Loss: -97.5257, Content Loss: 0.0002, PSNR: 37.99
Epoch [207 / 300]
===> Epoch [207 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 97.2944, D Real Loss: 97.9225, Wasserstein Loss: -97.2467, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [207 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 97.1934, D Real Loss: 97.8108, Wasserstein Loss: -97.1459, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [207 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.2275, D Real Loss: 97.8539, Wasserstein Loss: -97.1854, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [207 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 97.2005, D Real Loss: 97.8294, Wasserstein Loss: -97.1626, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [207 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 97.1187, D Real Loss: 97.7420, Wasserstein Loss: -97.0766, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [207 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 97.2320, D Real Loss: 97.8538, Wasserstein Loss: -97.1995, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [207 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.3091, D Real Loss: 97.9268, Wasserstein Loss: -97.2736, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [207 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 97.3885, D Real Loss: 98.0092, Wasserstein Loss: -97.3551, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [207 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 97.3381, D Real Loss: 97.9581, Wasserstein Loss: -97.3100, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [207 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 97.3574, D Real Loss: 97.9770, Wasserstein Loss: -97.3336, Content Loss: 0.0002, PSNR: 37.95
Epoch [207 / 300]: Gradient Penalty: 0.0384, D Fake Loss: 97.4022, D Real Loss: 98.0214, Wasserstein Loss: -97.3793, Content Loss: 0.0002, PSNR: 37.95
Epoch [208 / 300]
===> Epoch [208 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0353, D Fake Loss: 98.1985, D Real Loss: 98.8131, Wasserstein Loss: -98.1255, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [208 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 98.0006, D Real Loss: 98.6248, Wasserstein Loss: -97.9585, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [208 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 97.7305, D Real Loss: 98.3586, Wasserstein Loss: -97.6906, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [208 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.4426, D Real Loss: 98.0747, Wasserstein Loss: -97.3901, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [208 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 97.6533, D Real Loss: 98.2866, Wasserstein Loss: -97.6197, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [208 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0405, D Fake Loss: 97.5274, D Real Loss: 98.1684, Wasserstein Loss: -97.4843, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [208 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 97.3894, D Real Loss: 98.0253, Wasserstein Loss: -97.3497, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [208 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 97.4088, D Real Loss: 98.0440, Wasserstein Loss: -97.3699, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [208 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 97.4337, D Real Loss: 98.0659, Wasserstein Loss: -97.4022, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [208 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 97.3536, D Real Loss: 97.9827, Wasserstein Loss: -97.3161, Content Loss: 0.0002, PSNR: 37.88
Epoch [208 / 300]: Gradient Penalty: 0.0392, D Fake Loss: 97.3225, D Real Loss: 97.9509, Wasserstein Loss: -97.2849, Content Loss: 0.0002, PSNR: 37.88
Epoch [209 / 300]
===> Epoch [209 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 98.9018, D Real Loss: 99.5239, Wasserstein Loss: -98.9316, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [209 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 98.2478, D Real Loss: 98.8959, Wasserstein Loss: -98.2337, Content Loss: 0.0002, PSNR: 37.77
===> Epoch [209 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 98.5537, D Real Loss: 99.2062, Wasserstein Loss: -98.5359, Content Loss: 0.0002, PSNR: 37.78
===> Epoch [209 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 98.1975, D Real Loss: 98.8342, Wasserstein Loss: -98.1830, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [209 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 98.2707, D Real Loss: 98.8995, Wasserstein Loss: -98.2539, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [209 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.2746, D Real Loss: 98.9123, Wasserstein Loss: -98.2516, Content Loss: 0.0002, PSNR: 37.87
===> Epoch [209 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.4326, D Real Loss: 99.0679, Wasserstein Loss: -98.4151, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [209 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.2970, D Real Loss: 98.9290, Wasserstein Loss: -98.2823, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [209 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 98.2214, D Real Loss: 98.8541, Wasserstein Loss: -98.2048, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [209 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 98.0864, D Real Loss: 98.7187, Wasserstein Loss: -98.0592, Content Loss: 0.0002, PSNR: 37.95
Epoch [209 / 300]: Gradient Penalty: 0.0398, D Fake Loss: 98.0381, D Real Loss: 98.6704, Wasserstein Loss: -98.0119, Content Loss: 0.0002, PSNR: 37.96
Epoch [210 / 300]
===> Epoch [210 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 96.0568, D Real Loss: 96.6841, Wasserstein Loss: -96.0361, Content Loss: 0.0002, PSNR: 37.69
===> Epoch [210 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 97.2677, D Real Loss: 97.8877, Wasserstein Loss: -97.2700, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [210 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.1033, D Real Loss: 98.7341, Wasserstein Loss: -98.0925, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [210 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 98.2653, D Real Loss: 98.8912, Wasserstein Loss: -98.2521, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [210 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 98.2860, D Real Loss: 98.9128, Wasserstein Loss: -98.2759, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [210 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 98.3196, D Real Loss: 98.9478, Wasserstein Loss: -98.3012, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [210 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 98.3211, D Real Loss: 98.9537, Wasserstein Loss: -98.3013, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [210 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 98.4291, D Real Loss: 99.0609, Wasserstein Loss: -98.4006, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [210 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 98.4568, D Real Loss: 99.0870, Wasserstein Loss: -98.4270, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [210 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 98.4169, D Real Loss: 99.0454, Wasserstein Loss: -98.3912, Content Loss: 0.0002, PSNR: 37.93
Epoch [210 / 300]: Gradient Penalty: 0.0389, D Fake Loss: 98.3831, D Real Loss: 99.0108, Wasserstein Loss: -98.3584, Content Loss: 0.0002, PSNR: 37.92
Epoch [211 / 300]
===> Epoch [211 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 98.8137, D Real Loss: 99.4737, Wasserstein Loss: -98.8365, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [211 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 99.1480, D Real Loss: 99.7832, Wasserstein Loss: -99.1451, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [211 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 99.2308, D Real Loss: 99.8588, Wasserstein Loss: -99.2076, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [211 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 98.5021, D Real Loss: 99.1274, Wasserstein Loss: -98.4722, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [211 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 98.1541, D Real Loss: 98.7831, Wasserstein Loss: -98.1284, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [211 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 98.3438, D Real Loss: 98.9704, Wasserstein Loss: -98.3208, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [211 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 98.4324, D Real Loss: 99.0556, Wasserstein Loss: -98.4077, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [211 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.3077, D Real Loss: 98.9265, Wasserstein Loss: -98.2835, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [211 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 98.3203, D Real Loss: 98.9390, Wasserstein Loss: -98.2982, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [211 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.3779, D Real Loss: 98.9975, Wasserstein Loss: -98.3509, Content Loss: 0.0002, PSNR: 37.98
Epoch [211 / 300]: Gradient Penalty: 0.0382, D Fake Loss: 98.3345, D Real Loss: 98.9533, Wasserstein Loss: -98.3048, Content Loss: 0.0002, PSNR: 37.98
Epoch [212 / 300]
===> Epoch [212 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 97.5322, D Real Loss: 98.1807, Wasserstein Loss: -97.5520, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [212 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0424, D Fake Loss: 98.4886, D Real Loss: 99.1551, Wasserstein Loss: -98.4946, Content Loss: 0.0002, PSNR: 37.80
===> Epoch [212 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 98.6158, D Real Loss: 99.2673, Wasserstein Loss: -98.6207, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [212 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 98.9827, D Real Loss: 99.6223, Wasserstein Loss: -98.9705, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [212 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.8085, D Real Loss: 99.4424, Wasserstein Loss: -98.7948, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [212 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.4700, D Real Loss: 99.0980, Wasserstein Loss: -98.4523, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [212 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.3889, D Real Loss: 99.0115, Wasserstein Loss: -98.3675, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [212 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 98.0694, D Real Loss: 98.6918, Wasserstein Loss: -98.0537, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [212 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 98.2246, D Real Loss: 98.8493, Wasserstein Loss: -98.2108, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [212 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.0276, D Real Loss: 98.6483, Wasserstein Loss: -98.0117, Content Loss: 0.0002, PSNR: 37.95
Epoch [212 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 98.0531, D Real Loss: 98.6734, Wasserstein Loss: -98.0366, Content Loss: 0.0002, PSNR: 37.96
Epoch [213 / 300]
===> Epoch [213 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0352, D Fake Loss: 97.4655, D Real Loss: 98.0745, Wasserstein Loss: -97.4091, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [213 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 97.0547, D Real Loss: 97.6712, Wasserstein Loss: -96.9892, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [213 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 97.9003, D Real Loss: 98.5212, Wasserstein Loss: -97.8730, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [213 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 98.0709, D Real Loss: 98.6898, Wasserstein Loss: -98.0465, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [213 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 97.6916, D Real Loss: 98.3046, Wasserstein Loss: -97.6673, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [213 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 97.6668, D Real Loss: 98.2860, Wasserstein Loss: -97.6334, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [213 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 97.7530, D Real Loss: 98.3743, Wasserstein Loss: -97.7292, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [213 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.9751, D Real Loss: 98.6002, Wasserstein Loss: -97.9431, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [213 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.0922, D Real Loss: 98.7143, Wasserstein Loss: -98.0586, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [213 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 98.0049, D Real Loss: 98.6256, Wasserstein Loss: -97.9761, Content Loss: 0.0002, PSNR: 37.95
Epoch [213 / 300]: Gradient Penalty: 0.0385, D Fake Loss: 98.0129, D Real Loss: 98.6335, Wasserstein Loss: -97.9857, Content Loss: 0.0002, PSNR: 37.95
Epoch [214 / 300]
===> Epoch [214 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 97.5269, D Real Loss: 98.1606, Wasserstein Loss: -97.4994, Content Loss: 0.0002, PSNR: 37.76
===> Epoch [214 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.4053, D Real Loss: 99.0325, Wasserstein Loss: -98.3913, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [214 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 98.8782, D Real Loss: 99.5070, Wasserstein Loss: -98.8556, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [214 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 98.5736, D Real Loss: 99.1980, Wasserstein Loss: -98.5481, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [214 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 98.3259, D Real Loss: 98.9482, Wasserstein Loss: -98.2949, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [214 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 98.1814, D Real Loss: 98.8003, Wasserstein Loss: -98.1527, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [214 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 98.3375, D Real Loss: 98.9585, Wasserstein Loss: -98.3029, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [214 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.3090, D Real Loss: 98.9328, Wasserstein Loss: -98.2821, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [214 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 98.1670, D Real Loss: 98.7909, Wasserstein Loss: -98.1377, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [214 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.3145, D Real Loss: 98.9367, Wasserstein Loss: -98.2866, Content Loss: 0.0002, PSNR: 37.98
Epoch [214 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 98.2931, D Real Loss: 98.9145, Wasserstein Loss: -98.2617, Content Loss: 0.0002, PSNR: 37.98
Epoch [215 / 300]
===> Epoch [215 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0343, D Fake Loss: 97.4495, D Real Loss: 98.0428, Wasserstein Loss: -97.4503, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [215 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 97.9081, D Real Loss: 98.5136, Wasserstein Loss: -97.9080, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [215 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 97.9244, D Real Loss: 98.5434, Wasserstein Loss: -97.9130, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [215 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 97.8800, D Real Loss: 98.4962, Wasserstein Loss: -97.8696, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [215 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 97.6565, D Real Loss: 98.2714, Wasserstein Loss: -97.6394, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [215 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 97.9125, D Real Loss: 98.5262, Wasserstein Loss: -97.8967, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [215 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 98.0702, D Real Loss: 98.6856, Wasserstein Loss: -98.0543, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [215 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 97.8279, D Real Loss: 98.4442, Wasserstein Loss: -97.8069, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [215 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 97.8885, D Real Loss: 98.5037, Wasserstein Loss: -97.8680, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [215 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 98.0275, D Real Loss: 98.6419, Wasserstein Loss: -98.0102, Content Loss: 0.0002, PSNR: 38.06
Epoch [215 / 300]: Gradient Penalty: 0.0373, D Fake Loss: 98.0382, D Real Loss: 98.6520, Wasserstein Loss: -98.0199, Content Loss: 0.0002, PSNR: 38.06
Epoch [216 / 300]
===> Epoch [216 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 100.3741, D Real Loss: 101.0162, Wasserstein Loss: -100.3675, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [216 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0414, D Fake Loss: 99.7875, D Real Loss: 100.4414, Wasserstein Loss: -99.7615, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [216 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 99.2507, D Real Loss: 99.8928, Wasserstein Loss: -99.2347, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [216 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 99.0658, D Real Loss: 99.6992, Wasserstein Loss: -99.0522, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [216 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.6515, D Real Loss: 99.2810, Wasserstein Loss: -98.6286, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [216 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.2010, D Real Loss: 98.8293, Wasserstein Loss: -98.1755, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [216 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.1957, D Real Loss: 98.8198, Wasserstein Loss: -98.1698, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [216 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 98.1680, D Real Loss: 98.7913, Wasserstein Loss: -98.1478, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [216 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 98.1521, D Real Loss: 98.7787, Wasserstein Loss: -98.1253, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [216 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 98.0677, D Real Loss: 98.6931, Wasserstein Loss: -98.0420, Content Loss: 0.0002, PSNR: 37.98
Epoch [216 / 300]: Gradient Penalty: 0.0391, D Fake Loss: 98.0695, D Real Loss: 98.6945, Wasserstein Loss: -98.0436, Content Loss: 0.0002, PSNR: 37.98
Epoch [217 / 300]
===> Epoch [217 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0458, D Fake Loss: 98.1752, D Real Loss: 98.8261, Wasserstein Loss: -98.1274, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [217 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0419, D Fake Loss: 98.3459, D Real Loss: 98.9829, Wasserstein Loss: -98.3190, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [217 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 98.5801, D Real Loss: 99.2084, Wasserstein Loss: -98.5649, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [217 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 98.2171, D Real Loss: 98.8382, Wasserstein Loss: -98.1936, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [217 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 98.2210, D Real Loss: 98.8398, Wasserstein Loss: -98.1960, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [217 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.2772, D Real Loss: 98.8953, Wasserstein Loss: -98.2503, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [217 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 98.1760, D Real Loss: 98.7941, Wasserstein Loss: -98.1520, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [217 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 98.2646, D Real Loss: 98.8831, Wasserstein Loss: -98.2397, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [217 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 98.3196, D Real Loss: 98.9387, Wasserstein Loss: -98.2996, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [217 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 98.5302, D Real Loss: 99.1484, Wasserstein Loss: -98.5090, Content Loss: 0.0002, PSNR: 38.07
Epoch [217 / 300]: Gradient Penalty: 0.0382, D Fake Loss: 98.5384, D Real Loss: 99.1565, Wasserstein Loss: -98.5130, Content Loss: 0.0002, PSNR: 38.07
Epoch [218 / 300]
===> Epoch [218 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0358, D Fake Loss: 97.8890, D Real Loss: 98.5058, Wasserstein Loss: -97.8803, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [218 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 98.5114, D Real Loss: 99.1379, Wasserstein Loss: -98.4794, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [218 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 98.6003, D Real Loss: 99.2273, Wasserstein Loss: -98.5825, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [218 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 98.6098, D Real Loss: 99.2361, Wasserstein Loss: -98.5957, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [218 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 98.8631, D Real Loss: 99.4898, Wasserstein Loss: -98.8370, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [218 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 98.7971, D Real Loss: 99.4196, Wasserstein Loss: -98.7765, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [218 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 98.5541, D Real Loss: 99.1760, Wasserstein Loss: -98.5332, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [218 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 98.2940, D Real Loss: 98.9141, Wasserstein Loss: -98.2691, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [218 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 98.4926, D Real Loss: 99.1128, Wasserstein Loss: -98.4680, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [218 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 98.3708, D Real Loss: 98.9929, Wasserstein Loss: -98.3423, Content Loss: 0.0002, PSNR: 37.98
Epoch [218 / 300]: Gradient Penalty: 0.0379, D Fake Loss: 98.3522, D Real Loss: 98.9743, Wasserstein Loss: -98.3279, Content Loss: 0.0002, PSNR: 37.98
Epoch [219 / 300]
===> Epoch [219 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 99.4787, D Real Loss: 100.1178, Wasserstein Loss: -99.4600, Content Loss: 0.0002, PSNR: 37.92
===> Epoch [219 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 99.1874, D Real Loss: 99.8093, Wasserstein Loss: -99.1582, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [219 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 99.3757, D Real Loss: 100.0015, Wasserstein Loss: -99.3390, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [219 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.8375, D Real Loss: 99.4723, Wasserstein Loss: -98.8202, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [219 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 98.8334, D Real Loss: 99.4597, Wasserstein Loss: -98.8031, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [219 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 98.6305, D Real Loss: 99.2516, Wasserstein Loss: -98.5972, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [219 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 98.4198, D Real Loss: 99.0362, Wasserstein Loss: -98.3969, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [219 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 98.3868, D Real Loss: 99.0055, Wasserstein Loss: -98.3637, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [219 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 98.4280, D Real Loss: 99.0452, Wasserstein Loss: -98.4046, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [219 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 98.3079, D Real Loss: 98.9261, Wasserstein Loss: -98.2834, Content Loss: 0.0002, PSNR: 38.00
Epoch [219 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 98.3198, D Real Loss: 98.9390, Wasserstein Loss: -98.2978, Content Loss: 0.0002, PSNR: 38.00
Epoch [220 / 300]
===> Epoch [220 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 98.3360, D Real Loss: 98.9541, Wasserstein Loss: -98.2520, Content Loss: 0.0002, PSNR: 37.86
===> Epoch [220 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 97.9138, D Real Loss: 98.5391, Wasserstein Loss: -97.8588, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [220 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 97.8199, D Real Loss: 98.4463, Wasserstein Loss: -97.7725, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [220 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 98.1257, D Real Loss: 98.7606, Wasserstein Loss: -98.0894, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [220 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 98.2939, D Real Loss: 98.9326, Wasserstein Loss: -98.2536, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [220 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 98.3239, D Real Loss: 98.9592, Wasserstein Loss: -98.2929, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [220 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 98.3088, D Real Loss: 98.9470, Wasserstein Loss: -98.2734, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [220 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 98.1923, D Real Loss: 98.8225, Wasserstein Loss: -98.1581, Content Loss: 0.0002, PSNR: 37.94
===> Epoch [220 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.2892, D Real Loss: 98.9159, Wasserstein Loss: -98.2610, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [220 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 98.4208, D Real Loss: 99.0462, Wasserstein Loss: -98.3920, Content Loss: 0.0002, PSNR: 37.97
Epoch [220 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 98.4174, D Real Loss: 99.0422, Wasserstein Loss: -98.3851, Content Loss: 0.0002, PSNR: 37.96
Epoch [221 / 300]
===> Epoch [221 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 98.8423, D Real Loss: 99.4579, Wasserstein Loss: -98.8765, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [221 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 98.9573, D Real Loss: 99.5820, Wasserstein Loss: -98.9412, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [221 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 98.7931, D Real Loss: 99.4187, Wasserstein Loss: -98.7831, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [221 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 98.6969, D Real Loss: 99.3205, Wasserstein Loss: -98.6807, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [221 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 98.4358, D Real Loss: 99.0611, Wasserstein Loss: -98.4116, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [221 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 98.5478, D Real Loss: 99.1783, Wasserstein Loss: -98.5345, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [221 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 98.5819, D Real Loss: 99.2086, Wasserstein Loss: -98.5597, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [221 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 98.6190, D Real Loss: 99.2420, Wasserstein Loss: -98.5948, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [221 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 98.7226, D Real Loss: 99.3464, Wasserstein Loss: -98.6990, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [221 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 98.6392, D Real Loss: 99.2626, Wasserstein Loss: -98.6164, Content Loss: 0.0002, PSNR: 37.95
Epoch [221 / 300]: Gradient Penalty: 0.0392, D Fake Loss: 98.6544, D Real Loss: 99.2771, Wasserstein Loss: -98.6331, Content Loss: 0.0002, PSNR: 37.94
Epoch [222 / 300]
===> Epoch [222 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 98.4905, D Real Loss: 99.0941, Wasserstein Loss: -98.4955, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [222 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 99.5133, D Real Loss: 100.1231, Wasserstein Loss: -99.4844, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [222 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 98.9724, D Real Loss: 99.5822, Wasserstein Loss: -98.9247, Content Loss: 0.0002, PSNR: 37.73
===> Epoch [222 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 99.1508, D Real Loss: 99.7701, Wasserstein Loss: -99.1222, Content Loss: 0.0002, PSNR: 37.68
===> Epoch [222 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 99.2563, D Real Loss: 99.8876, Wasserstein Loss: -99.2240, Content Loss: 0.0002, PSNR: 37.70
===> Epoch [222 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 99.3755, D Real Loss: 100.0146, Wasserstein Loss: -99.3427, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [222 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 99.4459, D Real Loss: 100.0798, Wasserstein Loss: -99.4206, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [222 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0405, D Fake Loss: 99.4842, D Real Loss: 100.1178, Wasserstein Loss: -99.4537, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [222 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 99.5657, D Real Loss: 100.1969, Wasserstein Loss: -99.5319, Content Loss: 0.0002, PSNR: 37.85
===> Epoch [222 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 99.5972, D Real Loss: 100.2280, Wasserstein Loss: -99.5679, Content Loss: 0.0002, PSNR: 37.87
Epoch [222 / 300]: Gradient Penalty: 0.0397, D Fake Loss: 99.6180, D Real Loss: 100.2484, Wasserstein Loss: -99.5898, Content Loss: 0.0002, PSNR: 37.87
Epoch [223 / 300]
===> Epoch [223 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0414, D Fake Loss: 100.8711, D Real Loss: 101.5296, Wasserstein Loss: -100.8267, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [223 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 100.1214, D Real Loss: 100.7661, Wasserstein Loss: -100.0828, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [223 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 100.3418, D Real Loss: 100.9826, Wasserstein Loss: -100.3212, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [223 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.2890, D Real Loss: 100.9228, Wasserstein Loss: -100.2530, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [223 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 99.8984, D Real Loss: 100.5244, Wasserstein Loss: -99.8762, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [223 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 99.7243, D Real Loss: 100.3473, Wasserstein Loss: -99.6987, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [223 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 99.7798, D Real Loss: 100.4062, Wasserstein Loss: -99.7470, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [223 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 99.6213, D Real Loss: 100.2479, Wasserstein Loss: -99.5950, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [223 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 99.5879, D Real Loss: 100.2143, Wasserstein Loss: -99.5602, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [223 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 99.4834, D Real Loss: 100.1076, Wasserstein Loss: -99.4547, Content Loss: 0.0002, PSNR: 38.05
Epoch [223 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 99.4676, D Real Loss: 100.0918, Wasserstein Loss: -99.4388, Content Loss: 0.0002, PSNR: 38.05
Epoch [224 / 300]
===> Epoch [224 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0418, D Fake Loss: 100.5489, D Real Loss: 101.1975, Wasserstein Loss: -100.5587, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [224 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 100.1967, D Real Loss: 100.8488, Wasserstein Loss: -100.1919, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [224 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 100.1969, D Real Loss: 100.8462, Wasserstein Loss: -100.1723, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [224 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 99.9609, D Real Loss: 100.6079, Wasserstein Loss: -99.9402, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [224 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 100.0723, D Real Loss: 100.7184, Wasserstein Loss: -100.0532, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [224 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 100.0732, D Real Loss: 100.7134, Wasserstein Loss: -100.0543, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [224 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 99.9396, D Real Loss: 100.5741, Wasserstein Loss: -99.9165, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [224 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 99.9123, D Real Loss: 100.5487, Wasserstein Loss: -99.8923, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [224 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 100.0712, D Real Loss: 100.7057, Wasserstein Loss: -100.0492, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [224 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 99.9683, D Real Loss: 100.6012, Wasserstein Loss: -99.9433, Content Loss: 0.0002, PSNR: 38.05
Epoch [224 / 300]: Gradient Penalty: 0.0397, D Fake Loss: 99.9482, D Real Loss: 100.5806, Wasserstein Loss: -99.9236, Content Loss: 0.0002, PSNR: 38.05
Epoch [225 / 300]
===> Epoch [225 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 99.2717, D Real Loss: 99.8888, Wasserstein Loss: -99.2092, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [225 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 99.9651, D Real Loss: 100.5958, Wasserstein Loss: -99.9642, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [225 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 99.9195, D Real Loss: 100.5496, Wasserstein Loss: -99.9068, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [225 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0403, D Fake Loss: 99.8354, D Real Loss: 100.4612, Wasserstein Loss: -99.8082, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [225 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 99.4815, D Real Loss: 100.1033, Wasserstein Loss: -99.4287, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [225 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 98.9370, D Real Loss: 99.5562, Wasserstein Loss: -98.8572, Content Loss: 0.0002, PSNR: 37.67
===> Epoch [225 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 98.7400, D Real Loss: 99.3578, Wasserstein Loss: -98.6939, Content Loss: 0.0002, PSNR: 37.65
===> Epoch [225 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 99.0906, D Real Loss: 99.7105, Wasserstein Loss: -99.0571, Content Loss: 0.0002, PSNR: 37.64
===> Epoch [225 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 99.3499, D Real Loss: 99.9724, Wasserstein Loss: -99.3211, Content Loss: 0.0002, PSNR: 37.67
===> Epoch [225 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 99.6996, D Real Loss: 100.3229, Wasserstein Loss: -99.6703, Content Loss: 0.0002, PSNR: 37.67
Epoch [225 / 300]: Gradient Penalty: 0.0397, D Fake Loss: 99.7350, D Real Loss: 100.3586, Wasserstein Loss: -99.7059, Content Loss: 0.0002, PSNR: 37.67
Epoch [226 / 300]
===> Epoch [226 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 103.8615, D Real Loss: 104.4877, Wasserstein Loss: -103.8402, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [226 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 103.2644, D Real Loss: 103.8997, Wasserstein Loss: -103.2266, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [226 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 103.0977, D Real Loss: 103.7337, Wasserstein Loss: -103.0702, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [226 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0405, D Fake Loss: 103.2294, D Real Loss: 103.8700, Wasserstein Loss: -103.2047, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [226 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 103.3546, D Real Loss: 103.9918, Wasserstein Loss: -103.3328, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [226 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 103.2937, D Real Loss: 103.9243, Wasserstein Loss: -103.2689, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [226 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 103.2905, D Real Loss: 103.9232, Wasserstein Loss: -103.2685, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [226 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 103.1021, D Real Loss: 103.7307, Wasserstein Loss: -103.0780, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [226 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 102.9082, D Real Loss: 103.5325, Wasserstein Loss: -102.8794, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [226 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 102.7595, D Real Loss: 103.3865, Wasserstein Loss: -102.7357, Content Loss: 0.0002, PSNR: 38.03
Epoch [226 / 300]: Gradient Penalty: 0.0384, D Fake Loss: 102.7688, D Real Loss: 103.3957, Wasserstein Loss: -102.7450, Content Loss: 0.0002, PSNR: 38.03
Epoch [227 / 300]
===> Epoch [227 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 102.4685, D Real Loss: 103.1146, Wasserstein Loss: -102.4105, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [227 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 102.0928, D Real Loss: 102.7240, Wasserstein Loss: -102.0553, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [227 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.9495, D Real Loss: 102.5756, Wasserstein Loss: -101.9233, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [227 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 102.0004, D Real Loss: 102.6352, Wasserstein Loss: -101.9634, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [227 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.7188, D Real Loss: 102.3517, Wasserstein Loss: -101.6836, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [227 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.8275, D Real Loss: 102.4564, Wasserstein Loss: -101.7991, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [227 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 101.7457, D Real Loss: 102.3806, Wasserstein Loss: -101.7180, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [227 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.7769, D Real Loss: 102.4105, Wasserstein Loss: -101.7476, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [227 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.5609, D Real Loss: 102.1922, Wasserstein Loss: -101.5343, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [227 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.5107, D Real Loss: 102.1406, Wasserstein Loss: -101.4845, Content Loss: 0.0002, PSNR: 38.03
Epoch [227 / 300]: Gradient Penalty: 0.0391, D Fake Loss: 101.5183, D Real Loss: 102.1485, Wasserstein Loss: -101.4927, Content Loss: 0.0002, PSNR: 38.02
Epoch [228 / 300]
===> Epoch [228 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 100.7017, D Real Loss: 101.3444, Wasserstein Loss: -100.6319, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [228 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 100.6528, D Real Loss: 101.2669, Wasserstein Loss: -100.6079, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [228 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.1332, D Real Loss: 101.7500, Wasserstein Loss: -101.1111, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [228 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.1201, D Real Loss: 101.7328, Wasserstein Loss: -101.0865, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [228 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.0961, D Real Loss: 101.7078, Wasserstein Loss: -101.0662, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [228 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.9907, D Real Loss: 101.6081, Wasserstein Loss: -100.9600, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [228 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 100.8630, D Real Loss: 101.4772, Wasserstein Loss: -100.8336, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [228 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 100.8233, D Real Loss: 101.4363, Wasserstein Loss: -100.7962, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [228 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 100.7087, D Real Loss: 101.3229, Wasserstein Loss: -100.6835, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [228 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 100.6511, D Real Loss: 101.2652, Wasserstein Loss: -100.6204, Content Loss: 0.0002, PSNR: 38.10
Epoch [228 / 300]: Gradient Penalty: 0.0374, D Fake Loss: 100.6243, D Real Loss: 101.2395, Wasserstein Loss: -100.5956, Content Loss: 0.0002, PSNR: 38.10
Epoch [229 / 300]
===> Epoch [229 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 101.1192, D Real Loss: 101.7476, Wasserstein Loss: -101.1287, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [229 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 100.7939, D Real Loss: 101.4326, Wasserstein Loss: -100.7965, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [229 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.1599, D Real Loss: 101.7895, Wasserstein Loss: -101.1511, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [229 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.0581, D Real Loss: 101.6887, Wasserstein Loss: -101.0344, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [229 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.9037, D Real Loss: 101.5279, Wasserstein Loss: -100.8839, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [229 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.7558, D Real Loss: 101.3774, Wasserstein Loss: -100.7348, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [229 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.6871, D Real Loss: 101.3095, Wasserstein Loss: -100.6668, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [229 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.7659, D Real Loss: 101.3889, Wasserstein Loss: -100.7502, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [229 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.6734, D Real Loss: 101.2946, Wasserstein Loss: -100.6477, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [229 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.6087, D Real Loss: 101.2288, Wasserstein Loss: -100.5873, Content Loss: 0.0002, PSNR: 38.07
Epoch [229 / 300]: Gradient Penalty: 0.0382, D Fake Loss: 100.6158, D Real Loss: 101.2357, Wasserstein Loss: -100.5952, Content Loss: 0.0002, PSNR: 38.08
Epoch [230 / 300]
===> Epoch [230 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0414, D Fake Loss: 101.3238, D Real Loss: 101.9603, Wasserstein Loss: -101.2878, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [230 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0420, D Fake Loss: 100.5270, D Real Loss: 101.1568, Wasserstein Loss: -100.4973, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [230 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0423, D Fake Loss: 100.3040, D Real Loss: 100.9338, Wasserstein Loss: -100.2726, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [230 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 100.3890, D Real Loss: 101.0157, Wasserstein Loss: -100.3659, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [230 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 100.5665, D Real Loss: 101.1901, Wasserstein Loss: -100.5422, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [230 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.5323, D Real Loss: 101.1578, Wasserstein Loss: -100.5052, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [230 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.6143, D Real Loss: 101.2403, Wasserstein Loss: -100.5881, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [230 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.5918, D Real Loss: 101.2170, Wasserstein Loss: -100.5679, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [230 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.6954, D Real Loss: 101.3193, Wasserstein Loss: -100.6739, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [230 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 100.6146, D Real Loss: 101.2375, Wasserstein Loss: -100.5887, Content Loss: 0.0002, PSNR: 38.09
Epoch [230 / 300]: Gradient Penalty: 0.0391, D Fake Loss: 100.6128, D Real Loss: 101.2355, Wasserstein Loss: -100.5880, Content Loss: 0.0002, PSNR: 38.08
Epoch [231 / 300]
===> Epoch [231 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 100.5408, D Real Loss: 101.1841, Wasserstein Loss: -100.5275, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [231 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 100.7171, D Real Loss: 101.3478, Wasserstein Loss: -100.7035, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [231 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.5778, D Real Loss: 101.2044, Wasserstein Loss: -100.5604, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [231 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.2003, D Real Loss: 100.8241, Wasserstein Loss: -100.1743, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [231 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.3760, D Real Loss: 101.0005, Wasserstein Loss: -100.3508, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [231 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.4088, D Real Loss: 101.0363, Wasserstein Loss: -100.3812, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [231 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.5035, D Real Loss: 101.1324, Wasserstein Loss: -100.4780, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [231 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 100.1968, D Real Loss: 100.8289, Wasserstein Loss: -100.1690, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [231 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 100.1889, D Real Loss: 100.8200, Wasserstein Loss: -100.1645, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [231 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.1452, D Real Loss: 100.7752, Wasserstein Loss: -100.1209, Content Loss: 0.0002, PSNR: 38.04
Epoch [231 / 300]: Gradient Penalty: 0.0391, D Fake Loss: 100.1616, D Real Loss: 100.7916, Wasserstein Loss: -100.1363, Content Loss: 0.0002, PSNR: 38.05
Epoch [232 / 300]
===> Epoch [232 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 99.2776, D Real Loss: 99.9180, Wasserstein Loss: -99.2468, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [232 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 100.0661, D Real Loss: 100.6868, Wasserstein Loss: -100.0338, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [232 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 100.0885, D Real Loss: 100.7049, Wasserstein Loss: -100.0468, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [232 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.2877, D Real Loss: 100.9082, Wasserstein Loss: -100.2619, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [232 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.3516, D Real Loss: 100.9733, Wasserstein Loss: -100.3219, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [232 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 100.2755, D Real Loss: 100.9053, Wasserstein Loss: -100.2527, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [232 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 100.4252, D Real Loss: 101.0565, Wasserstein Loss: -100.3987, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [232 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 100.4296, D Real Loss: 101.0601, Wasserstein Loss: -100.4042, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [232 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.3801, D Real Loss: 101.0094, Wasserstein Loss: -100.3495, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [232 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 100.3082, D Real Loss: 100.9415, Wasserstein Loss: -100.2817, Content Loss: 0.0002, PSNR: 38.02
Epoch [232 / 300]: Gradient Penalty: 0.0401, D Fake Loss: 100.3143, D Real Loss: 100.9471, Wasserstein Loss: -100.2888, Content Loss: 0.0002, PSNR: 38.02
Epoch [233 / 300]
===> Epoch [233 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 102.1148, D Real Loss: 102.7606, Wasserstein Loss: -102.0811, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [233 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.4991, D Real Loss: 102.1417, Wasserstein Loss: -101.4610, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [233 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.7622, D Real Loss: 102.3994, Wasserstein Loss: -101.7440, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [233 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 101.5516, D Real Loss: 102.1891, Wasserstein Loss: -101.5164, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [233 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 101.2879, D Real Loss: 101.9157, Wasserstein Loss: -101.2597, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [233 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.1840, D Real Loss: 101.8074, Wasserstein Loss: -101.1588, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [233 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.2155, D Real Loss: 101.8371, Wasserstein Loss: -101.1923, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [233 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.2382, D Real Loss: 101.8637, Wasserstein Loss: -101.2074, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [233 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.0761, D Real Loss: 101.7018, Wasserstein Loss: -101.0385, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [233 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 100.8457, D Real Loss: 101.4702, Wasserstein Loss: -100.8187, Content Loss: 0.0002, PSNR: 37.97
Epoch [233 / 300]: Gradient Penalty: 0.0392, D Fake Loss: 100.8739, D Real Loss: 101.4979, Wasserstein Loss: -100.8518, Content Loss: 0.0002, PSNR: 37.97
Epoch [234 / 300]
===> Epoch [234 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0421, D Fake Loss: 101.9334, D Real Loss: 102.6098, Wasserstein Loss: -101.9012, Content Loss: 0.0002, PSNR: 37.75
===> Epoch [234 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0409, D Fake Loss: 102.3387, D Real Loss: 103.0012, Wasserstein Loss: -102.3034, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [234 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 102.0909, D Real Loss: 102.7421, Wasserstein Loss: -102.0392, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [234 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.6840, D Real Loss: 102.3232, Wasserstein Loss: -101.6380, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [234 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.5357, D Real Loss: 102.1730, Wasserstein Loss: -101.4955, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [234 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.2329, D Real Loss: 101.8613, Wasserstein Loss: -101.2012, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [234 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.2279, D Real Loss: 101.8540, Wasserstein Loss: -101.1981, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [234 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 101.2104, D Real Loss: 101.8343, Wasserstein Loss: -101.1793, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [234 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 101.0830, D Real Loss: 101.7055, Wasserstein Loss: -101.0482, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [234 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 100.9753, D Real Loss: 101.5940, Wasserstein Loss: -100.9452, Content Loss: 0.0002, PSNR: 38.08
Epoch [234 / 300]: Gradient Penalty: 0.0362, D Fake Loss: 100.9723, D Real Loss: 101.5907, Wasserstein Loss: -100.9414, Content Loss: 0.0002, PSNR: 38.07
Epoch [235 / 300]
===> Epoch [235 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 99.7815, D Real Loss: 100.4165, Wasserstein Loss: -99.7766, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [235 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 100.6332, D Real Loss: 101.2606, Wasserstein Loss: -100.6213, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [235 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.7139, D Real Loss: 101.3440, Wasserstein Loss: -100.6822, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [235 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.5520, D Real Loss: 101.1799, Wasserstein Loss: -100.5215, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [235 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 100.7296, D Real Loss: 101.3585, Wasserstein Loss: -100.7080, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [235 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 100.8777, D Real Loss: 101.5105, Wasserstein Loss: -100.8493, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [235 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 100.7609, D Real Loss: 101.3917, Wasserstein Loss: -100.7342, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [235 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 100.8093, D Real Loss: 101.4389, Wasserstein Loss: -100.7805, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [235 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0396, D Fake Loss: 100.7725, D Real Loss: 101.4054, Wasserstein Loss: -100.7470, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [235 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 100.7831, D Real Loss: 101.4179, Wasserstein Loss: -100.7547, Content Loss: 0.0002, PSNR: 37.95
Epoch [235 / 300]: Gradient Penalty: 0.0398, D Fake Loss: 100.7705, D Real Loss: 101.4057, Wasserstein Loss: -100.7429, Content Loss: 0.0002, PSNR: 37.95
Epoch [236 / 300]
===> Epoch [236 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0418, D Fake Loss: 101.3988, D Real Loss: 102.0525, Wasserstein Loss: -101.3955, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [236 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.2730, D Real Loss: 101.9049, Wasserstein Loss: -101.2451, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [236 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 100.9691, D Real Loss: 101.5924, Wasserstein Loss: -100.9411, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [236 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.0732, D Real Loss: 101.7052, Wasserstein Loss: -101.0611, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [236 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.2952, D Real Loss: 101.9270, Wasserstein Loss: -101.2770, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [236 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.2085, D Real Loss: 101.8403, Wasserstein Loss: -101.1896, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [236 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.2303, D Real Loss: 101.8625, Wasserstein Loss: -101.2107, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [236 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.3460, D Real Loss: 101.9775, Wasserstein Loss: -101.3256, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [236 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.2890, D Real Loss: 101.9206, Wasserstein Loss: -101.2663, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [236 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.2609, D Real Loss: 101.8889, Wasserstein Loss: -101.2380, Content Loss: 0.0002, PSNR: 38.05
Epoch [236 / 300]: Gradient Penalty: 0.0386, D Fake Loss: 101.2676, D Real Loss: 101.8959, Wasserstein Loss: -101.2444, Content Loss: 0.0002, PSNR: 38.05
Epoch [237 / 300]
===> Epoch [237 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 100.4667, D Real Loss: 101.1006, Wasserstein Loss: -100.4449, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [237 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.8673, D Real Loss: 101.4966, Wasserstein Loss: -100.8496, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [237 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.6649, D Real Loss: 101.2841, Wasserstein Loss: -100.6333, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [237 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.8671, D Real Loss: 101.4920, Wasserstein Loss: -100.8437, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [237 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 100.9689, D Real Loss: 101.5913, Wasserstein Loss: -100.9454, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [237 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.9037, D Real Loss: 101.5336, Wasserstein Loss: -100.8769, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [237 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.9170, D Real Loss: 101.5454, Wasserstein Loss: -100.8912, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [237 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.7341, D Real Loss: 101.3643, Wasserstein Loss: -100.7059, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [237 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.8455, D Real Loss: 101.4754, Wasserstein Loss: -100.8206, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [237 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.8558, D Real Loss: 101.4830, Wasserstein Loss: -100.8322, Content Loss: 0.0002, PSNR: 38.07
Epoch [237 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 100.8670, D Real Loss: 101.4933, Wasserstein Loss: -100.8412, Content Loss: 0.0002, PSNR: 38.07
Epoch [238 / 300]
===> Epoch [238 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0350, D Fake Loss: 100.6177, D Real Loss: 101.2375, Wasserstein Loss: -100.6098, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [238 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 100.2763, D Real Loss: 100.8889, Wasserstein Loss: -100.2356, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [238 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.3731, D Real Loss: 100.9977, Wasserstein Loss: -100.3329, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [238 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 100.4298, D Real Loss: 101.0501, Wasserstein Loss: -100.4085, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [238 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.5162, D Real Loss: 101.1472, Wasserstein Loss: -100.4911, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [238 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.5439, D Real Loss: 101.1768, Wasserstein Loss: -100.5180, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [238 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.4159, D Real Loss: 101.0466, Wasserstein Loss: -100.3923, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [238 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.5772, D Real Loss: 101.2065, Wasserstein Loss: -100.5516, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [238 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.4606, D Real Loss: 101.0867, Wasserstein Loss: -100.4314, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [238 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.4045, D Real Loss: 101.0298, Wasserstein Loss: -100.3828, Content Loss: 0.0002, PSNR: 38.05
Epoch [238 / 300]: Gradient Penalty: 0.0386, D Fake Loss: 100.4460, D Real Loss: 101.0721, Wasserstein Loss: -100.4258, Content Loss: 0.0002, PSNR: 38.04
Epoch [239 / 300]
===> Epoch [239 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 102.0637, D Real Loss: 102.7115, Wasserstein Loss: -102.0215, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [239 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0409, D Fake Loss: 101.2097, D Real Loss: 101.8558, Wasserstein Loss: -101.1524, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [239 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 101.1706, D Real Loss: 101.8187, Wasserstein Loss: -101.1252, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [239 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 101.0361, D Real Loss: 101.6802, Wasserstein Loss: -100.9973, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [239 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 101.4499, D Real Loss: 102.0957, Wasserstein Loss: -101.4146, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [239 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.3788, D Real Loss: 102.0197, Wasserstein Loss: -101.3412, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [239 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 101.1578, D Real Loss: 101.7989, Wasserstein Loss: -101.1275, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [239 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.0792, D Real Loss: 101.7158, Wasserstein Loss: -101.0459, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [239 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.1043, D Real Loss: 101.7425, Wasserstein Loss: -101.0703, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [239 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.1426, D Real Loss: 101.7796, Wasserstein Loss: -101.1112, Content Loss: 0.0002, PSNR: 37.98
Epoch [239 / 300]: Gradient Penalty: 0.0398, D Fake Loss: 101.1493, D Real Loss: 101.7863, Wasserstein Loss: -101.1177, Content Loss: 0.0002, PSNR: 37.98
Epoch [240 / 300]
===> Epoch [240 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.1725, D Real Loss: 101.7980, Wasserstein Loss: -101.1186, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [240 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.9531, D Real Loss: 101.5757, Wasserstein Loss: -100.9194, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [240 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 100.9922, D Real Loss: 101.6178, Wasserstein Loss: -100.9623, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [240 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.0689, D Real Loss: 101.6980, Wasserstein Loss: -101.0307, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [240 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.0269, D Real Loss: 101.6614, Wasserstein Loss: -100.9969, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [240 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.1053, D Real Loss: 101.7421, Wasserstein Loss: -101.0753, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [240 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.1141, D Real Loss: 101.7499, Wasserstein Loss: -101.0892, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [240 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 100.9940, D Real Loss: 101.6285, Wasserstein Loss: -100.9595, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [240 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.9015, D Real Loss: 101.5327, Wasserstein Loss: -100.8773, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [240 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.1211, D Real Loss: 101.7519, Wasserstein Loss: -101.0971, Content Loss: 0.0002, PSNR: 38.03
Epoch [240 / 300]: Gradient Penalty: 0.0386, D Fake Loss: 101.1328, D Real Loss: 101.7632, Wasserstein Loss: -101.1088, Content Loss: 0.0002, PSNR: 38.04
Epoch [241 / 300]
===> Epoch [241 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.9975, D Real Loss: 101.6323, Wasserstein Loss: -100.9123, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [241 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 100.1535, D Real Loss: 100.7891, Wasserstein Loss: -100.1157, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [241 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 100.7639, D Real Loss: 101.3941, Wasserstein Loss: -100.7372, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [241 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.2382, D Real Loss: 101.8669, Wasserstein Loss: -101.2131, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [241 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.1695, D Real Loss: 101.7962, Wasserstein Loss: -101.1542, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [241 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.2671, D Real Loss: 101.8984, Wasserstein Loss: -101.2404, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [241 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.3760, D Real Loss: 102.0039, Wasserstein Loss: -101.3486, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [241 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.2035, D Real Loss: 101.8320, Wasserstein Loss: -101.1809, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [241 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.0968, D Real Loss: 101.7229, Wasserstein Loss: -101.0725, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [241 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.1117, D Real Loss: 101.7416, Wasserstein Loss: -101.0854, Content Loss: 0.0002, PSNR: 38.14
Epoch [241 / 300]: Gradient Penalty: 0.0379, D Fake Loss: 101.0969, D Real Loss: 101.7265, Wasserstein Loss: -101.0680, Content Loss: 0.0002, PSNR: 38.14
Epoch [242 / 300]
===> Epoch [242 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0444, D Fake Loss: 100.1153, D Real Loss: 100.7767, Wasserstein Loss: -100.1051, Content Loss: 0.0002, PSNR: 37.81
===> Epoch [242 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 100.5604, D Real Loss: 101.2050, Wasserstein Loss: -100.5643, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [242 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.0226, D Real Loss: 101.6615, Wasserstein Loss: -101.0078, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [242 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.7723, D Real Loss: 101.4070, Wasserstein Loss: -100.7564, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [242 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 100.5177, D Real Loss: 101.1486, Wasserstein Loss: -100.4983, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [242 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.4945, D Real Loss: 101.1253, Wasserstein Loss: -100.4771, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [242 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.4306, D Real Loss: 101.0634, Wasserstein Loss: -100.4100, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [242 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 100.4575, D Real Loss: 101.0946, Wasserstein Loss: -100.4367, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [242 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 100.4078, D Real Loss: 101.0417, Wasserstein Loss: -100.3844, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [242 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 100.3311, D Real Loss: 100.9623, Wasserstein Loss: -100.3082, Content Loss: 0.0002, PSNR: 38.09
Epoch [242 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 100.3213, D Real Loss: 100.9525, Wasserstein Loss: -100.3004, Content Loss: 0.0002, PSNR: 38.09
Epoch [243 / 300]
===> Epoch [243 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0452, D Fake Loss: 99.6354, D Real Loss: 100.3545, Wasserstein Loss: -99.6125, Content Loss: 0.0002, PSNR: 37.71
===> Epoch [243 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0416, D Fake Loss: 99.6910, D Real Loss: 100.3621, Wasserstein Loss: -99.6664, Content Loss: 0.0002, PSNR: 37.88
===> Epoch [243 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 100.0984, D Real Loss: 100.7530, Wasserstein Loss: -100.0545, Content Loss: 0.0002, PSNR: 37.91
===> Epoch [243 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.1087, D Real Loss: 100.7568, Wasserstein Loss: -100.0805, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [243 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.2824, D Real Loss: 100.9226, Wasserstein Loss: -100.2485, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [243 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 100.1440, D Real Loss: 100.7850, Wasserstein Loss: -100.1153, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [243 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 100.2264, D Real Loss: 100.8663, Wasserstein Loss: -100.2006, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [243 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 100.3314, D Real Loss: 100.9712, Wasserstein Loss: -100.3015, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [243 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 100.2248, D Real Loss: 100.8619, Wasserstein Loss: -100.2031, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [243 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.2660, D Real Loss: 100.9041, Wasserstein Loss: -100.2416, Content Loss: 0.0002, PSNR: 38.06
Epoch [243 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 100.2673, D Real Loss: 100.9051, Wasserstein Loss: -100.2427, Content Loss: 0.0002, PSNR: 38.06
Epoch [244 / 300]
===> Epoch [244 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.0600, D Real Loss: 100.6924, Wasserstein Loss: -100.0514, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [244 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0403, D Fake Loss: 101.0868, D Real Loss: 101.7219, Wasserstein Loss: -101.0577, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [244 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0408, D Fake Loss: 100.9809, D Real Loss: 101.6195, Wasserstein Loss: -100.9512, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [244 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 101.0225, D Real Loss: 101.6644, Wasserstein Loss: -100.9945, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [244 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 101.0044, D Real Loss: 101.6423, Wasserstein Loss: -100.9838, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [244 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.1439, D Real Loss: 101.7749, Wasserstein Loss: -101.1102, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [244 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 100.9916, D Real Loss: 101.6194, Wasserstein Loss: -100.9622, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [244 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.0291, D Real Loss: 101.6579, Wasserstein Loss: -101.0035, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [244 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.0803, D Real Loss: 101.7103, Wasserstein Loss: -101.0550, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [244 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 100.9544, D Real Loss: 101.5830, Wasserstein Loss: -100.9295, Content Loss: 0.0002, PSNR: 38.07
Epoch [244 / 300]: Gradient Penalty: 0.0397, D Fake Loss: 100.9575, D Real Loss: 101.5859, Wasserstein Loss: -100.9297, Content Loss: 0.0002, PSNR: 38.06
Epoch [245 / 300]
===> Epoch [245 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 100.8111, D Real Loss: 101.4249, Wasserstein Loss: -100.7564, Content Loss: 0.0002, PSNR: 37.81
===> Epoch [245 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 100.7085, D Real Loss: 101.3400, Wasserstein Loss: -100.6702, Content Loss: 0.0002, PSNR: 37.74
===> Epoch [245 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 100.1259, D Real Loss: 100.7546, Wasserstein Loss: -100.0875, Content Loss: 0.0002, PSNR: 37.84
===> Epoch [245 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.3890, D Real Loss: 101.0124, Wasserstein Loss: -100.3678, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [245 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.4759, D Real Loss: 101.1004, Wasserstein Loss: -100.4460, Content Loss: 0.0002, PSNR: 37.93
===> Epoch [245 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.5666, D Real Loss: 101.1887, Wasserstein Loss: -100.5432, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [245 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 100.7441, D Real Loss: 101.3673, Wasserstein Loss: -100.7272, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [245 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.7667, D Real Loss: 101.3881, Wasserstein Loss: -100.7404, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [245 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 100.5650, D Real Loss: 101.1840, Wasserstein Loss: -100.5412, Content Loss: 0.0002, PSNR: 38.02
===> Epoch [245 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 100.5676, D Real Loss: 101.1882, Wasserstein Loss: -100.5421, Content Loss: 0.0002, PSNR: 38.03
Epoch [245 / 300]: Gradient Penalty: 0.0375, D Fake Loss: 100.5620, D Real Loss: 101.1835, Wasserstein Loss: -100.5351, Content Loss: 0.0002, PSNR: 38.03
Epoch [246 / 300]
===> Epoch [246 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.3185, D Real Loss: 100.9511, Wasserstein Loss: -100.2711, Content Loss: 0.0002, PSNR: 37.82
===> Epoch [246 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 101.1454, D Real Loss: 101.7796, Wasserstein Loss: -101.1374, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [246 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.0039, D Real Loss: 101.6346, Wasserstein Loss: -100.9854, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [246 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.2949, D Real Loss: 101.9210, Wasserstein Loss: -101.2770, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [246 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.2063, D Real Loss: 101.8296, Wasserstein Loss: -101.1918, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [246 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.3261, D Real Loss: 101.9512, Wasserstein Loss: -101.3094, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [246 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.0882, D Real Loss: 101.7111, Wasserstein Loss: -101.0611, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [246 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 101.0775, D Real Loss: 101.7035, Wasserstein Loss: -101.0554, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [246 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.9583, D Real Loss: 101.5862, Wasserstein Loss: -100.9387, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [246 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.0447, D Real Loss: 101.6736, Wasserstein Loss: -101.0224, Content Loss: 0.0002, PSNR: 38.13
Epoch [246 / 300]: Gradient Penalty: 0.0389, D Fake Loss: 101.0310, D Real Loss: 101.6603, Wasserstein Loss: -101.0080, Content Loss: 0.0002, PSNR: 38.13
Epoch [247 / 300]
===> Epoch [247 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.8560, D Real Loss: 101.4794, Wasserstein Loss: -100.8401, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [247 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 101.7113, D Real Loss: 102.3664, Wasserstein Loss: -101.7121, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [247 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 101.1299, D Real Loss: 101.7838, Wasserstein Loss: -101.1118, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [247 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0424, D Fake Loss: 101.3407, D Real Loss: 101.9934, Wasserstein Loss: -101.3169, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [247 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0420, D Fake Loss: 101.3756, D Real Loss: 102.0207, Wasserstein Loss: -101.3551, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [247 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0419, D Fake Loss: 101.2809, D Real Loss: 101.9266, Wasserstein Loss: -101.2562, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [247 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 101.2145, D Real Loss: 101.8586, Wasserstein Loss: -101.1929, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [247 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0413, D Fake Loss: 101.3636, D Real Loss: 102.0095, Wasserstein Loss: -101.3442, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [247 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0412, D Fake Loss: 101.2561, D Real Loss: 101.9001, Wasserstein Loss: -101.2288, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [247 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 101.1328, D Real Loss: 101.7733, Wasserstein Loss: -101.1060, Content Loss: 0.0002, PSNR: 38.01
Epoch [247 / 300]: Gradient Penalty: 0.0406, D Fake Loss: 101.1310, D Real Loss: 101.7704, Wasserstein Loss: -101.1055, Content Loss: 0.0002, PSNR: 38.01
Epoch [248 / 300]
===> Epoch [248 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.8948, D Real Loss: 102.5108, Wasserstein Loss: -101.9171, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [248 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.0056, D Real Loss: 101.6154, Wasserstein Loss: -100.9699, Content Loss: 0.0002, PSNR: 38.28
===> Epoch [248 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 100.6951, D Real Loss: 101.3186, Wasserstein Loss: -100.6758, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [248 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.0161, D Real Loss: 101.6432, Wasserstein Loss: -100.9863, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [248 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 100.9403, D Real Loss: 101.5622, Wasserstein Loss: -100.9175, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [248 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 100.8948, D Real Loss: 101.5155, Wasserstein Loss: -100.8715, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [248 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 100.9506, D Real Loss: 101.5756, Wasserstein Loss: -100.9252, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [248 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 100.8619, D Real Loss: 101.4872, Wasserstein Loss: -100.8357, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [248 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 100.7144, D Real Loss: 101.3392, Wasserstein Loss: -100.6875, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [248 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 100.6888, D Real Loss: 101.3155, Wasserstein Loss: -100.6654, Content Loss: 0.0002, PSNR: 38.12
Epoch [248 / 300]: Gradient Penalty: 0.0374, D Fake Loss: 100.6906, D Real Loss: 101.3170, Wasserstein Loss: -100.6660, Content Loss: 0.0002, PSNR: 38.12
Epoch [249 / 300]
===> Epoch [249 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0430, D Fake Loss: 102.0586, D Real Loss: 102.7430, Wasserstein Loss: -102.0036, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [249 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.3188, D Real Loss: 101.9740, Wasserstein Loss: -101.3050, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [249 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0401, D Fake Loss: 101.0469, D Real Loss: 101.7005, Wasserstein Loss: -101.0267, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [249 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.8996, D Real Loss: 101.5410, Wasserstein Loss: -100.8795, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [249 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.9395, D Real Loss: 101.5776, Wasserstein Loss: -100.9154, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [249 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 100.9720, D Real Loss: 101.6065, Wasserstein Loss: -100.9516, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [249 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 100.9373, D Real Loss: 101.5689, Wasserstein Loss: -100.9193, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [249 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.1237, D Real Loss: 101.7573, Wasserstein Loss: -101.0967, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [249 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.2389, D Real Loss: 101.8712, Wasserstein Loss: -101.2156, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [249 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.2775, D Real Loss: 101.9096, Wasserstein Loss: -101.2530, Content Loss: 0.0002, PSNR: 38.13
Epoch [249 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 101.2719, D Real Loss: 101.9041, Wasserstein Loss: -101.2477, Content Loss: 0.0002, PSNR: 38.13
Epoch [250 / 300]
===> Epoch [250 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.9933, D Real Loss: 102.6270, Wasserstein Loss: -101.9870, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [250 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.0835, D Real Loss: 101.7114, Wasserstein Loss: -101.0636, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [250 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 101.2907, D Real Loss: 101.9168, Wasserstein Loss: -101.2662, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [250 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.2729, D Real Loss: 101.8999, Wasserstein Loss: -101.2553, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [250 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.3076, D Real Loss: 101.9393, Wasserstein Loss: -101.2850, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [250 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.3959, D Real Loss: 102.0261, Wasserstein Loss: -101.3695, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [250 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.3694, D Real Loss: 102.0021, Wasserstein Loss: -101.3438, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [250 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.2689, D Real Loss: 101.9022, Wasserstein Loss: -101.2416, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [250 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.1569, D Real Loss: 101.7900, Wasserstein Loss: -101.1305, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [250 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.1091, D Real Loss: 101.7408, Wasserstein Loss: -101.0833, Content Loss: 0.0002, PSNR: 38.12
Epoch [250 / 300]: Gradient Penalty: 0.0385, D Fake Loss: 101.0867, D Real Loss: 101.7182, Wasserstein Loss: -101.0598, Content Loss: 0.0002, PSNR: 38.12
Epoch [251 / 300]
===> Epoch [251 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 100.4768, D Real Loss: 101.1086, Wasserstein Loss: -100.4664, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [251 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.6659, D Real Loss: 101.3005, Wasserstein Loss: -100.6512, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [251 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.1636, D Real Loss: 101.7890, Wasserstein Loss: -101.1549, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [251 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.4388, D Real Loss: 102.0635, Wasserstein Loss: -101.4247, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [251 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.3156, D Real Loss: 101.9427, Wasserstein Loss: -101.3064, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [251 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.3379, D Real Loss: 101.9610, Wasserstein Loss: -101.3184, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [251 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.2155, D Real Loss: 101.8366, Wasserstein Loss: -101.1956, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [251 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.0964, D Real Loss: 101.7200, Wasserstein Loss: -101.0794, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [251 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.0365, D Real Loss: 101.6589, Wasserstein Loss: -101.0187, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [251 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 100.9817, D Real Loss: 101.6043, Wasserstein Loss: -100.9660, Content Loss: 0.0002, PSNR: 38.15
Epoch [251 / 300]: Gradient Penalty: 0.0370, D Fake Loss: 101.0136, D Real Loss: 101.6366, Wasserstein Loss: -100.9962, Content Loss: 0.0002, PSNR: 38.15
Epoch [252 / 300]
===> Epoch [252 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 101.2942, D Real Loss: 101.9441, Wasserstein Loss: -101.2415, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [252 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.1863, D Real Loss: 101.8198, Wasserstein Loss: -101.1564, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [252 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.1787, D Real Loss: 101.8116, Wasserstein Loss: -101.1250, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [252 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.9848, D Real Loss: 101.6279, Wasserstein Loss: -100.9414, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [252 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 100.9092, D Real Loss: 101.5498, Wasserstein Loss: -100.8864, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [252 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.0124, D Real Loss: 101.6504, Wasserstein Loss: -100.9868, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [252 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.0533, D Real Loss: 101.6876, Wasserstein Loss: -101.0276, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [252 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 100.9522, D Real Loss: 101.5813, Wasserstein Loss: -100.9245, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [252 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 100.8560, D Real Loss: 101.4818, Wasserstein Loss: -100.8281, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [252 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 100.8476, D Real Loss: 101.4733, Wasserstein Loss: -100.8234, Content Loss: 0.0002, PSNR: 38.22
Epoch [252 / 300]: Gradient Penalty: 0.0368, D Fake Loss: 100.8604, D Real Loss: 101.4859, Wasserstein Loss: -100.8370, Content Loss: 0.0002, PSNR: 38.22
Epoch [253 / 300]
===> Epoch [253 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.6007, D Real Loss: 102.2362, Wasserstein Loss: -101.5137, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [253 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 100.4574, D Real Loss: 101.0913, Wasserstein Loss: -100.4236, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [253 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.8771, D Real Loss: 101.5094, Wasserstein Loss: -100.8575, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [253 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.3789, D Real Loss: 102.0239, Wasserstein Loss: -101.3537, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [253 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 101.5692, D Real Loss: 102.2216, Wasserstein Loss: -101.5354, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [253 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 101.3999, D Real Loss: 102.0456, Wasserstein Loss: -101.3672, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [253 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.2038, D Real Loss: 101.8458, Wasserstein Loss: -101.1771, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [253 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.2725, D Real Loss: 101.9131, Wasserstein Loss: -101.2416, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [253 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 101.1954, D Real Loss: 101.8372, Wasserstein Loss: -101.1640, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [253 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.0552, D Real Loss: 101.6948, Wasserstein Loss: -101.0280, Content Loss: 0.0002, PSNR: 38.07
Epoch [253 / 300]: Gradient Penalty: 0.0393, D Fake Loss: 101.0628, D Real Loss: 101.7016, Wasserstein Loss: -101.0360, Content Loss: 0.0002, PSNR: 38.07
Epoch [254 / 300]
===> Epoch [254 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0419, D Fake Loss: 101.4494, D Real Loss: 102.0920, Wasserstein Loss: -101.3863, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [254 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 100.8754, D Real Loss: 101.4956, Wasserstein Loss: -100.8364, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [254 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 100.5895, D Real Loss: 101.2111, Wasserstein Loss: -100.5767, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [254 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.5938, D Real Loss: 101.2139, Wasserstein Loss: -100.5731, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [254 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.7814, D Real Loss: 101.3992, Wasserstein Loss: -100.7551, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [254 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 100.9176, D Real Loss: 101.5453, Wasserstein Loss: -100.8939, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [254 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.8219, D Real Loss: 101.4471, Wasserstein Loss: -100.8027, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [254 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.9741, D Real Loss: 101.6004, Wasserstein Loss: -100.9495, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [254 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.0356, D Real Loss: 101.6623, Wasserstein Loss: -101.0122, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [254 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.9388, D Real Loss: 101.5629, Wasserstein Loss: -100.9128, Content Loss: 0.0002, PSNR: 38.18
Epoch [254 / 300]: Gradient Penalty: 0.0381, D Fake Loss: 100.9293, D Real Loss: 101.5531, Wasserstein Loss: -100.9057, Content Loss: 0.0002, PSNR: 38.18
Epoch [255 / 300]
===> Epoch [255 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 101.4213, D Real Loss: 102.0708, Wasserstein Loss: -101.4294, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [255 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.4872, D Real Loss: 102.1229, Wasserstein Loss: -101.4586, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [255 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.5509, D Real Loss: 102.1882, Wasserstein Loss: -101.5291, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [255 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 101.4206, D Real Loss: 102.0553, Wasserstein Loss: -101.3991, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [255 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0404, D Fake Loss: 101.3978, D Real Loss: 102.0380, Wasserstein Loss: -101.3797, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [255 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.2226, D Real Loss: 101.8577, Wasserstein Loss: -101.1936, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [255 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 101.3232, D Real Loss: 101.9594, Wasserstein Loss: -101.2984, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [255 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0403, D Fake Loss: 101.2997, D Real Loss: 101.9332, Wasserstein Loss: -101.2709, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [255 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.2955, D Real Loss: 101.9259, Wasserstein Loss: -101.2719, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [255 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.3807, D Real Loss: 102.0107, Wasserstein Loss: -101.3587, Content Loss: 0.0002, PSNR: 38.04
Epoch [255 / 300]: Gradient Penalty: 0.0397, D Fake Loss: 101.4229, D Real Loss: 102.0541, Wasserstein Loss: -101.4021, Content Loss: 0.0002, PSNR: 38.05
Epoch [256 / 300]
===> Epoch [256 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.0193, D Real Loss: 101.6456, Wasserstein Loss: -100.9141, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [256 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.8065, D Real Loss: 101.4425, Wasserstein Loss: -100.7475, Content Loss: 0.0002, PSNR: 37.90
===> Epoch [256 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 100.7435, D Real Loss: 101.3747, Wasserstein Loss: -100.6952, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [256 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.0940, D Real Loss: 101.7237, Wasserstein Loss: -101.0628, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [256 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.1176, D Real Loss: 101.7462, Wasserstein Loss: -101.0766, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [256 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.1753, D Real Loss: 101.8009, Wasserstein Loss: -101.1462, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [256 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.1048, D Real Loss: 101.7320, Wasserstein Loss: -101.0703, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [256 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.1858, D Real Loss: 101.8147, Wasserstein Loss: -101.1548, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [256 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.4195, D Real Loss: 102.0466, Wasserstein Loss: -101.3868, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [256 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.3094, D Real Loss: 101.9352, Wasserstein Loss: -101.2798, Content Loss: 0.0002, PSNR: 38.18
Epoch [256 / 300]: Gradient Penalty: 0.0371, D Fake Loss: 101.3280, D Real Loss: 101.9532, Wasserstein Loss: -101.3013, Content Loss: 0.0002, PSNR: 38.18
Epoch [257 / 300]
===> Epoch [257 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.7443, D Real Loss: 102.3985, Wasserstein Loss: -101.6910, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [257 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.8305, D Real Loss: 102.4846, Wasserstein Loss: -101.7935, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [257 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.5731, D Real Loss: 102.2169, Wasserstein Loss: -101.5381, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [257 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.4152, D Real Loss: 102.0546, Wasserstein Loss: -101.3897, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [257 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.3278, D Real Loss: 101.9642, Wasserstein Loss: -101.2922, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [257 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 100.9994, D Real Loss: 101.6330, Wasserstein Loss: -100.9692, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [257 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 100.9653, D Real Loss: 101.6032, Wasserstein Loss: -100.9354, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [257 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 100.8841, D Real Loss: 101.5170, Wasserstein Loss: -100.8550, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [257 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 100.9406, D Real Loss: 101.5740, Wasserstein Loss: -100.9127, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [257 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.9043, D Real Loss: 101.5405, Wasserstein Loss: -100.8742, Content Loss: 0.0002, PSNR: 38.12
Epoch [257 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 100.8922, D Real Loss: 101.5279, Wasserstein Loss: -100.8623, Content Loss: 0.0002, PSNR: 38.12
Epoch [258 / 300]
===> Epoch [258 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 100.5503, D Real Loss: 101.1797, Wasserstein Loss: -100.5186, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [258 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 100.7999, D Real Loss: 101.4359, Wasserstein Loss: -100.7792, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [258 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 100.9432, D Real Loss: 101.5747, Wasserstein Loss: -100.9251, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [258 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 100.8974, D Real Loss: 101.5305, Wasserstein Loss: -100.8837, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [258 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.2747, D Real Loss: 101.9094, Wasserstein Loss: -101.2679, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [258 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.4058, D Real Loss: 102.0386, Wasserstein Loss: -101.3834, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [258 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.2900, D Real Loss: 101.9213, Wasserstein Loss: -101.2693, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [258 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.4155, D Real Loss: 102.0452, Wasserstein Loss: -101.3927, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [258 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.2895, D Real Loss: 101.9185, Wasserstein Loss: -101.2678, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [258 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.2320, D Real Loss: 101.8628, Wasserstein Loss: -101.2043, Content Loss: 0.0002, PSNR: 38.15
Epoch [258 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 101.1857, D Real Loss: 101.8166, Wasserstein Loss: -101.1565, Content Loss: 0.0002, PSNR: 38.13
Epoch [259 / 300]
===> Epoch [259 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0403, D Fake Loss: 101.0051, D Real Loss: 101.6229, Wasserstein Loss: -100.9843, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [259 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.5213, D Real Loss: 102.1454, Wasserstein Loss: -101.5126, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [259 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.1024, D Real Loss: 101.7186, Wasserstein Loss: -101.1086, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [259 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.0668, D Real Loss: 101.6948, Wasserstein Loss: -101.0562, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [259 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.0154, D Real Loss: 101.6435, Wasserstein Loss: -101.0060, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [259 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.2102, D Real Loss: 101.8414, Wasserstein Loss: -101.1979, Content Loss: 0.0002, PSNR: 37.99
===> Epoch [259 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.1377, D Real Loss: 101.7677, Wasserstein Loss: -101.1173, Content Loss: 0.0002, PSNR: 38.00
===> Epoch [259 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.0154, D Real Loss: 101.6422, Wasserstein Loss: -100.9956, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [259 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.0999, D Real Loss: 101.7272, Wasserstein Loss: -101.0844, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [259 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.2576, D Real Loss: 101.8883, Wasserstein Loss: -101.2356, Content Loss: 0.0002, PSNR: 38.07
Epoch [259 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 101.2345, D Real Loss: 101.8658, Wasserstein Loss: -101.2125, Content Loss: 0.0002, PSNR: 38.07
Epoch [260 / 300]
===> Epoch [260 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 100.7362, D Real Loss: 101.3698, Wasserstein Loss: -100.7494, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [260 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.5995, D Real Loss: 102.2346, Wasserstein Loss: -101.5929, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [260 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.5694, D Real Loss: 102.2067, Wasserstein Loss: -101.5589, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [260 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.5494, D Real Loss: 102.1860, Wasserstein Loss: -101.5253, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [260 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.4418, D Real Loss: 102.0773, Wasserstein Loss: -101.4261, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [260 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.5930, D Real Loss: 102.2258, Wasserstein Loss: -101.5773, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [260 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.5743, D Real Loss: 102.2073, Wasserstein Loss: -101.5596, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [260 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.4283, D Real Loss: 102.0654, Wasserstein Loss: -101.4105, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [260 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.4559, D Real Loss: 102.0923, Wasserstein Loss: -101.4357, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [260 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.4068, D Real Loss: 102.0402, Wasserstein Loss: -101.3851, Content Loss: 0.0002, PSNR: 38.16
Epoch [260 / 300]: Gradient Penalty: 0.0380, D Fake Loss: 101.3938, D Real Loss: 102.0270, Wasserstein Loss: -101.3711, Content Loss: 0.0002, PSNR: 38.16
Epoch [261 / 300]
===> Epoch [261 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 100.7760, D Real Loss: 101.4384, Wasserstein Loss: -100.7174, Content Loss: 0.0002, PSNR: 37.96
===> Epoch [261 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 100.8147, D Real Loss: 101.4741, Wasserstein Loss: -100.8013, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [261 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 100.7660, D Real Loss: 101.4169, Wasserstein Loss: -100.7396, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [261 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.7785, D Real Loss: 101.4267, Wasserstein Loss: -100.7623, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [261 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.7445, D Real Loss: 101.3866, Wasserstein Loss: -100.7274, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [261 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 100.8846, D Real Loss: 101.5255, Wasserstein Loss: -100.8663, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [261 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.8510, D Real Loss: 101.4890, Wasserstein Loss: -100.8316, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [261 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 100.6620, D Real Loss: 101.2953, Wasserstein Loss: -100.6408, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [261 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 100.7139, D Real Loss: 101.3454, Wasserstein Loss: -100.6892, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [261 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 100.7210, D Real Loss: 101.3540, Wasserstein Loss: -100.6917, Content Loss: 0.0002, PSNR: 38.14
Epoch [261 / 300]: Gradient Penalty: 0.0386, D Fake Loss: 100.6916, D Real Loss: 101.3242, Wasserstein Loss: -100.6660, Content Loss: 0.0002, PSNR: 38.14
Epoch [262 / 300]
===> Epoch [262 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0357, D Fake Loss: 102.6955, D Real Loss: 103.3171, Wasserstein Loss: -102.6772, Content Loss: 0.0002, PSNR: 38.28
===> Epoch [262 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0356, D Fake Loss: 102.3057, D Real Loss: 102.9311, Wasserstein Loss: -102.2943, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [262 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0357, D Fake Loss: 101.8751, D Real Loss: 102.5039, Wasserstein Loss: -101.8538, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [262 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.8301, D Real Loss: 102.4665, Wasserstein Loss: -101.8154, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [262 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.4241, D Real Loss: 102.0584, Wasserstein Loss: -101.3979, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [262 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.1462, D Real Loss: 101.7864, Wasserstein Loss: -101.1204, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [262 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.1569, D Real Loss: 101.7974, Wasserstein Loss: -101.1402, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [262 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.4010, D Real Loss: 102.0395, Wasserstein Loss: -101.3822, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [262 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.5298, D Real Loss: 102.1670, Wasserstein Loss: -101.5081, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [262 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.4234, D Real Loss: 102.0579, Wasserstein Loss: -101.4028, Content Loss: 0.0002, PSNR: 38.20
Epoch [262 / 300]: Gradient Penalty: 0.0381, D Fake Loss: 101.4196, D Real Loss: 102.0536, Wasserstein Loss: -101.3985, Content Loss: 0.0002, PSNR: 38.20
Epoch [263 / 300]
===> Epoch [263 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.9456, D Real Loss: 102.5811, Wasserstein Loss: -101.9113, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [263 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0423, D Fake Loss: 101.2025, D Real Loss: 101.8532, Wasserstein Loss: -101.1742, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [263 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0415, D Fake Loss: 101.4671, D Real Loss: 102.1186, Wasserstein Loss: -101.4483, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [263 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 101.5213, D Real Loss: 102.1617, Wasserstein Loss: -101.5056, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [263 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.5462, D Real Loss: 102.1823, Wasserstein Loss: -101.5232, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [263 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.5095, D Real Loss: 102.1390, Wasserstein Loss: -101.4932, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [263 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.5515, D Real Loss: 102.1834, Wasserstein Loss: -101.5247, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [263 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.4371, D Real Loss: 102.0678, Wasserstein Loss: -101.4117, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [263 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 101.2644, D Real Loss: 101.8947, Wasserstein Loss: -101.2372, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [263 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.1825, D Real Loss: 101.8115, Wasserstein Loss: -101.1633, Content Loss: 0.0002, PSNR: 38.17
Epoch [263 / 300]: Gradient Penalty: 0.0379, D Fake Loss: 101.2152, D Real Loss: 101.8436, Wasserstein Loss: -101.1969, Content Loss: 0.0002, PSNR: 38.17
Epoch [264 / 300]
===> Epoch [264 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 101.3331, D Real Loss: 101.9544, Wasserstein Loss: -101.2491, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [264 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 101.5584, D Real Loss: 102.1788, Wasserstein Loss: -101.5021, Content Loss: 0.0002, PSNR: 38.33
===> Epoch [264 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.0308, D Real Loss: 101.6654, Wasserstein Loss: -100.9962, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [264 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.2297, D Real Loss: 101.8674, Wasserstein Loss: -101.2025, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [264 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.5164, D Real Loss: 102.1568, Wasserstein Loss: -101.4838, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [264 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.4370, D Real Loss: 102.0776, Wasserstein Loss: -101.4106, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [264 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 101.5473, D Real Loss: 102.1880, Wasserstein Loss: -101.5122, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [264 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.5087, D Real Loss: 102.1485, Wasserstein Loss: -101.4781, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [264 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.3896, D Real Loss: 102.0247, Wasserstein Loss: -101.3585, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [264 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.2644, D Real Loss: 101.8986, Wasserstein Loss: -101.2301, Content Loss: 0.0002, PSNR: 38.18
Epoch [264 / 300]: Gradient Penalty: 0.0385, D Fake Loss: 101.2186, D Real Loss: 101.8522, Wasserstein Loss: -101.1832, Content Loss: 0.0002, PSNR: 38.18
Epoch [265 / 300]
===> Epoch [265 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 100.4196, D Real Loss: 101.0590, Wasserstein Loss: -100.4762, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [265 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.0452, D Real Loss: 101.6816, Wasserstein Loss: -101.0708, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [265 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.4094, D Real Loss: 102.0545, Wasserstein Loss: -101.4055, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [265 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.5202, D Real Loss: 102.1630, Wasserstein Loss: -101.5103, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [265 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.4104, D Real Loss: 102.0486, Wasserstein Loss: -101.4013, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [265 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.4947, D Real Loss: 102.1324, Wasserstein Loss: -101.4808, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [265 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.3884, D Real Loss: 102.0233, Wasserstein Loss: -101.3727, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [265 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.3133, D Real Loss: 101.9434, Wasserstein Loss: -101.3005, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [265 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.3727, D Real Loss: 102.0035, Wasserstein Loss: -101.3554, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [265 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 101.3108, D Real Loss: 101.9425, Wasserstein Loss: -101.2969, Content Loss: 0.0002, PSNR: 38.17
Epoch [265 / 300]: Gradient Penalty: 0.0380, D Fake Loss: 101.3507, D Real Loss: 101.9817, Wasserstein Loss: -101.3392, Content Loss: 0.0002, PSNR: 38.17
Epoch [266 / 300]
===> Epoch [266 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 100.9162, D Real Loss: 101.5430, Wasserstein Loss: -100.8271, Content Loss: 0.0002, PSNR: 37.79
===> Epoch [266 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.0818, D Real Loss: 101.7129, Wasserstein Loss: -101.0066, Content Loss: 0.0002, PSNR: 37.75
===> Epoch [266 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 100.9592, D Real Loss: 101.5855, Wasserstein Loss: -100.9072, Content Loss: 0.0002, PSNR: 37.89
===> Epoch [266 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.0833, D Real Loss: 101.7127, Wasserstein Loss: -101.0323, Content Loss: 0.0002, PSNR: 37.95
===> Epoch [266 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.2042, D Real Loss: 101.8374, Wasserstein Loss: -101.1646, Content Loss: 0.0002, PSNR: 37.98
===> Epoch [266 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.3417, D Real Loss: 101.9751, Wasserstein Loss: -101.2996, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [266 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.3562, D Real Loss: 101.9910, Wasserstein Loss: -101.3207, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [266 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.3603, D Real Loss: 101.9921, Wasserstein Loss: -101.3244, Content Loss: 0.0002, PSNR: 38.03
===> Epoch [266 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.1658, D Real Loss: 101.7933, Wasserstein Loss: -101.1274, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [266 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.1861, D Real Loss: 101.8178, Wasserstein Loss: -101.1488, Content Loss: 0.0002, PSNR: 38.06
Epoch [266 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 101.1608, D Real Loss: 101.7926, Wasserstein Loss: -101.1249, Content Loss: 0.0002, PSNR: 38.06
Epoch [267 / 300]
===> Epoch [267 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 102.0901, D Real Loss: 102.7314, Wasserstein Loss: -102.1186, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [267 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 102.2934, D Real Loss: 102.9421, Wasserstein Loss: -102.2775, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [267 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.8718, D Real Loss: 102.5118, Wasserstein Loss: -101.8536, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [267 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.6042, D Real Loss: 102.2417, Wasserstein Loss: -101.6016, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [267 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.4282, D Real Loss: 102.0644, Wasserstein Loss: -101.4093, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [267 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.4403, D Real Loss: 102.0767, Wasserstein Loss: -101.4226, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [267 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.4417, D Real Loss: 102.0771, Wasserstein Loss: -101.4251, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [267 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.6126, D Real Loss: 102.2518, Wasserstein Loss: -101.5907, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [267 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.5278, D Real Loss: 102.1649, Wasserstein Loss: -101.5069, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [267 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.5947, D Real Loss: 102.2314, Wasserstein Loss: -101.5720, Content Loss: 0.0002, PSNR: 38.18
Epoch [267 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 101.5736, D Real Loss: 102.2095, Wasserstein Loss: -101.5499, Content Loss: 0.0002, PSNR: 38.18
Epoch [268 / 300]
===> Epoch [268 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0356, D Fake Loss: 102.2122, D Real Loss: 102.8430, Wasserstein Loss: -102.1855, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [268 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 101.0207, D Real Loss: 101.6531, Wasserstein Loss: -101.0168, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [268 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.9839, D Real Loss: 102.6224, Wasserstein Loss: -101.9525, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [268 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.4903, D Real Loss: 102.1281, Wasserstein Loss: -101.4635, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [268 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.2980, D Real Loss: 101.9318, Wasserstein Loss: -101.2826, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [268 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 101.4533, D Real Loss: 102.0908, Wasserstein Loss: -101.4324, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [268 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.6397, D Real Loss: 102.2780, Wasserstein Loss: -101.6181, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [268 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.5127, D Real Loss: 102.1471, Wasserstein Loss: -101.4932, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [268 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.6483, D Real Loss: 102.2812, Wasserstein Loss: -101.6292, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [268 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.4326, D Real Loss: 102.0646, Wasserstein Loss: -101.4127, Content Loss: 0.0002, PSNR: 38.20
Epoch [268 / 300]: Gradient Penalty: 0.0379, D Fake Loss: 101.4485, D Real Loss: 102.0801, Wasserstein Loss: -101.4289, Content Loss: 0.0002, PSNR: 38.20
Epoch [269 / 300]
===> Epoch [269 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 101.2192, D Real Loss: 101.8508, Wasserstein Loss: -101.1906, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [269 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.2527, D Real Loss: 101.8803, Wasserstein Loss: -101.2148, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [269 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.2676, D Real Loss: 101.9079, Wasserstein Loss: -101.2358, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [269 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.4055, D Real Loss: 102.0470, Wasserstein Loss: -101.3854, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [269 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 101.1670, D Real Loss: 101.8054, Wasserstein Loss: -101.1302, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [269 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.0564, D Real Loss: 101.6941, Wasserstein Loss: -101.0321, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [269 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.0165, D Real Loss: 101.6524, Wasserstein Loss: -100.9877, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [269 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.1309, D Real Loss: 101.7639, Wasserstein Loss: -101.1040, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [269 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.1166, D Real Loss: 101.7449, Wasserstein Loss: -101.0866, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [269 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.1192, D Real Loss: 101.7472, Wasserstein Loss: -101.0906, Content Loss: 0.0002, PSNR: 38.19
Epoch [269 / 300]: Gradient Penalty: 0.0372, D Fake Loss: 101.1076, D Real Loss: 101.7358, Wasserstein Loss: -101.0824, Content Loss: 0.0002, PSNR: 38.19
Epoch [270 / 300]
===> Epoch [270 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0381, D Fake Loss: 102.4418, D Real Loss: 103.0756, Wasserstein Loss: -102.4085, Content Loss: 0.0002, PSNR: 38.34
===> Epoch [270 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.8872, D Real Loss: 102.5196, Wasserstein Loss: -101.8640, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [270 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.8208, D Real Loss: 102.4540, Wasserstein Loss: -101.7983, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [270 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.6217, D Real Loss: 102.2591, Wasserstein Loss: -101.6004, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [270 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.6954, D Real Loss: 102.3300, Wasserstein Loss: -101.6702, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [270 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.6083, D Real Loss: 102.2381, Wasserstein Loss: -101.5815, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [270 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.4627, D Real Loss: 102.0923, Wasserstein Loss: -101.4388, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [270 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.6312, D Real Loss: 102.2621, Wasserstein Loss: -101.6054, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [270 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.4049, D Real Loss: 102.0345, Wasserstein Loss: -101.3781, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [270 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.3880, D Real Loss: 102.0173, Wasserstein Loss: -101.3638, Content Loss: 0.0002, PSNR: 38.24
Epoch [270 / 300]: Gradient Penalty: 0.0377, D Fake Loss: 101.3834, D Real Loss: 102.0120, Wasserstein Loss: -101.3596, Content Loss: 0.0002, PSNR: 38.24
Epoch [271 / 300]
===> Epoch [271 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0411, D Fake Loss: 102.3317, D Real Loss: 102.9849, Wasserstein Loss: -102.3240, Content Loss: 0.0002, PSNR: 38.07
===> Epoch [271 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0395, D Fake Loss: 101.9505, D Real Loss: 102.5924, Wasserstein Loss: -101.9247, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [271 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 102.1539, D Real Loss: 102.7939, Wasserstein Loss: -102.1404, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [271 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 102.2071, D Real Loss: 102.8468, Wasserstein Loss: -102.1795, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [271 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.9877, D Real Loss: 102.6258, Wasserstein Loss: -101.9562, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [271 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 102.0744, D Real Loss: 102.7132, Wasserstein Loss: -102.0459, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [271 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.9490, D Real Loss: 102.5884, Wasserstein Loss: -101.9208, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [271 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.8775, D Real Loss: 102.5161, Wasserstein Loss: -101.8550, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [271 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.7683, D Real Loss: 102.4063, Wasserstein Loss: -101.7465, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [271 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.5739, D Real Loss: 102.2101, Wasserstein Loss: -101.5476, Content Loss: 0.0002, PSNR: 38.16
Epoch [271 / 300]: Gradient Penalty: 0.0387, D Fake Loss: 101.5610, D Real Loss: 102.1967, Wasserstein Loss: -101.5355, Content Loss: 0.0002, PSNR: 38.16
Epoch [272 / 300]
===> Epoch [272 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 101.1319, D Real Loss: 101.7791, Wasserstein Loss: -101.1582, Content Loss: 0.0002, PSNR: 38.15
===> Epoch [272 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 101.4819, D Real Loss: 102.1132, Wasserstein Loss: -101.4764, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [272 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.5947, D Real Loss: 102.2268, Wasserstein Loss: -101.5709, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [272 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.4050, D Real Loss: 102.0396, Wasserstein Loss: -101.3690, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [272 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0399, D Fake Loss: 101.3204, D Real Loss: 101.9583, Wasserstein Loss: -101.2966, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [272 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.3269, D Real Loss: 101.9629, Wasserstein Loss: -101.3080, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [272 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 101.3083, D Real Loss: 101.9413, Wasserstein Loss: -101.2864, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [272 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.2915, D Real Loss: 101.9230, Wasserstein Loss: -101.2707, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [272 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.3762, D Real Loss: 102.0079, Wasserstein Loss: -101.3537, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [272 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.5986, D Real Loss: 102.2321, Wasserstein Loss: -101.5793, Content Loss: 0.0002, PSNR: 38.17
Epoch [272 / 300]: Gradient Penalty: 0.0388, D Fake Loss: 101.6244, D Real Loss: 102.2584, Wasserstein Loss: -101.6039, Content Loss: 0.0002, PSNR: 38.17
Epoch [273 / 300]
===> Epoch [273 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0452, D Fake Loss: 102.3917, D Real Loss: 103.0634, Wasserstein Loss: -102.3539, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [273 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0410, D Fake Loss: 101.5071, D Real Loss: 102.1580, Wasserstein Loss: -101.4684, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [273 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 101.5011, D Real Loss: 102.1439, Wasserstein Loss: -101.4714, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [273 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 101.6420, D Real Loss: 102.2860, Wasserstein Loss: -101.6041, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [273 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 101.7003, D Real Loss: 102.3409, Wasserstein Loss: -101.6712, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [273 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 102.0470, D Real Loss: 102.6873, Wasserstein Loss: -102.0190, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [273 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 102.0131, D Real Loss: 102.6515, Wasserstein Loss: -101.9881, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [273 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.9563, D Real Loss: 102.5931, Wasserstein Loss: -101.9280, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [273 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.8260, D Real Loss: 102.4608, Wasserstein Loss: -101.7977, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [273 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.8755, D Real Loss: 102.5099, Wasserstein Loss: -101.8537, Content Loss: 0.0002, PSNR: 38.25
Epoch [273 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 101.9018, D Real Loss: 102.5361, Wasserstein Loss: -101.8798, Content Loss: 0.0002, PSNR: 38.25
Epoch [274 / 300]
===> Epoch [274 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 102.5344, D Real Loss: 103.1830, Wasserstein Loss: -102.4452, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [274 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 101.6431, D Real Loss: 102.2850, Wasserstein Loss: -101.5868, Content Loss: 0.0002, PSNR: 38.09
===> Epoch [274 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.3585, D Real Loss: 101.9871, Wasserstein Loss: -101.3260, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [274 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.5875, D Real Loss: 102.2137, Wasserstein Loss: -101.5600, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.2577, D Real Loss: 101.8847, Wasserstein Loss: -101.2217, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.2765, D Real Loss: 101.9085, Wasserstein Loss: -101.2445, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.2029, D Real Loss: 101.8359, Wasserstein Loss: -101.1741, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.3098, D Real Loss: 101.9431, Wasserstein Loss: -101.2827, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0373, D Fake Loss: 101.3259, D Real Loss: 101.9598, Wasserstein Loss: -101.2965, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [274 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.2384, D Real Loss: 101.8697, Wasserstein Loss: -101.2087, Content Loss: 0.0002, PSNR: 38.19
Epoch [274 / 300]: Gradient Penalty: 0.0370, D Fake Loss: 101.2077, D Real Loss: 101.8382, Wasserstein Loss: -101.1762, Content Loss: 0.0002, PSNR: 38.19
Epoch [275 / 300]
===> Epoch [275 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0341, D Fake Loss: 100.0492, D Real Loss: 100.6525, Wasserstein Loss: -100.0806, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [275 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.1206, D Real Loss: 101.7388, Wasserstein Loss: -101.1387, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [275 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.5944, D Real Loss: 102.2196, Wasserstein Loss: -101.5915, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [275 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.7732, D Real Loss: 102.4027, Wasserstein Loss: -101.7561, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [275 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.7948, D Real Loss: 102.4258, Wasserstein Loss: -101.7856, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [275 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.4806, D Real Loss: 102.1117, Wasserstein Loss: -101.4626, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [275 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.4953, D Real Loss: 102.1306, Wasserstein Loss: -101.4770, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [275 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.4229, D Real Loss: 102.0541, Wasserstein Loss: -101.3989, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [275 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.4470, D Real Loss: 102.0835, Wasserstein Loss: -101.4290, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [275 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 101.6989, D Real Loss: 102.3365, Wasserstein Loss: -101.6742, Content Loss: 0.0002, PSNR: 38.13
Epoch [275 / 300]: Gradient Penalty: 0.0390, D Fake Loss: 101.6669, D Real Loss: 102.3038, Wasserstein Loss: -101.6426, Content Loss: 0.0002, PSNR: 38.13
Epoch [276 / 300]
===> Epoch [276 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0360, D Fake Loss: 101.1372, D Real Loss: 101.7649, Wasserstein Loss: -101.1401, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [276 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 101.5694, D Real Loss: 102.2030, Wasserstein Loss: -101.5785, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [276 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.4805, D Real Loss: 102.1277, Wasserstein Loss: -101.4715, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [276 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.7600, D Real Loss: 102.4038, Wasserstein Loss: -101.7567, Content Loss: 0.0002, PSNR: 38.11
===> Epoch [276 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.8052, D Real Loss: 102.4438, Wasserstein Loss: -101.7963, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [276 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.9629, D Real Loss: 102.6015, Wasserstein Loss: -101.9506, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [276 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.7197, D Real Loss: 102.3560, Wasserstein Loss: -101.7017, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [276 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.6814, D Real Loss: 102.3163, Wasserstein Loss: -101.6666, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [276 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.5838, D Real Loss: 102.2175, Wasserstein Loss: -101.5699, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [276 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.5929, D Real Loss: 102.2264, Wasserstein Loss: -101.5732, Content Loss: 0.0002, PSNR: 38.18
Epoch [276 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 101.5949, D Real Loss: 102.2280, Wasserstein Loss: -101.5724, Content Loss: 0.0002, PSNR: 38.17
Epoch [277 / 300]
===> Epoch [277 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 100.7825, D Real Loss: 101.4348, Wasserstein Loss: -100.7563, Content Loss: 0.0002, PSNR: 37.83
===> Epoch [277 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.2861, D Real Loss: 101.9231, Wasserstein Loss: -101.2689, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [277 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.1140, D Real Loss: 101.7472, Wasserstein Loss: -101.0939, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [277 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.5239, D Real Loss: 102.1675, Wasserstein Loss: -101.5087, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [277 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 101.6799, D Real Loss: 102.3234, Wasserstein Loss: -101.6536, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [277 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 101.6673, D Real Loss: 102.3068, Wasserstein Loss: -101.6389, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [277 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.5580, D Real Loss: 102.1944, Wasserstein Loss: -101.5264, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [277 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.5008, D Real Loss: 102.1359, Wasserstein Loss: -101.4714, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [277 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.6127, D Real Loss: 102.2487, Wasserstein Loss: -101.5900, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [277 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.4782, D Real Loss: 102.1115, Wasserstein Loss: -101.4548, Content Loss: 0.0002, PSNR: 38.20
Epoch [277 / 300]: Gradient Penalty: 0.0378, D Fake Loss: 101.4739, D Real Loss: 102.1070, Wasserstein Loss: -101.4512, Content Loss: 0.0002, PSNR: 38.20
Epoch [278 / 300]
===> Epoch [278 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0342, D Fake Loss: 101.1396, D Real Loss: 101.7455, Wasserstein Loss: -101.1316, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [278 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0356, D Fake Loss: 101.4239, D Real Loss: 102.0367, Wasserstein Loss: -101.3995, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [278 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0348, D Fake Loss: 101.0977, D Real Loss: 101.7140, Wasserstein Loss: -101.0699, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [278 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 101.2273, D Real Loss: 101.8477, Wasserstein Loss: -101.2099, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [278 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0359, D Fake Loss: 101.6050, D Real Loss: 102.2272, Wasserstein Loss: -101.5868, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [278 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.5378, D Real Loss: 102.1639, Wasserstein Loss: -101.5177, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [278 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.3335, D Real Loss: 101.9645, Wasserstein Loss: -101.3141, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [278 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.4999, D Real Loss: 102.1303, Wasserstein Loss: -101.4775, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [278 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.5142, D Real Loss: 102.1445, Wasserstein Loss: -101.4888, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [278 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.4942, D Real Loss: 102.1226, Wasserstein Loss: -101.4687, Content Loss: 0.0002, PSNR: 38.24
Epoch [278 / 300]: Gradient Penalty: 0.0374, D Fake Loss: 101.4790, D Real Loss: 102.1065, Wasserstein Loss: -101.4533, Content Loss: 0.0002, PSNR: 38.24
Epoch [279 / 300]
===> Epoch [279 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.2633, D Real Loss: 101.9068, Wasserstein Loss: -101.2532, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [279 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.1815, D Real Loss: 101.8157, Wasserstein Loss: -101.1685, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [279 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 100.8506, D Real Loss: 101.4890, Wasserstein Loss: -100.8353, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [279 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0394, D Fake Loss: 101.3146, D Real Loss: 101.9601, Wasserstein Loss: -101.3043, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [279 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 101.4364, D Real Loss: 102.0758, Wasserstein Loss: -101.4210, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [279 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0386, D Fake Loss: 101.4737, D Real Loss: 102.1102, Wasserstein Loss: -101.4539, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [279 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.4280, D Real Loss: 102.0596, Wasserstein Loss: -101.4075, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [279 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 101.5538, D Real Loss: 102.1850, Wasserstein Loss: -101.5326, Content Loss: 0.0002, PSNR: 38.18
===> Epoch [279 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.6158, D Real Loss: 102.2461, Wasserstein Loss: -101.5971, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [279 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0375, D Fake Loss: 101.6893, D Real Loss: 102.3184, Wasserstein Loss: -101.6674, Content Loss: 0.0002, PSNR: 38.16
Epoch [279 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 101.6825, D Real Loss: 102.3124, Wasserstein Loss: -101.6572, Content Loss: 0.0002, PSNR: 38.16
Epoch [280 / 300]
===> Epoch [280 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0402, D Fake Loss: 100.8649, D Real Loss: 101.5033, Wasserstein Loss: -100.8634, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [280 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 102.0064, D Real Loss: 102.6434, Wasserstein Loss: -101.9849, Content Loss: 0.0002, PSNR: 38.35
===> Epoch [280 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.9410, D Real Loss: 102.5726, Wasserstein Loss: -101.9337, Content Loss: 0.0002, PSNR: 38.37
===> Epoch [280 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.8247, D Real Loss: 102.4528, Wasserstein Loss: -101.8156, Content Loss: 0.0002, PSNR: 38.32
===> Epoch [280 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0359, D Fake Loss: 101.8252, D Real Loss: 102.4496, Wasserstein Loss: -101.8112, Content Loss: 0.0002, PSNR: 38.33
===> Epoch [280 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 101.7431, D Real Loss: 102.3686, Wasserstein Loss: -101.7311, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [280 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0360, D Fake Loss: 101.8016, D Real Loss: 102.4251, Wasserstein Loss: -101.7861, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [280 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0360, D Fake Loss: 101.7844, D Real Loss: 102.4077, Wasserstein Loss: -101.7687, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [280 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 101.8605, D Real Loss: 102.4833, Wasserstein Loss: -101.8470, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [280 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 101.8632, D Real Loss: 102.4868, Wasserstein Loss: -101.8414, Content Loss: 0.0002, PSNR: 38.27
Epoch [280 / 300]: Gradient Penalty: 0.0362, D Fake Loss: 101.8462, D Real Loss: 102.4695, Wasserstein Loss: -101.8250, Content Loss: 0.0002, PSNR: 38.27
Epoch [281 / 300]
===> Epoch [281 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 100.9428, D Real Loss: 101.5665, Wasserstein Loss: -100.9330, Content Loss: 0.0002, PSNR: 38.05
===> Epoch [281 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0360, D Fake Loss: 101.8518, D Real Loss: 102.4704, Wasserstein Loss: -101.8516, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [281 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 102.1609, D Real Loss: 102.7975, Wasserstein Loss: -102.1409, Content Loss: 0.0002, PSNR: 38.12
===> Epoch [281 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.9179, D Real Loss: 102.5544, Wasserstein Loss: -101.9009, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [281 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 102.2296, D Real Loss: 102.8654, Wasserstein Loss: -102.2101, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [281 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 102.1558, D Real Loss: 102.7893, Wasserstein Loss: -102.1356, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [281 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 102.0568, D Real Loss: 102.6885, Wasserstein Loss: -102.0341, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [281 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0370, D Fake Loss: 102.1959, D Real Loss: 102.8283, Wasserstein Loss: -102.1744, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [281 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 102.2954, D Real Loss: 102.9235, Wasserstein Loss: -102.2724, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [281 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 102.2842, D Real Loss: 102.9139, Wasserstein Loss: -102.2573, Content Loss: 0.0002, PSNR: 38.21
Epoch [281 / 300]: Gradient Penalty: 0.0369, D Fake Loss: 102.2364, D Real Loss: 102.8661, Wasserstein Loss: -102.2085, Content Loss: 0.0002, PSNR: 38.21
Epoch [282 / 300]
===> Epoch [282 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0406, D Fake Loss: 102.0591, D Real Loss: 102.7346, Wasserstein Loss: -102.1258, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [282 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0392, D Fake Loss: 102.0068, D Real Loss: 102.6517, Wasserstein Loss: -102.0089, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [282 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0384, D Fake Loss: 101.8726, D Real Loss: 102.5066, Wasserstein Loss: -101.8686, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [282 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.6950, D Real Loss: 102.3273, Wasserstein Loss: -101.6805, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [282 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0378, D Fake Loss: 101.4830, D Real Loss: 102.1136, Wasserstein Loss: -101.4723, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [282 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.5769, D Real Loss: 102.2058, Wasserstein Loss: -101.5675, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [282 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.5260, D Real Loss: 102.1536, Wasserstein Loss: -101.5176, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [282 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.6144, D Real Loss: 102.2444, Wasserstein Loss: -101.5986, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [282 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0380, D Fake Loss: 101.6549, D Real Loss: 102.2880, Wasserstein Loss: -101.6388, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [282 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 101.6574, D Real Loss: 102.2936, Wasserstein Loss: -101.6420, Content Loss: 0.0002, PSNR: 38.20
Epoch [282 / 300]: Gradient Penalty: 0.0387, D Fake Loss: 101.6721, D Real Loss: 102.3079, Wasserstein Loss: -101.6546, Content Loss: 0.0002, PSNR: 38.21
Epoch [283 / 300]
===> Epoch [283 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0389, D Fake Loss: 102.1099, D Real Loss: 102.7547, Wasserstein Loss: -102.0785, Content Loss: 0.0002, PSNR: 37.97
===> Epoch [283 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0407, D Fake Loss: 102.0587, D Real Loss: 102.6995, Wasserstein Loss: -102.0270, Content Loss: 0.0002, PSNR: 38.01
===> Epoch [283 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0400, D Fake Loss: 102.3346, D Real Loss: 102.9695, Wasserstein Loss: -102.3100, Content Loss: 0.0002, PSNR: 38.04
===> Epoch [283 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0397, D Fake Loss: 102.6252, D Real Loss: 103.2630, Wasserstein Loss: -102.6036, Content Loss: 0.0002, PSNR: 38.08
===> Epoch [283 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 102.5309, D Real Loss: 103.1702, Wasserstein Loss: -102.4971, Content Loss: 0.0002, PSNR: 38.06
===> Epoch [283 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 102.4290, D Real Loss: 103.0665, Wasserstein Loss: -102.4001, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [283 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 102.4570, D Real Loss: 103.0915, Wasserstein Loss: -102.4271, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [283 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 102.3419, D Real Loss: 102.9768, Wasserstein Loss: -102.3152, Content Loss: 0.0002, PSNR: 38.13
===> Epoch [283 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 102.3977, D Real Loss: 103.0332, Wasserstein Loss: -102.3694, Content Loss: 0.0002, PSNR: 38.14
===> Epoch [283 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 102.3394, D Real Loss: 102.9750, Wasserstein Loss: -102.3139, Content Loss: 0.0002, PSNR: 38.15
Epoch [283 / 300]: Gradient Penalty: 0.0383, D Fake Loss: 102.3471, D Real Loss: 102.9828, Wasserstein Loss: -102.3206, Content Loss: 0.0002, PSNR: 38.15
Epoch [284 / 300]
===> Epoch [284 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 103.2912, D Real Loss: 103.9261, Wasserstein Loss: -103.2596, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [284 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 103.1263, D Real Loss: 103.7638, Wasserstein Loss: -103.0965, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [284 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0385, D Fake Loss: 102.7641, D Real Loss: 103.4032, Wasserstein Loss: -102.7407, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [284 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 102.9004, D Real Loss: 103.5423, Wasserstein Loss: -102.8664, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [284 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0398, D Fake Loss: 102.5896, D Real Loss: 103.2306, Wasserstein Loss: -102.5575, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [284 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0391, D Fake Loss: 102.4160, D Real Loss: 103.0535, Wasserstein Loss: -102.3910, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [284 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0387, D Fake Loss: 102.4716, D Real Loss: 103.1091, Wasserstein Loss: -102.4529, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [284 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 102.4176, D Real Loss: 103.0515, Wasserstein Loss: -102.3934, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [284 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0379, D Fake Loss: 102.2943, D Real Loss: 102.9286, Wasserstein Loss: -102.2716, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [284 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 102.3510, D Real Loss: 102.9831, Wasserstein Loss: -102.3298, Content Loss: 0.0002, PSNR: 38.25
Epoch [284 / 300]: Gradient Penalty: 0.0374, D Fake Loss: 102.3509, D Real Loss: 102.9830, Wasserstein Loss: -102.3269, Content Loss: 0.0002, PSNR: 38.25
Epoch [285 / 300]
===> Epoch [285 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0357, D Fake Loss: 101.1845, D Real Loss: 101.8148, Wasserstein Loss: -101.1491, Content Loss: 0.0002, PSNR: 38.28
===> Epoch [285 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0354, D Fake Loss: 101.6134, D Real Loss: 102.2277, Wasserstein Loss: -101.5881, Content Loss: 0.0002, PSNR: 38.31
===> Epoch [285 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.4592, D Real Loss: 102.0928, Wasserstein Loss: -101.4426, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [285 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.5605, D Real Loss: 102.1995, Wasserstein Loss: -101.5442, Content Loss: 0.0002, PSNR: 38.16
===> Epoch [285 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 101.5086, D Real Loss: 102.1460, Wasserstein Loss: -101.4953, Content Loss: 0.0002, PSNR: 38.17
===> Epoch [285 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.6225, D Real Loss: 102.2549, Wasserstein Loss: -101.6034, Content Loss: 0.0002, PSNR: 38.20
===> Epoch [285 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.7030, D Real Loss: 102.3336, Wasserstein Loss: -101.6782, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [285 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0372, D Fake Loss: 101.7304, D Real Loss: 102.3617, Wasserstein Loss: -101.7078, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [285 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.7125, D Real Loss: 102.3437, Wasserstein Loss: -101.6872, Content Loss: 0.0002, PSNR: 38.22
===> Epoch [285 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0371, D Fake Loss: 101.6790, D Real Loss: 102.3092, Wasserstein Loss: -101.6543, Content Loss: 0.0002, PSNR: 38.22
Epoch [285 / 300]: Gradient Penalty: 0.0371, D Fake Loss: 101.6697, D Real Loss: 102.3000, Wasserstein Loss: -101.6469, Content Loss: 0.0002, PSNR: 38.23
Epoch [286 / 300]
===> Epoch [286 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 102.0064, D Real Loss: 102.6680, Wasserstein Loss: -102.0124, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [286 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0390, D Fake Loss: 102.5288, D Real Loss: 103.1810, Wasserstein Loss: -102.5174, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [286 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0393, D Fake Loss: 102.3590, D Real Loss: 103.0065, Wasserstein Loss: -102.3379, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [286 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 102.0345, D Real Loss: 102.6729, Wasserstein Loss: -102.0132, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [286 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0388, D Fake Loss: 102.2047, D Real Loss: 102.8448, Wasserstein Loss: -102.1779, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [286 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0383, D Fake Loss: 101.9390, D Real Loss: 102.5773, Wasserstein Loss: -101.9148, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [286 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 101.9001, D Real Loss: 102.5358, Wasserstein Loss: -101.8765, Content Loss: 0.0002, PSNR: 38.28
===> Epoch [286 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.8492, D Real Loss: 102.4820, Wasserstein Loss: -101.8251, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [286 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.9997, D Real Loss: 102.6307, Wasserstein Loss: -101.9827, Content Loss: 0.0002, PSNR: 38.30
===> Epoch [286 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0376, D Fake Loss: 101.9750, D Real Loss: 102.6049, Wasserstein Loss: -101.9507, Content Loss: 0.0002, PSNR: 38.29
Epoch [286 / 300]: Gradient Penalty: 0.0376, D Fake Loss: 101.9597, D Real Loss: 102.5891, Wasserstein Loss: -101.9355, Content Loss: 0.0002, PSNR: 38.29
Epoch [287 / 300]
===> Epoch [287 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0347, D Fake Loss: 102.3757, D Real Loss: 102.9982, Wasserstein Loss: -102.3642, Content Loss: 0.0002, PSNR: 38.37
===> Epoch [287 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0337, D Fake Loss: 102.4967, D Real Loss: 103.1019, Wasserstein Loss: -102.4954, Content Loss: 0.0002, PSNR: 38.43
===> Epoch [287 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0355, D Fake Loss: 102.0754, D Real Loss: 102.6973, Wasserstein Loss: -102.0454, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [287 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 102.0090, D Real Loss: 102.6333, Wasserstein Loss: -101.9900, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [287 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 101.8381, D Real Loss: 102.4632, Wasserstein Loss: -101.8085, Content Loss: 0.0002, PSNR: 38.28
===> Epoch [287 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.6903, D Real Loss: 102.3141, Wasserstein Loss: -101.6659, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [287 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0364, D Fake Loss: 101.8577, D Real Loss: 102.4831, Wasserstein Loss: -101.8416, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [287 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 101.9419, D Real Loss: 102.5666, Wasserstein Loss: -101.9203, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [287 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 101.8167, D Real Loss: 102.4402, Wasserstein Loss: -101.7956, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [287 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.7511, D Real Loss: 102.3768, Wasserstein Loss: -101.7281, Content Loss: 0.0002, PSNR: 38.23
Epoch [287 / 300]: Gradient Penalty: 0.0365, D Fake Loss: 101.7582, D Real Loss: 102.3841, Wasserstein Loss: -101.7380, Content Loss: 0.0002, PSNR: 38.24
Epoch [288 / 300]
===> Epoch [288 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0332, D Fake Loss: 101.6260, D Real Loss: 102.2386, Wasserstein Loss: -101.5526, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [288 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0347, D Fake Loss: 101.2264, D Real Loss: 101.8441, Wasserstein Loss: -101.1899, Content Loss: 0.0002, PSNR: 38.19
===> Epoch [288 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0358, D Fake Loss: 101.4549, D Real Loss: 102.0855, Wasserstein Loss: -101.4305, Content Loss: 0.0002, PSNR: 38.21
===> Epoch [288 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0356, D Fake Loss: 101.2899, D Real Loss: 101.9177, Wasserstein Loss: -101.2610, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [288 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0367, D Fake Loss: 101.5575, D Real Loss: 102.1871, Wasserstein Loss: -101.5274, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [288 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0366, D Fake Loss: 101.5600, D Real Loss: 102.1877, Wasserstein Loss: -101.5316, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [288 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 101.5633, D Real Loss: 102.1876, Wasserstein Loss: -101.5398, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [288 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0362, D Fake Loss: 101.5934, D Real Loss: 102.2163, Wasserstein Loss: -101.5753, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [288 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0359, D Fake Loss: 101.5837, D Real Loss: 102.2058, Wasserstein Loss: -101.5654, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [288 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0361, D Fake Loss: 101.8006, D Real Loss: 102.4259, Wasserstein Loss: -101.7748, Content Loss: 0.0002, PSNR: 38.25
Epoch [288 / 300]: Gradient Penalty: 0.0361, D Fake Loss: 101.7852, D Real Loss: 102.4104, Wasserstein Loss: -101.7574, Content Loss: 0.0002, PSNR: 38.25
Epoch [289 / 300]
===> Epoch [289 / 300]: Batch [105 / 1058]: Gradient Penalty: 0.0420, D Fake Loss: 100.4034, D Real Loss: 101.0709, Wasserstein Loss: -100.4134, Content Loss: 0.0002, PSNR: 38.10
===> Epoch [289 / 300]: Batch [210 / 1058]: Gradient Penalty: 0.0382, D Fake Loss: 100.9859, D Real Loss: 101.6228, Wasserstein Loss: -100.9684, Content Loss: 0.0002, PSNR: 38.24
===> Epoch [289 / 300]: Batch [315 / 1058]: Gradient Penalty: 0.0377, D Fake Loss: 101.1502, D Real Loss: 101.7895, Wasserstein Loss: -101.1355, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [289 / 300]: Batch [420 / 1058]: Gradient Penalty: 0.0374, D Fake Loss: 101.0319, D Real Loss: 101.6668, Wasserstein Loss: -101.0132, Content Loss: 0.0002, PSNR: 38.23
===> Epoch [289 / 300]: Batch [525 / 1058]: Gradient Penalty: 0.0369, D Fake Loss: 100.9931, D Real Loss: 101.6207, Wasserstein Loss: -100.9820, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [289 / 300]: Batch [630 / 1058]: Gradient Penalty: 0.0368, D Fake Loss: 101.1512, D Real Loss: 101.7773, Wasserstein Loss: -101.1336, Content Loss: 0.0002, PSNR: 38.25
===> Epoch [289 / 300]: Batch [735 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 101.2601, D Real Loss: 101.8854, Wasserstein Loss: -101.2400, Content Loss: 0.0002, PSNR: 38.27
===> Epoch [289 / 300]: Batch [840 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.1461, D Real Loss: 101.7740, Wasserstein Loss: -101.1264, Content Loss: 0.0002, PSNR: 38.26
===> Epoch [289 / 300]: Batch [945 / 1058]: Gradient Penalty: 0.0365, D Fake Loss: 101.1122, D Real Loss: 101.7387, Wasserstein Loss: -101.0904, Content Loss: 0.0002, PSNR: 38.29
===> Epoch [289 / 300]: Batch [1050 / 1058]: Gradient Penalty: 0.0363, D Fake Loss: 101.0999, D Real Loss: 101.7237, Wasserstein Loss: -101.0806, Content Loss: 0.0002, PSNR: 38.30
Epoch [289 / 300]: Gradient Penalty: 0.0363, D Fake Loss: 101.1096, D Real Loss: 101.7330, Wasserstein Loss: -101.0899, Content Loss: 0.0002, PSNR: 38.30
Epoch [290 / 300]
