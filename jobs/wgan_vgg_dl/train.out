Sun Aug 23 02:40:19 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX TIT...  Off  | 00000000:04:00.0 Off |                  N/A |
| 19%   32C    P0    68W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX TIT...  Off  | 00000000:05:00.0 Off |                  N/A |
| 19%   34C    P0    69W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX TIT...  Off  | 00000000:08:00.0 Off |                  N/A |
| 36%   77C    P2   175W / 250W |    948MiB / 12212MiB |     61%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX TIT...  Off  | 00000000:09:00.0 Off |                  N/A |
| 51%   82C    P2   170W / 250W |  12199MiB / 12212MiB |    100%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   4  GeForce GTX TIT...  Off  | 00000000:84:00.0 Off |                  N/A |
| 34%   66C    P0    73W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   5  GeForce GTX TIT...  Off  | 00000000:85:00.0 Off |                  N/A |
| 19%   47C    P0    69W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   6  GeForce GTX TIT...  Off  | 00000000:88:00.0 Off |                  N/A |
| 18%   30C    P0    70W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   7  GeForce GTX TIT...  Off  | 00000000:89:00.0 Off |                  N/A |
| 18%   33C    P0    69W / 250W |      0MiB / 12212MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    2   N/A  N/A     10224      C   ...pyt-1-4-pl-073/bin/python      945MiB |
|    3   N/A  N/A       665      C   ...a3/envs/eval_2/bin/python    12196MiB |
+-----------------------------------------------------------------------------+
Running python script now
Beginning training for WGAN-VGG model...
Using the following hyperparemters:
Data:                 /data/pacole2/DeepLesionPreprocessed/miniStudies/
Dataset:              DeepLesion
Upscale:              1
Learning rate:        1e-05
Number of epochs:     100
Prints per epoch:     10
Start decay:          99
Batch size:           64
Content loss:         vgg
CLambda:              10000.0
WLambda:              1.0
Check Sample:         True
Checkpoint directory: checkpoints/wgan_vgg/
Load:                 False
Prefix:               last
Cuda:                 2

GPU devices being used:  [0, 1]
Epoch [1 / 100]
0.06844321638345718 0.05523151904344559
0.06849639117717743 0.055288270115852356
0.0684518814086914 0.0553494431078434
0.06849963217973709 0.05540307238698006
0.06848099827766418 0.05536448583006859
0.06845454126596451 0.05547419190406799
0.0683804377913475 0.055541668087244034
0.06850194185972214 0.05544646084308624
0.06835570931434631 0.05549714341759682
0.06843209266662598 0.05561968684196472
0.06844346970319748 0.05557558313012123
0.06845144927501678 0.055638622492551804
0.0686502754688263 0.05572955310344696
0.06860227882862091 0.05574847012758255
0.06864117085933685 0.05578118562698364
0.06876837462186813 0.055796317756175995
===> Epoch [1 / 100]: Batch [16 / 159]: Gradient Penalty: 9.9450, D Fake Loss: -0.0284, D Real Loss: -0.0255, Wasserstein Loss: 0.0276, Content Loss: 0.7289, PSNR: 13.45 SSIM: 0.27
0.06869279593229294 0.055848948657512665
0.06899596750736237 0.055882155895233154
0.06896868348121643 0.055941712111234665
0.06913544237613678 0.05601415038108826
0.06928209215402603 0.056062426418066025
0.06937547773122787 0.05611308664083481
0.06959276646375656 0.05612491816282272
0.06973203271627426 0.056171707808971405
0.06989344954490662 0.05624612048268318
0.06996030360460281 0.05627027153968811
0.07002168893814087 0.056318219751119614
0.07028244435787201 0.05634666979312897
0.07041068375110626 0.056400056928396225
0.07031320035457611 0.056458380073308945
0.0705639123916626 0.05641999840736389
0.07073235511779785 0.056460268795490265
===> Epoch [1 / 100]: Batch [32 / 159]: Gradient Penalty: 9.8758, D Fake Loss: -0.0176, D Real Loss: 0.0016, Wasserstein Loss: 0.0162, Content Loss: 0.7506, PSNR: 13.08 SSIM: 0.26
0.07066143304109573 0.056550294160842896
0.07067519426345825 0.05658237636089325
0.07106631994247437 0.05662672966718674
0.07094179093837738 0.0566096305847168
0.07113192230463028 0.056649524718523026
0.07119232416152954 0.056752391159534454
0.07124388217926025 0.056791700422763824
0.07140802592039108 0.056786537170410156
0.07161233574151993 0.05686463043093681
0.07151374220848083 0.056904636323451996
0.07161153107881546 0.05696820840239525
0.07168345153331757 0.05697840452194214
0.07166031748056412 0.05701308697462082
0.07169990986585617 0.05701889842748642
0.07180532813072205 0.057062018662691116
0.0719771757721901 0.05707911401987076
===> Epoch [1 / 100]: Batch [48 / 159]: Gradient Penalty: 9.7132, D Fake Loss: -0.0028, D Real Loss: 0.0770, Wasserstein Loss: 0.0013, Content Loss: 0.7518, PSNR: 12.95 SSIM: 0.26
0.07212604582309723 0.057166337966918945
0.07224623113870621 0.057188648730516434
0.0722278356552124 0.057177912443876266
0.0724506676197052 0.0572810024023056
0.07232266664505005 0.05721849575638771
0.07213589549064636 0.057321205735206604
0.07208611071109772 0.05733662098646164
0.07213384658098221 0.05731620267033577
0.07231788337230682 0.05731438845396042
0.07221513986587524 0.05731869488954544
0.07207264751195908 0.057363495230674744
0.07222279906272888 0.0573757067322731
0.07224805653095245 0.05739843472838402
0.07221172004938126 0.05745701864361763
0.0723903626203537 0.057450827211141586
0.07266745716333389 0.05753322318196297
===> Epoch [1 / 100]: Batch [64 / 159]: Gradient Penalty: 9.3693, D Fake Loss: 0.0122, D Real Loss: 0.2251, Wasserstein Loss: -0.0138, Content Loss: 0.7454, PSNR: 13.04 SSIM: 0.26
0.07287274301052094 0.057567376643419266
0.07307177782058716 0.057625070214271545
0.07283882796764374 0.05762703716754913
0.07291790097951889 0.057653557509183884
0.07296938449144363 0.05772355571389198
0.07327617704868317 0.05772409960627556
0.07312825322151184 0.05775899812579155
0.07332459092140198 0.057808876037597656
0.07343905419111252 0.05786678567528725
0.07342914491891861 0.05789146199822426
0.07353503257036209 0.05790557339787483
0.07356151938438416 0.05794879049062729
0.0735504999756813 0.05796198546886444
0.07366195321083069 0.05795959010720253
0.07381971180438995 0.05801220238208771
0.07376742362976074 0.05802912637591362
===> Epoch [1 / 100]: Batch [80 / 159]: Gradient Penalty: 8.7391, D Fake Loss: 0.0214, D Real Loss: 0.5214, Wasserstein Loss: -0.0220, Content Loss: 0.7459, PSNR: 13.04 SSIM: 0.26
0.07393649220466614 0.0580432265996933
0.07374228537082672 0.05802499130368233
0.07412685453891754 0.05802357196807861
0.0740366131067276 0.05815071240067482
0.07451243698596954 0.05816859379410744
0.07423518598079681 0.058222122490406036
0.07421386241912842 0.058249302208423615
0.0741952508687973 0.058188796043395996
0.07399964332580566 0.05827886983752251
0.0744265541434288 0.05825966224074364
0.07428104430437088 0.05829896032810211
0.07426399737596512 0.05828164517879486
0.07407543063163757 0.0582599975168705
0.07407665997743607 0.058265432715415955
0.0742000937461853 0.058268532156944275
0.0745847225189209 0.05827438086271286
===> Epoch [1 / 100]: Batch [96 / 159]: Gradient Penalty: 7.8075, D Fake Loss: 0.0073, D Real Loss: 1.0410, Wasserstein Loss: -0.0057, Content Loss: 0.7453, PSNR: 13.06 SSIM: 0.26
0.07396461814641953 0.05827128887176514
0.07413796335458755 0.05825674906373024
0.0743231475353241 0.05829087644815445
0.07441145181655884 0.058308813720941544
0.07465976476669312 0.058327529579401016
0.07447563111782074 0.058350469917058945
0.07465921342372894 0.05839340016245842
0.0746954083442688 0.058419156819581985
0.07468945533037186 0.05842475965619087
0.07505808770656586 0.05841974541544914
0.07491006702184677 0.058324843645095825
0.07475448399782181 0.05840552598237991
0.07528793811798096 0.058395832777023315
0.07504981011152267 0.0584067665040493
0.07430306822061539 0.058414146304130554
0.07479969412088394 0.058370254933834076
===> Epoch [1 / 100]: Batch [112 / 159]: Gradient Penalty: 6.8469, D Fake Loss: -0.0333, D Real Loss: 1.9139, Wasserstein Loss: 0.0370, Content Loss: 0.7398, PSNR: 13.09 SSIM: 0.26
0.07512462884187698 0.058370016515254974
0.07502003014087677 0.05845533683896065
0.07548334449529648 0.05848659947514534
0.0755433589220047 0.05852901190519333
0.07530362159013748 0.05853758007287979
0.07562349736690521 0.05855855345726013
0.07547995448112488 0.05852177366614342
0.07570105791091919 0.05857640504837036
0.07542228698730469 0.05858228728175163
0.07590829581022263 0.05861145630478859
0.0758056789636612 0.05863136053085327
0.07655167579650879 0.05860156938433647
0.07551011443138123 0.058623895049095154
0.07592849433422089 0.05862624943256378
0.07566031813621521 0.05861365795135498
0.07551951706409454 0.058555182069540024
===> Epoch [1 / 100]: Batch [128 / 159]: Gradient Penalty: 6.2346, D Fake Loss: -0.1124, D Real Loss: 3.1900, Wasserstein Loss: 0.1198, Content Loss: 0.7367, PSNR: 13.07 SSIM: 0.26
0.07625331729650497 0.05858519673347473
0.07572934776544571 0.05855700001120567
0.07570473849773407 0.05853138491511345
0.07544204592704773 0.058442436158657074
0.07530234754085541 0.058491483330726624
0.07578158378601074 0.0584496408700943
0.07511474937200546 0.05846687778830528
0.07558313757181168 0.05844038352370262
0.0755409300327301 0.058412160724401474
0.07536835968494415 0.0584116205573082
0.07507022470235825 0.05842992290854454
0.07545919716358185 0.058431483805179596
0.07535408437252045 0.05844788998365402
0.07564845681190491 0.05847173184156418
0.07567957788705826 0.05849110335111618
0.07582646608352661 0.058517057448625565
===> Epoch [1 / 100]: Batch [144 / 159]: Gradient Penalty: 5.9364, D Fake Loss: -0.2462, D Real Loss: 4.2924, Wasserstein Loss: 0.2579, Content Loss: 0.7349, PSNR: 13.15 SSIM: 0.26
0.07575254142284393 0.058478906750679016
0.07585135847330093 0.058508384972810745
0.07600012421607971 0.05850345641374588
0.07607865333557129 0.05852215364575386
0.07603573799133301 0.058543555438518524
0.07664182782173157 0.058562956750392914
0.07637946307659149 0.05855093151330948
0.07693049311637878 0.05859994888305664
0.07684099674224854 0.05862011760473251
0.07717209309339523 0.0586622878909111
0.07699015736579895 0.05865393206477165
0.07711495459079742 0.05868956446647644
0.07714643329381943 0.05866904556751251
0.0773068368434906 0.05869118496775627
0.07727915048599243 0.05871069058775902
Epoch [1 / 100]: Gradient Penalty: 5.7493, D Fake Loss: -0.4252, D Real Loss: 5.4687, Wasserstein Loss: 0.4433, Content Loss: 0.7344, PSNR: 13.11 SSIM: 0.26
Epoch [2 / 100]
0.07780110836029053 0.058730971068143845
0.0776607021689415 0.058667924255132675
0.07801468670368195 0.05867960676550865
0.07743314653635025 0.05865245312452316
0.07747332751750946 0.05859953537583351
0.0778278037905693 0.05857306718826294
0.07724432647228241 0.05853663757443428
0.07718459516763687 0.058478888124227524
0.07774487882852554 0.05850713700056076
0.07735896855592728 0.058507274836301804
0.07738641649484634 0.058521024882793427
0.07780332863330841 0.058510441333055496
0.07755470275878906 0.05852895602583885
0.07779684662818909 0.058522917330265045
0.07804617285728455 0.05850386247038841
0.07803437113761902 0.05850674584507942
===> Epoch [2 / 100]: Batch [16 / 159]: Gradient Penalty: 4.4977, D Fake Loss: -3.2818, D Real Loss: 14.5720, Wasserstein Loss: 3.3542, Content Loss: 0.7215, PSNR: 13.67 SSIM: 0.27
0.07823541015386581 0.05851351469755173
0.07816296815872192 0.05851317197084427
0.07851588726043701 0.05852733924984932
0.07846970111131668 0.05855468288064003
0.07899931073188782 0.05855727568268776
0.07870493084192276 0.058594029396772385
0.07894514501094818 0.05857035145163536
0.07927471399307251 0.058622393757104874
0.0791955292224884 0.058616720139980316
0.0791487917304039 0.05865873768925667
0.07922042906284332 0.05864199250936508
0.08005857467651367 0.0586312934756279
0.07911982387304306 0.05867212638258934
0.07984189689159393 0.05868608504533768
0.07984620332717896 0.058695364743471146
0.07983376085758209 0.05871336907148361
===> Epoch [2 / 100]: Batch [32 / 159]: Gradient Penalty: 4.7038, D Fake Loss: -3.9441, D Real Loss: 16.1679, Wasserstein Loss: 4.0322, Content Loss: 0.7298, PSNR: 13.20 SSIM: 0.27
0.08065912127494812 0.058763593435287476
0.08090334385633469 0.058777496218681335
0.08026424050331116 0.058791134506464005
0.08131139725446701 0.05878397822380066
0.08087126910686493 0.058769505470991135
0.08113216608762741 0.058753930032253265
0.08146393299102783 0.05874793231487274
0.08116069436073303 0.05875421687960625
0.08134923875331879 0.058782342821359634
0.08220238238573074 0.05879545211791992
0.08178454637527466 0.05879363417625427
0.08253836631774902 0.0587809793651104
0.0822945311665535 0.058772772550582886
0.08254197984933853 0.05876729637384415
0.08212430030107498 0.05876368284225464
0.08222165703773499 0.058791372925043106
===> Epoch [2 / 100]: Batch [48 / 159]: Gradient Penalty: 4.8849, D Fake Loss: -4.6337, D Real Loss: 16.5859, Wasserstein Loss: 4.7233, Content Loss: 0.7291, PSNR: 13.04 SSIM: 0.27
0.0824536383152008 0.05879450589418411
0.08322601765394211 0.05874104052782059
0.08168558776378632 0.05873668193817139
0.08335589617490768 0.05870220065116882
0.08282917737960815 0.0586889311671257
0.08279603719711304 0.05866869539022446
0.08289651572704315 0.058662787079811096
0.08432326465845108 0.0586121566593647
0.08345509320497513 0.05858326703310013
0.08273758739233017 0.05854463204741478
0.08340775221586227 0.0585474893450737
0.08261159807443619 0.05856078863143921
0.08366204798221588 0.058551009744405746
0.08261445164680481 0.058521028608083725
0.08316013216972351 0.05849824100732803
0.08363399654626846 0.058452337980270386
===> Epoch [2 / 100]: Batch [64 / 159]: Gradient Penalty: 4.9954, D Fake Loss: -5.2852, D Real Loss: 16.1042, Wasserstein Loss: 5.3667, Content Loss: 0.7255, PSNR: 13.16 SSIM: 0.28
0.08399718999862671 0.058460433036088943
0.08392103016376495 0.058453790843486786
0.08502158522605896 0.05828164145350456
0.08446398377418518 0.058408498764038086
0.08453978598117828 0.05833134800195694
0.0847480297088623 0.05843871831893921
0.0846756100654602 0.058352336287498474
0.08551038801670074 0.05832870304584503
0.08557246625423431 0.05829945579171181
0.08535455167293549 0.05818406492471695
0.08578664064407349 0.05812946707010269
0.08511365205049515 0.05809876695275307
0.08433486521244049 0.05796140804886818
0.08570107817649841 0.05794822424650192
0.08531776815652847 0.05790017545223236
0.08629409968852997 0.05759032070636749
===> Epoch [2 / 100]: Batch [80 / 159]: Gradient Penalty: 5.0939, D Fake Loss: -5.8366, D Real Loss: 15.8702, Wasserstein Loss: 5.9077, Content Loss: 0.7286, PSNR: 13.19 SSIM: 0.28
0.08573046326637268 0.057665444910526276
0.08768109977245331 0.057785265147686005
0.08734557777643204 0.05777479335665703
0.08686256408691406 0.05748659744858742
0.08813361823558807 0.05777326971292496
0.08760078251361847 0.05720221623778343
0.08696523308753967 0.057471610605716705
0.08833347260951996 0.057742465287446976
0.0865570455789566 0.05741960182785988
0.08917954564094543 0.057137757539749146
0.087492436170578 0.05725402384996414
0.08698096871376038 0.057217951864004135
0.08863627910614014 0.05701390653848648
0.09135247766971588 0.05638938769698143
0.08857172727584839 0.05677428096532822
0.08920470625162125 0.05675261840224266
===> Epoch [2 / 100]: Batch [96 / 159]: Gradient Penalty: 5.1747, D Fake Loss: -6.3789, D Real Loss: 15.5825, Wasserstein Loss: 6.4562, Content Loss: 0.7256, PSNR: 13.23 SSIM: 0.28
0.08923977613449097 0.056670915335416794
0.09108735620975494 0.056069545447826385
0.08924286812543869 0.056401800364255905
0.08926203846931458 0.05651747062802315
0.09060952067375183 0.05643168464303017
0.09177333116531372 0.0565095990896225
0.09101889282464981 0.05650167912244797
0.09001293778419495 0.056122563779354095
0.09081708639860153 0.05624757707118988
0.09091803431510925 0.05623459070920944
0.09310728311538696 0.0557674802839756
0.0937245562672615 0.055996429175138474
0.09129509329795837 0.055324405431747437
0.09244358539581299 0.05559604614973068
0.09095585346221924 0.05573689565062523
0.09183462709188461 0.05578572303056717
===> Epoch [2 / 100]: Batch [112 / 159]: Gradient Penalty: 5.2488, D Fake Loss: -7.0506, D Real Loss: 15.1802, Wasserstein Loss: 7.1422, Content Loss: 0.7210, PSNR: 13.26 SSIM: 0.28
0.09317630529403687 0.05564214661717415
0.0931786596775055 0.0554380938410759
0.09320268034934998 0.05533590540289879
0.09564071893692017 0.054931819438934326
0.09299398958683014 0.055379729717969894
0.09654839336872101 0.05517899617552757
0.09454703330993652 0.055393192917108536
0.09556777775287628 0.055300790816545486
0.09721790254116058 0.05524129047989845
0.0952824205160141 0.05508028343319893
0.09651947766542435 0.05531713366508484
0.09553589671850204 0.05520571023225784
0.09765557944774628 0.05501624569296837
0.09664562344551086 0.05483795329928398
0.09616603702306747 0.05451362580060959
0.09709540009498596 0.05370274931192398
===> Epoch [2 / 100]: Batch [128 / 159]: Gradient Penalty: 5.3007, D Fake Loss: -7.8340, D Real Loss: 14.6760, Wasserstein Loss: 7.9320, Content Loss: 0.7159, PSNR: 13.27 SSIM: 0.29
0.09702970087528229 0.05425908789038658
0.09793873876333237 0.05445598065853119
0.09790835529565811 0.05366178974509239
0.0999855175614357 0.05412636324763298
0.09916725754737854 0.05349066108465195
0.09802956879138947 0.05372441187500954
0.09773783385753632 0.05265018343925476
0.09780985116958618 0.05326981469988823
0.09885627776384354 0.052419550716876984
0.09968355298042297 0.05315479636192322
0.09997199475765228 0.053263209760189056
0.09997209906578064 0.05226578563451767
0.10253866016864777 0.052743200212717056
0.10014012455940247 0.05215318500995636
0.10148598998785019 0.05289911478757858
0.10061150789260864 0.05249365419149399
===> Epoch [2 / 100]: Batch [144 / 159]: Gradient Penalty: 5.3068, D Fake Loss: -8.7799, D Real Loss: 13.7739, Wasserstein Loss: 8.8883, Content Loss: 0.7122, PSNR: 13.38 SSIM: 0.29
0.10246947407722473 0.05280761793255806
0.10247497260570526 0.05250970274209976
0.10057833790779114 0.052102960646152496
0.10399142652750015 0.05268367752432823
0.10128356516361237 0.05262099951505661
0.10386240482330322 0.052814971655607224
0.10402093082666397 0.05207415670156479
0.10375002026557922 0.05181514099240303
0.10699747502803802 0.05232361704111099
0.10414814949035645 0.051520247012376785
0.10519757866859436 0.05163392052054405
0.10580303519964218 0.05196625739336014
0.10505625605583191 0.05161796137690544
0.10694970935583115 0.05150960758328438
0.10550479590892792 0.051473844796419144
Epoch [2 / 100]: Gradient Penalty: 5.3646, D Fake Loss: -9.7121, D Real Loss: 13.1477, Wasserstein Loss: 9.8203, Content Loss: 0.7096, PSNR: 13.37 SSIM: 0.29
Epoch [3 / 100]
0.10580465197563171 0.05113920196890831
0.10715580731630325 0.05040391534566879
0.10821236670017242 0.05094241350889206
0.10838985443115234 0.051012732088565826
0.11009839177131653 0.05084460973739624
0.10872338712215424 0.050209060311317444
0.10921090841293335 0.04992683604359627
0.10807070136070251 0.050518035888671875
0.11301207542419434 0.04957950860261917
0.10760238021612167 0.04940726235508919
0.11007019132375717 0.04992884024977684
0.11075785756111145 0.050038307905197144
0.10985752940177917 0.04988656938076019
0.10951043665409088 0.048348940908908844
0.10997827351093292 0.04949815943837166
0.11092113703489304 0.0487811379134655
===> Epoch [3 / 100]: Batch [16 / 159]: Gradient Penalty: 5.3232, D Fake Loss: -20.3110, D Real Loss: 2.9782, Wasserstein Loss: 20.4421, Content Loss: 0.6590, PSNR: 14.08 SSIM: 0.33
0.11363273859024048 0.04914303123950958
0.11669207364320755 0.04803832992911339
0.11344526708126068 0.04968159645795822
0.11368688941001892 0.04859621077775955
0.11382560431957245 0.04792158678174019
0.11534684896469116 0.04881100356578827
0.11690422892570496 0.04854438453912735
0.11615166068077087 0.04817041754722595
0.11693105101585388 0.04900749400258064
0.11423399299383163 0.04855303093791008
0.11653159558773041 0.047723859548568726
0.12028864771127701 0.04788592830300331
0.11617547273635864 0.048117745667696
0.11808668076992035 0.047421038150787354
0.11750824749469757 0.04718577861785889
0.1196049153804779 0.046680085361003876
===> Epoch [3 / 100]: Batch [32 / 159]: Gradient Penalty: 5.6054, D Fake Loss: -20.6568, D Real Loss: 3.7911, Wasserstein Loss: 20.7410, Content Loss: 0.6757, PSNR: 13.73 SSIM: 0.33
0.11929956078529358 0.04651867598295212
0.12063826620578766 0.04666515439748764
0.11784204095602036 0.04644778370857239
0.12376077473163605 0.04687342792749405
0.11945100128650665 0.04616614431142807
0.12372013926506042 0.046778373420238495
0.12006086111068726 0.046725355088710785
0.12028148770332336 0.04624101519584656
0.12543338537216187 0.046531256288290024
0.12314137071371078 0.04647018760442734
0.12995389103889465 0.04643303155899048
0.1226891353726387 0.046226002275943756
0.12778128683567047 0.04572506994009018
0.12653020024299622 0.04499870538711548
0.1253037452697754 0.0450773686170578
0.12826260924339294 0.04485096409916878
===> Epoch [3 / 100]: Batch [48 / 159]: Gradient Penalty: 5.6962, D Fake Loss: -20.7599, D Real Loss: 3.9949, Wasserstein Loss: 20.8232, Content Loss: 0.6778, PSNR: 13.65 SSIM: 0.34
0.12517942488193512 0.04446497932076454
0.12639302015304565 0.04480629414319992
0.1240609809756279 0.04362668842077255
0.12907877564430237 0.044609151780605316
0.12470479309558868 0.04421142861247063
0.12622159719467163 0.043852876871824265
0.1302374303340912 0.043705783784389496
0.12694129347801208 0.043994564563035965
0.12831903994083405 0.042509425431489944
0.1285046637058258 0.04302217438817024
0.12838980555534363 0.0430225133895874
0.12895748019218445 0.043511662632226944
0.13059918582439423 0.04220309481024742
0.13747479021549225 0.04200068116188049
0.1337014138698578 0.04167407006025314
0.1313519924879074 0.042206618934869766
===> Epoch [3 / 100]: Batch [64 / 159]: Gradient Penalty: 5.5969, D Fake Loss: -20.7804, D Real Loss: 3.5771, Wasserstein Loss: 20.8070, Content Loss: 0.6747, PSNR: 13.80 SSIM: 0.35
0.13147053122520447 0.04220492020249367
0.14159992337226868 0.04090370982885361
0.1379411816596985 0.041838038712739944
0.13633526861667633 0.04085098206996918
0.1376902163028717 0.04091832414269447
0.1443113088607788 0.04051657393574715
0.13789774477481842 0.04031582549214363
0.1346161961555481 0.03930113464593887
0.1380634605884552 0.040564946830272675
0.14064304530620575 0.04041370749473572
0.1384933441877365 0.039052098989486694
0.14261513948440552 0.03948185592889786
0.14735060930252075 0.039123114198446274
0.13869576156139374 0.03974340856075287
0.14183610677719116 0.03841676563024521
0.15081867575645447 0.0390055812895298
===> Epoch [3 / 100]: Batch [80 / 159]: Gradient Penalty: 5.5699, D Fake Loss: -20.6398, D Real Loss: 3.5930, Wasserstein Loss: 20.6530, Content Loss: 0.6736, PSNR: 13.83 SSIM: 0.35
0.14382216334342957 0.03785278648138046
0.14594148099422455 0.03799593448638916
0.14739185571670532 0.037570398300886154
0.1499396711587906 0.03767941892147064
0.14833073318004608 0.03811010345816612
0.14745968580245972 0.036744192242622375
0.1517029106616974 0.0379750058054924
0.1513015180826187 0.036619409918785095
0.1496077924966812 0.03690623492002487
0.1443927139043808 0.03668215870857239
0.14649051427841187 0.036618854850530624
0.14306306838989258 0.03566252440214157
0.14650675654411316 0.03560156002640724
0.1588270515203476 0.03591121733188629
0.15408645570278168 0.0350206084549427
0.14929315447807312 0.03567986935377121
===> Epoch [3 / 100]: Batch [96 / 159]: Gradient Penalty: 5.5316, D Fake Loss: -20.3666, D Real Loss: 3.6849, Wasserstein Loss: 20.3749, Content Loss: 0.6690, PSNR: 13.87 SSIM: 0.36
0.15685300529003143 0.03551013022661209
0.1568140983581543 0.03554932773113251
0.15419086813926697 0.034457217901945114
0.1628437340259552 0.03428308293223381
0.15390893816947937 0.03414960950613022
0.1531561017036438 0.034136705100536346
0.1566181182861328 0.03428986296057701
0.15838223695755005 0.03403262794017792
0.14928269386291504 0.03423983231186867
0.1535712480545044 0.032807547599077225
0.1657198965549469 0.033160679042339325
0.16553299129009247 0.033368587493896484
0.14995083212852478 0.03343857079744339
0.16457760334014893 0.03291034325957298
0.15779945254325867 0.03248808532953262
0.15523052215576172 0.03230263665318489
===> Epoch [3 / 100]: Batch [112 / 159]: Gradient Penalty: 5.4417, D Fake Loss: -20.0939, D Real Loss: 3.6447, Wasserstein Loss: 20.0999, Content Loss: 0.6620, PSNR: 13.98 SSIM: 0.37
0.1597561538219452 0.03216104954481125
0.15882396697998047 0.03233297914266586
0.16308431327342987 0.03241543471813202
0.1672302484512329 0.03221710026264191
0.158733531832695 0.031938448548316956
0.16252046823501587 0.0320558100938797
0.1669141948223114 0.03171825036406517
0.1599501669406891 0.03139689192175865
0.162428617477417 0.03160098195075989
0.16524970531463623 0.031134892255067825
0.16603511571884155 0.03095802664756775
0.16616438329219818 0.03134988620877266
0.16118885576725006 0.030535763129591942
0.16459323465824127 0.030792996287345886
0.16462551057338715 0.030760498717427254
0.16747058928012848 0.030391430482268333
===> Epoch [3 / 100]: Batch [128 / 159]: Gradient Penalty: 5.3876, D Fake Loss: -19.7738, D Real Loss: 3.7685, Wasserstein Loss: 19.7787, Content Loss: 0.6564, PSNR: 14.05 SSIM: 0.37
0.16701659560203552 0.030080126598477364
0.1787036657333374 0.029967203736305237
0.17203325033187866 0.030337095260620117
0.18234047293663025 0.029596908017992973
0.1680859923362732 0.02956899255514145
0.1727033406496048 0.029696093872189522
0.17462153732776642 0.02904445305466652
0.17457881569862366 0.02878369763493538
0.1856217086315155 0.028562311083078384
0.1733940839767456 0.028913648799061775
0.17533043026924133 0.029042256996035576
0.17655719816684723 0.02876613289117813
0.19211605191230774 0.028384974226355553
0.19439679384231567 0.028636200353503227
0.18244051933288574 0.027988674119114876
0.1823454648256302 0.028260987251996994
===> Epoch [3 / 100]: Batch [144 / 159]: Gradient Penalty: 5.2774, D Fake Loss: -19.5370, D Real Loss: 3.6080, Wasserstein Loss: 19.5417, Content Loss: 0.6522, PSNR: 14.22 SSIM: 0.38
0.1949242651462555 0.027846112847328186
0.1752012073993683 0.02809140644967556
0.1875205934047699 0.027871202677488327
0.1842709183692932 0.027964027598500252
0.18830780684947968 0.02818012423813343
0.1876237839460373 0.027996962890028954
0.17988385260105133 0.027662912383675575
0.17929627001285553 0.027722373604774475
0.18485352396965027 0.027399679645895958
0.18349196016788483 0.02782294899225235
0.18781529366970062 0.027384504675865173
0.1864621341228485 0.027086015790700912
0.2022184133529663 0.027397334575653076
0.1888953149318695 0.027600888162851334
0.19266702234745026 0.027078930288553238
Epoch [3 / 100]: Gradient Penalty: 5.2484, D Fake Loss: -19.2995, D Real Loss: 3.7460, Wasserstein Loss: 19.3029, Content Loss: 0.6492, PSNR: 14.25 SSIM: 0.39
Epoch [4 / 100]
0.18611828982830048 0.026938116177916527
0.19156569242477417 0.027280539274215698
0.19395527243614197 0.026838358491659164
0.1856851875782013 0.02669493295252323
0.18601828813552856 0.026274608448147774
0.20299874246120453 0.02642986550927162
0.1952895075082779 0.02637525647878647
0.19584785401821136 0.026079850271344185
0.18733564019203186 0.02633776143193245
0.18739116191864014 0.026287110522389412
0.20292671024799347 0.02597808465361595
0.1957588940858841 0.02586548775434494
0.2157103419303894 0.026019182056188583
0.19295533001422882 0.026006270200014114
0.20125827193260193 0.02587890625
0.1978810429573059 0.025687191635370255
===> Epoch [4 / 100]: Batch [16 / 159]: Gradient Penalty: 4.3126, D Fake Loss: -17.0292, D Real Loss: 2.7582, Wasserstein Loss: 17.0368, Content Loss: 0.6026, PSNR: 15.60 SSIM: 0.47
0.20188108086585999 0.02614162117242813
0.20652011036872864 0.025803258642554283
0.20254722237586975 0.025972677394747734
0.20390668511390686 0.02589135803282261
0.19801369309425354 0.025815865024924278
0.20245863497257233 0.025811556726694107
0.20208768546581268 0.025550449267029762
0.21096454560756683 0.02581031061708927
0.19993342459201813 0.02552657201886177
0.20923230051994324 0.025853833183646202
0.2152884602546692 0.025502363219857216
0.20152375102043152 0.025807224214076996
0.2103624790906906 0.0259759072214365
0.217105895280838 0.025925884023308754
0.2065095454454422 0.025422925129532814
0.20991960167884827 0.02549944631755352
===> Epoch [4 / 100]: Batch [32 / 159]: Gradient Penalty: 4.6845, D Fake Loss: -16.7210, D Real Loss: 4.4570, Wasserstein Loss: 16.7263, Content Loss: 0.6224, PSNR: 15.03 SSIM: 0.46
0.2035100758075714 0.025592539459466934
0.20828354358673096 0.02569381706416607
0.21685980260372162 0.02526361308991909
0.2126787304878235 0.025412071496248245
0.2068086564540863 0.02553332783281803
0.2070314586162567 0.024993525817990303
0.2193823754787445 0.025279365479946136
0.20723266899585724 0.025283511728048325
0.22750581800937653 0.025291848927736282
0.21063636243343353 0.02552703209221363
0.23122915625572205 0.024869650602340698
0.2171352505683899 0.025425676256418228
0.22453968226909637 0.025316398590803146
0.21368439495563507 0.025239501148462296
0.22722077369689941 0.02551307901740074
0.2141263484954834 0.02529009059071541
===> Epoch [4 / 100]: Batch [48 / 159]: Gradient Penalty: 4.8150, D Fake Loss: -16.4828, D Real Loss: 5.1058, Wasserstein Loss: 16.4809, Content Loss: 0.6235, PSNR: 14.85 SSIM: 0.46
0.2236221432685852 0.025070790201425552
0.20949257910251617 0.02500186488032341
0.2085874080657959 0.02507808990776539
0.2200607806444168 0.025070127099752426
0.20889410376548767 0.025033337995409966
0.20386841893196106 0.02501866966485977
0.20697343349456787 0.025091875344514847
0.22787214815616608 0.02506352588534355
0.2218053787946701 0.024828828871250153
0.2139042317867279 0.025099532678723335
0.22699126601219177 0.02497456595301628
0.21231307089328766 0.02530033141374588
0.222502201795578 0.024940859526395798
0.22191698849201202 0.025309503078460693
0.2176884412765503 0.024990186095237732
0.2219700664281845 0.025058327242732048
===> Epoch [4 / 100]: Batch [64 / 159]: Gradient Penalty: 4.7438, D Fake Loss: -16.3222, D Real Loss: 4.9974, Wasserstein Loss: 16.3159, Content Loss: 0.6165, PSNR: 14.92 SSIM: 0.47
0.22827139496803284 0.024704281240701675
0.2227339893579483 0.02454315684735775
0.22102808952331543 0.024685781449079514
0.2207149863243103 0.024396227672696114
0.22122880816459656 0.02435014769434929
0.22027570009231567 0.02457544021308422
0.23090890049934387 0.024317070841789246
0.2194257229566574 0.024810418486595154
0.2371760755777359 0.024172969162464142
0.2330200970172882 0.024414123967289925
0.24063365161418915 0.02451660856604576
0.2375631332397461 0.024139638990163803
0.22483904659748077 0.0243342537432909
0.233524352312088 0.02425508201122284
0.22111380100250244 0.024436548352241516
0.223716601729393 0.024583859369158745
===> Epoch [4 / 100]: Batch [80 / 159]: Gradient Penalty: 4.6894, D Fake Loss: -16.1561, D Real Loss: 4.9895, Wasserstein Loss: 16.1454, Content Loss: 0.6163, PSNR: 14.94 SSIM: 0.47
0.22726479172706604 0.024641668424010277
0.23569172620773315 0.024171875789761543
0.22953075170516968 0.024714749306440353
0.23148268461227417 0.024609273299574852
0.23509855568408966 0.02406458929181099
0.23451878130435944 0.02427162602543831
0.22451552748680115 0.024319587275385857
0.2331792712211609 0.024462327361106873
0.2277444750070572 0.024515561759471893
0.249009370803833 0.024804500862956047
0.24154099822044373 0.02464495599269867
0.23428384959697723 0.024445904418826103
0.24379132688045502 0.02456441894173622
0.23318368196487427 0.02406422607600689
0.2353726178407669 0.024007722735404968
0.2353895604610443 0.024026505649089813
===> Epoch [4 / 100]: Batch [96 / 159]: Gradient Penalty: 4.6666, D Fake Loss: -16.0366, D Real Loss: 5.0174, Wasserstein Loss: 16.0315, Content Loss: 0.6133, PSNR: 14.97 SSIM: 0.47
0.26271218061447144 0.023824281990528107
0.23319679498672485 0.024048596620559692
0.2285851538181305 0.02405821532011032
0.2367689609527588 0.024108750745654106
0.24921108782291412 0.02394183911383152
0.23870643973350525 0.023852327838540077
0.23255710303783417 0.023961331695318222
0.24276971817016602 0.024010322988033295
0.2267814725637436 0.024281365796923637
0.22443614900112152 0.02417941391468048
0.20812660455703735 0.024273807182908058
0.23158837854862213 0.023889752104878426
0.22567813098430634 0.0242801271378994
0.24527107179164886 0.024000033736228943
0.23879781365394592 0.02413870207965374
0.22411271929740906 0.023906802758574486
===> Epoch [4 / 100]: Batch [112 / 159]: Gradient Penalty: 4.5768, D Fake Loss: -16.0112, D Real Loss: 4.7410, Wasserstein Loss: 16.0004, Content Loss: 0.6086, PSNR: 15.08 SSIM: 0.48
0.23286210000514984 0.023735569790005684
0.23883463442325592 0.023673081770539284
0.2530319392681122 0.02383446879684925
0.2530122697353363 0.02414792776107788
0.23300838470458984 0.023936836048960686
0.241468608379364 0.023198114708065987
0.24403168261051178 0.021785877645015717
0.23476551473140717 0.024204177781939507
0.25127655267715454 0.022361619397997856
0.2270965725183487 0.02375701628625393
0.23532290756702423 0.02394128404557705
0.24490639567375183 0.02204178459942341
0.24633456766605377 0.0239984430372715
0.2344941794872284 0.024164319038391113
0.23294277489185333 0.02405005879700184
0.23755088448524475 0.024283461272716522
===> Epoch [4 / 100]: Batch [128 / 159]: Gradient Penalty: 4.5770, D Fake Loss: -15.9275, D Real Loss: 4.8007, Wasserstein Loss: 15.9286, Content Loss: 0.6038, PSNR: 15.11 SSIM: 0.48
0.2441064417362213 0.023945709690451622
0.24600937962532043 0.024262243881821632
0.26367470622062683 0.023966483771800995
0.24237202107906342 0.023814553394913673
0.23407724499702454 0.02398427575826645
0.2623502016067505 0.02374427579343319
0.2323034107685089 0.023827925324440002
0.24147701263427734 0.02379848062992096
0.24024811387062073 0.023594355210661888
0.24788129329681396 0.023217348381876945
0.25615808367729187 0.023306522518396378
0.22815917432308197 0.023155177012085915
0.2669285833835602 0.023796996101737022
0.25051501393318176 0.023233802989125252
0.2530631422996521 0.023557916283607483
0.26995742321014404 0.023319624364376068
===> Epoch [4 / 100]: Batch [144 / 159]: Gradient Penalty: 4.5243, D Fake Loss: -15.9586, D Real Loss: 4.5401, Wasserstein Loss: 15.9625, Content Loss: 0.6024, PSNR: 15.23 SSIM: 0.48
0.24768966436386108 0.02356797270476818
0.24683494865894318 0.02347579412162304
0.24548979103565216 0.02340422198176384
0.23835322260856628 0.022663883864879608
0.25148728489875793 0.02341771312057972
0.2379780113697052 0.023442160338163376
0.23395806550979614 0.023554865270853043
0.23919624090194702 0.02340548485517502
0.23809394240379333 0.023649845272302628
0.24731862545013428 0.0235736146569252
0.24521061778068542 0.023793159052729607
0.24344517290592194 0.023662475869059563
0.25521889328956604 0.02372703328728676
0.24486742913722992 0.023657677695155144
0.2460734248161316 0.024019701406359673
Epoch [4 / 100]: Gradient Penalty: 4.5246, D Fake Loss: -15.9667, D Real Loss: 4.5391, Wasserstein Loss: 15.9636, Content Loss: 0.6001, PSNR: 15.25 SSIM: 0.48
Epoch [5 / 100]
0.25707101821899414 0.023719092831015587
0.23617255687713623 0.023584213107824326
0.2518354058265686 0.02343805320560932
0.24029575288295746 0.02113601565361023
0.2605966627597809 0.02372279204428196
0.2597530484199524 0.024054093286395073
0.2393525242805481 0.023219002410769463
0.25727301836013794 0.022812901064753532
0.24448420107364655 0.02314457669854164
0.24309766292572021 0.023472417145967484
0.2286311239004135 0.023211359977722168
0.24289874732494354 0.023080995306372643
0.23490457236766815 0.023127201944589615
0.24049699306488037 0.023012341931462288
0.25232622027397156 0.023268939927220345
0.2445012331008911 0.023103008046746254
===> Epoch [5 / 100]: Batch [16 / 159]: Gradient Penalty: 4.1441, D Fake Loss: -16.3645, D Real Loss: 2.5573, Wasserstein Loss: 16.3416, Content Loss: 0.5536, PSNR: 16.02 SSIM: 0.51
0.23558790981769562 0.02308817207813263
0.27138978242874146 0.022354895249009132
0.24768267571926117 0.0230458565056324
0.2579566538333893 0.023549409583210945
0.2580828070640564 0.023009592667222023
0.2567797303199768 0.018708206713199615
0.2602134048938751 0.02317798137664795
0.24942508339881897 0.02332046441733837
0.24643270671367645 0.02331596612930298
0.24932168424129486 0.0234067365527153
0.2573278844356537 0.023247525095939636
0.22978909313678741 0.02355056069791317
0.24723225831985474 0.023550087586045265
0.24661463499069214 0.023531848564743996
0.24820809066295624 0.02348133735358715
0.2635951042175293 0.02322244457900524
===> Epoch [5 / 100]: Batch [32 / 159]: Gradient Penalty: 4.4781, D Fake Loss: -16.0948, D Real Loss: 4.1046, Wasserstein Loss: 16.1361, Content Loss: 0.5774, PSNR: 15.49 SSIM: 0.50
0.25612589716911316 0.02338474616408348
0.2569146454334259 0.023541860282421112
0.25435638427734375 0.023432618007063866
0.24364502727985382 0.023637061938643456
0.252216637134552 0.02336406707763672
0.27029913663864136 0.023020295426249504
0.2507183253765106 0.02325288951396942
0.25583571195602417 0.023718779906630516
0.2515344023704529 0.023533286526799202
0.26814424991607666 0.023220457136631012
0.24818173050880432 0.023151908069849014
0.26606351137161255 0.02326936088502407
0.26239013671875 0.0234568789601326
0.2494419813156128 0.023977315053343773
0.2572707533836365 0.023695476353168488
0.24606388807296753 0.023537125438451767
===> Epoch [5 / 100]: Batch [48 / 159]: Gradient Penalty: 4.5907, D Fake Loss: -16.0252, D Real Loss: 4.5649, Wasserstein Loss: 16.0450, Content Loss: 0.5826, PSNR: 15.27 SSIM: 0.50
0.28056392073631287 0.023877229541540146
0.24199213087558746 0.023654162883758545
0.23377348482608795 0.023827409371733665
0.24218761920928955 0.023602452129125595
0.2577454745769501 0.01955263316631317
0.25351840257644653 0.023563843220472336
0.2642434239387512 0.023556314408779144
0.25697773694992065 0.021488647907972336
0.25812405347824097 0.023717554286122322
0.27850598096847534 0.023894377052783966
0.25617414712905884 0.023601653054356575
0.26360470056533813 0.02383900247514248
0.2521176338195801 0.023560235276818275
0.2658572494983673 0.02374488115310669
0.25118720531463623 0.02377595379948616
0.26417797803878784 0.023698896169662476
===> Epoch [5 / 100]: Batch [64 / 159]: Gradient Penalty: 4.5689, D Fake Loss: -16.1746, D Real Loss: 4.3024, Wasserstein Loss: 16.1950, Content Loss: 0.5795, PSNR: 15.36 SSIM: 0.50
0.24929426610469818 0.02290402539074421
0.2599601149559021 0.02295219898223877
0.2742328345775604 0.024209123104810715
0.27745479345321655 0.0231348667293787
0.2484493851661682 0.01943666860461235
0.2513919770717621 0.02331336960196495
0.24969297647476196 0.023468168452382088
0.26637402176856995 0.023740891367197037
0.2446267455816269 0.020421229302883148
0.2574774920940399 0.023819439113140106
0.26976680755615234 0.022286541759967804
0.25566792488098145 0.02263054996728897
0.2494688332080841 0.022608980536460876
0.2798457145690918 0.021911069750785828
0.24379195272922516 0.023113494738936424
0.29315900802612305 0.016491346061229706
===> Epoch [5 / 100]: Batch [80 / 159]: Gradient Penalty: 4.6130, D Fake Loss: -16.2117, D Real Loss: 4.3619, Wasserstein Loss: 16.2325, Content Loss: 0.5773, PSNR: 15.30 SSIM: 0.50
0.2670567035675049 0.023413075134158134
0.2753013074398041 0.023273000493645668
0.2635197937488556 0.02382257580757141
0.2698606252670288 0.02213236317038536
0.2586378753185272 0.023331908509135246
0.2586200535297394 0.023335050791502
0.26274824142456055 0.02266532927751541
0.23849429190158844 0.02268492802977562
0.27161499857902527 0.022863207384943962
0.2651262581348419 0.02327689714729786
0.2698046565055847 0.023634396493434906
0.269336074590683 0.022544734179973602
0.2652241289615631 0.021695803850889206
0.272216796875 0.022624578326940536
0.2676403820514679 0.02201104536652565
0.24513684213161469 0.021868150681257248
===> Epoch [5 / 100]: Batch [96 / 159]: Gradient Penalty: 4.6123, D Fake Loss: -16.2810, D Real Loss: 4.2615, Wasserstein Loss: 16.3040, Content Loss: 0.5713, PSNR: 15.33 SSIM: 0.50
0.2514902353286743 0.022316738963127136
0.24455538392066956 0.022137220948934555
0.2773362100124359 0.02286812663078308
0.27005088329315186 0.022882509976625443
0.26801401376724243 0.021937325596809387
0.2815445065498352 0.017790287733078003
0.2797430157661438 0.0223948135972023
0.23733682930469513 0.021854303777217865
0.26561641693115234 0.02283705398440361
0.25816643238067627 0.0213068425655365
0.25798898935317993 0.022380467504262924
0.25840553641319275 0.02216537669301033
0.25233814120292664 0.022236701101064682
0.26720839738845825 0.022719930857419968
0.26259100437164307 0.02081792801618576
0.2697192430496216 0.022066466510295868
===> Epoch [5 / 100]: Batch [112 / 159]: Gradient Penalty: 4.6190, D Fake Loss: -16.3725, D Real Loss: 4.1327, Wasserstein Loss: 16.3900, Content Loss: 0.5673, PSNR: 15.38 SSIM: 0.50
0.2550177276134491 0.020998403429985046
0.2664145827293396 0.023716900497674942
0.25442320108413696 0.023623811081051826
0.2668117582798004 0.023379119113087654
0.25398755073547363 0.022095967084169388
0.25684502720832825 0.020979806780815125
0.2590309679508209 0.02142423763871193
0.2833218276500702 0.023486556485295296
0.2712361216545105 0.018846213817596436
0.27735206484794617 0.022315271198749542
0.2571873068809509 0.021372832357883453
0.2747788429260254 0.021662741899490356
0.2761843204498291 0.021931424736976624
0.2513222098350525 0.02164660021662712
0.2692905068397522 0.021955043077468872
0.2505126893520355 0.02145380899310112
===> Epoch [5 / 100]: Batch [128 / 159]: Gradient Penalty: 4.6300, D Fake Loss: -16.3402, D Real Loss: 4.2220, Wasserstein Loss: 16.3472, Content Loss: 0.5628, PSNR: 15.36 SSIM: 0.50
0.2707916796207428 0.021126825362443924
0.2673242688179016 0.020442742854356766
0.2608907222747803 0.02216985821723938
0.2811858355998993 0.01908980682492256
0.24863895773887634 0.020951002836227417
0.25338318943977356 0.020417939871549606
0.2696962058544159 0.021944645792245865
0.2561546564102173 0.020198822021484375
0.248031347990036 0.020911570638418198
0.2526126503944397 0.021143335849046707
0.25969597697257996 0.02146279066801071
0.2672503590583801 0.02056243270635605
0.2724410593509674 0.021653205156326294
0.26906779408454895 0.021934352815151215
0.2702457308769226 0.01748811826109886
0.25573644042015076 0.021239332854747772
===> Epoch [5 / 100]: Batch [144 / 159]: Gradient Penalty: 4.5928, D Fake Loss: -16.4293, D Real Loss: 3.9743, Wasserstein Loss: 16.4439, Content Loss: 0.5596, PSNR: 15.47 SSIM: 0.50
0.25669988989830017 0.02161337062716484
0.2640421390533447 0.02155984938144684
0.25626254081726074 0.01919156312942505
0.26736950874328613 0.020720049738883972
0.2520809769630432 0.02099963277578354
0.25023290514945984 0.01977747306227684
0.255746990442276 0.021366406232118607
0.2660196125507355 0.019111268222332
0.26835113763809204 0.020670820027589798
0.2642804980278015 0.02058711275458336
0.2532753646373749 0.016314595937728882
0.25794482231140137 0.021399423480033875
0.2612529993057251 0.018014051020145416
0.2575470209121704 0.016065798699855804
0.25993674993515015 0.021217811852693558
Epoch [5 / 100]: Gradient Penalty: 4.6224, D Fake Loss: -16.4761, D Real Loss: 4.0479, Wasserstein Loss: 16.4825, Content Loss: 0.5596, PSNR: 15.44 SSIM: 0.50
Epoch [6 / 100]
===> Epoch [6 / 100]: Batch [16 / 159]: Gradient Penalty: 4.3498, D Fake Loss: -17.2028, D Real Loss: 2.1144, Wasserstein Loss: 17.2639, Content Loss: 0.5361, PSNR: 15.90 SSIM: 0.51
===> Epoch [6 / 100]: Batch [32 / 159]: Gradient Penalty: 4.6439, D Fake Loss: -17.1195, D Real Loss: 3.4851, Wasserstein Loss: 17.1211, Content Loss: 0.5425, PSNR: 15.48 SSIM: 0.51
===> Epoch [6 / 100]: Batch [48 / 159]: Gradient Penalty: 4.7299, D Fake Loss: -17.0939, D Real Loss: 3.8653, Wasserstein Loss: 17.0910, Content Loss: 0.5446, PSNR: 15.30 SSIM: 0.51
===> Epoch [6 / 100]: Batch [64 / 159]: Gradient Penalty: 4.7793, D Fake Loss: -17.1269, D Real Loss: 3.9640, Wasserstein Loss: 17.1457, Content Loss: 0.5410, PSNR: 15.29 SSIM: 0.51
===> Epoch [6 / 100]: Batch [80 / 159]: Gradient Penalty: 4.7360, D Fake Loss: -17.1724, D Real Loss: 3.7821, Wasserstein Loss: 17.1879, Content Loss: 0.5389, PSNR: 15.31 SSIM: 0.51
===> Epoch [6 / 100]: Batch [96 / 159]: Gradient Penalty: 4.6967, D Fake Loss: -17.2435, D Real Loss: 3.5783, Wasserstein Loss: 17.2574, Content Loss: 0.5341, PSNR: 15.35 SSIM: 0.51
===> Epoch [6 / 100]: Batch [112 / 159]: Gradient Penalty: 4.6723, D Fake Loss: -17.3074, D Real Loss: 3.4292, Wasserstein Loss: 17.3142, Content Loss: 0.5316, PSNR: 15.40 SSIM: 0.51
===> Epoch [6 / 100]: Batch [128 / 159]: Gradient Penalty: 4.6692, D Fake Loss: -17.3196, D Real Loss: 3.4047, Wasserstein Loss: 17.3314, Content Loss: 0.5290, PSNR: 15.41 SSIM: 0.51
===> Epoch [6 / 100]: Batch [144 / 159]: Gradient Penalty: 4.6560, D Fake Loss: -17.4027, D Real Loss: 3.2381, Wasserstein Loss: 17.4151, Content Loss: 0.5252, PSNR: 15.48 SSIM: 0.51
Epoch [6 / 100]: Gradient Penalty: 4.6703, D Fake Loss: -17.4557, D Real Loss: 3.2410, Wasserstein Loss: 17.4586, Content Loss: 0.5238, PSNR: 15.46 SSIM: 0.51
Epoch [7 / 100]
===> Epoch [7 / 100]: Batch [16 / 159]: Gradient Penalty: 4.4869, D Fake Loss: -17.8508, D Real Loss: 2.0224, Wasserstein Loss: 17.9043, Content Loss: 0.4754, PSNR: 15.76 SSIM: 0.53
===> Epoch [7 / 100]: Batch [32 / 159]: Gradient Penalty: 4.7174, D Fake Loss: -17.5803, D Real Loss: 3.2637, Wasserstein Loss: 17.6396, Content Loss: 0.4946, PSNR: 15.42 SSIM: 0.52
===> Epoch [7 / 100]: Batch [48 / 159]: Gradient Penalty: 4.7491, D Fake Loss: -17.5464, D Real Loss: 3.4902, Wasserstein Loss: 17.5545, Content Loss: 0.5050, PSNR: 15.26 SSIM: 0.52
===> Epoch [7 / 100]: Batch [64 / 159]: Gradient Penalty: 4.6857, D Fake Loss: -17.5210, D Real Loss: 3.2846, Wasserstein Loss: 17.5457, Content Loss: 0.5006, PSNR: 15.43 SSIM: 0.52
===> Epoch [7 / 100]: Batch [80 / 159]: Gradient Penalty: 4.6777, D Fake Loss: -17.5412, D Real Loss: 3.2425, Wasserstein Loss: 17.5596, Content Loss: 0.5004, PSNR: 15.40 SSIM: 0.52
===> Epoch [7 / 100]: Batch [96 / 159]: Gradient Penalty: 4.6517, D Fake Loss: -17.4685, D Real Loss: 3.2164, Wasserstein Loss: 17.4725, Content Loss: 0.4956, PSNR: 15.41 SSIM: 0.53
===> Epoch [7 / 100]: Batch [112 / 159]: Gradient Penalty: 4.6187, D Fake Loss: -17.4169, D Real Loss: 3.1401, Wasserstein Loss: 17.4225, Content Loss: 0.4919, PSNR: 15.47 SSIM: 0.53
===> Epoch [7 / 100]: Batch [128 / 159]: Gradient Penalty: 4.5881, D Fake Loss: -17.3554, D Real Loss: 3.1149, Wasserstein Loss: 17.3629, Content Loss: 0.4890, PSNR: 15.50 SSIM: 0.53
===> Epoch [7 / 100]: Batch [144 / 159]: Gradient Penalty: 4.5527, D Fake Loss: -17.3892, D Real Loss: 2.9008, Wasserstein Loss: 17.4018, Content Loss: 0.4859, PSNR: 15.62 SSIM: 0.53
Epoch [7 / 100]: Gradient Penalty: 4.5576, D Fake Loss: -17.3641, D Real Loss: 2.9664, Wasserstein Loss: 17.3752, Content Loss: 0.4844, PSNR: 15.60 SSIM: 0.53
Epoch [8 / 100]
===> Epoch [8 / 100]: Batch [16 / 159]: Gradient Penalty: 4.1775, D Fake Loss: -17.3988, D Real Loss: 1.6253, Wasserstein Loss: 17.4112, Content Loss: 0.4548, PSNR: 16.14 SSIM: 0.55
===> Epoch [8 / 100]: Batch [32 / 159]: Gradient Penalty: 4.4648, D Fake Loss: -17.1314, D Real Loss: 2.9562, Wasserstein Loss: 17.1133, Content Loss: 0.4691, PSNR: 15.79 SSIM: 0.55
===> Epoch [8 / 100]: Batch [48 / 159]: Gradient Penalty: 4.4593, D Fake Loss: -17.0369, D Real Loss: 3.0509, Wasserstein Loss: 17.0344, Content Loss: 0.4708, PSNR: 15.72 SSIM: 0.55
===> Epoch [8 / 100]: Batch [64 / 159]: Gradient Penalty: 4.3849, D Fake Loss: -17.0938, D Real Loss: 2.7229, Wasserstein Loss: 17.0893, Content Loss: 0.4657, PSNR: 15.83 SSIM: 0.56
===> Epoch [8 / 100]: Batch [80 / 159]: Gradient Penalty: 4.3361, D Fake Loss: -17.0685, D Real Loss: 2.5166, Wasserstein Loss: 17.0657, Content Loss: 0.4645, PSNR: 15.90 SSIM: 0.56
===> Epoch [8 / 100]: Batch [96 / 159]: Gradient Penalty: 4.3084, D Fake Loss: -16.9334, D Real Loss: 2.5490, Wasserstein Loss: 16.9367, Content Loss: 0.4605, PSNR: 15.92 SSIM: 0.56
===> Epoch [8 / 100]: Batch [112 / 159]: Gradient Penalty: 4.2708, D Fake Loss: -16.8612, D Real Loss: 2.4913, Wasserstein Loss: 16.8590, Content Loss: 0.4572, PSNR: 15.98 SSIM: 0.56
===> Epoch [8 / 100]: Batch [128 / 159]: Gradient Penalty: 4.2496, D Fake Loss: -16.7605, D Real Loss: 2.5212, Wasserstein Loss: 16.7595, Content Loss: 0.4550, PSNR: 15.99 SSIM: 0.57
===> Epoch [8 / 100]: Batch [144 / 159]: Gradient Penalty: 4.2209, D Fake Loss: -16.7394, D Real Loss: 2.3972, Wasserstein Loss: 16.7446, Content Loss: 0.4522, PSNR: 16.12 SSIM: 0.57
Epoch [8 / 100]: Gradient Penalty: 4.2006, D Fake Loss: -16.6938, D Real Loss: 2.3905, Wasserstein Loss: 16.6982, Content Loss: 0.4522, PSNR: 16.12 SSIM: 0.57
Epoch [9 / 100]
===> Epoch [9 / 100]: Batch [16 / 159]: Gradient Penalty: 3.5796, D Fake Loss: -16.4343, D Real Loss: 0.2735, Wasserstein Loss: 16.4135, Content Loss: 0.4138, PSNR: 17.26 SSIM: 0.61
===> Epoch [9 / 100]: Batch [32 / 159]: Gradient Penalty: 3.7890, D Fake Loss: -16.2049, D Real Loss: 1.4458, Wasserstein Loss: 16.2166, Content Loss: 0.4310, PSNR: 16.77 SSIM: 0.61
===> Epoch [9 / 100]: Batch [48 / 159]: Gradient Penalty: 3.8570, D Fake Loss: -15.9844, D Real Loss: 1.8795, Wasserstein Loss: 16.0133, Content Loss: 0.4337, PSNR: 16.63 SSIM: 0.61
===> Epoch [9 / 100]: Batch [64 / 159]: Gradient Penalty: 3.7490, D Fake Loss: -16.0100, D Real Loss: 1.5182, Wasserstein Loss: 16.0092, Content Loss: 0.4321, PSNR: 16.73 SSIM: 0.61
===> Epoch [9 / 100]: Batch [80 / 159]: Gradient Penalty: 3.7143, D Fake Loss: -15.8539, D Real Loss: 1.5356, Wasserstein Loss: 15.8613, Content Loss: 0.4335, PSNR: 16.73 SSIM: 0.61
===> Epoch [9 / 100]: Batch [96 / 159]: Gradient Penalty: 3.6611, D Fake Loss: -15.7110, D Real Loss: 1.5237, Wasserstein Loss: 15.7053, Content Loss: 0.4316, PSNR: 16.76 SSIM: 0.62
===> Epoch [9 / 100]: Batch [112 / 159]: Gradient Penalty: 3.6248, D Fake Loss: -15.6327, D Real Loss: 1.4625, Wasserstein Loss: 15.6262, Content Loss: 0.4272, PSNR: 16.83 SSIM: 0.62
===> Epoch [9 / 100]: Batch [128 / 159]: Gradient Penalty: 3.5969, D Fake Loss: -15.5375, D Real Loss: 1.4579, Wasserstein Loss: 15.5470, Content Loss: 0.4255, PSNR: 16.85 SSIM: 0.62
===> Epoch [9 / 100]: Batch [144 / 159]: Gradient Penalty: 3.5481, D Fake Loss: -15.5534, D Real Loss: 1.2385, Wasserstein Loss: 15.5601, Content Loss: 0.4246, PSNR: 17.00 SSIM: 0.62
Epoch [9 / 100]: Gradient Penalty: 3.5121, D Fake Loss: -15.5207, D Real Loss: 1.1422, Wasserstein Loss: 15.5296, Content Loss: 0.4260, PSNR: 17.03 SSIM: 0.63
Epoch [10 / 100]
===> Epoch [10 / 100]: Batch [16 / 159]: Gradient Penalty: 3.0184, D Fake Loss: -15.3916, D Real Loss: -0.5750, Wasserstein Loss: 15.3364, Content Loss: 0.4010, PSNR: 17.99 SSIM: 0.66
===> Epoch [10 / 100]: Batch [32 / 159]: Gradient Penalty: 3.1308, D Fake Loss: -14.9199, D Real Loss: 0.4550, Wasserstein Loss: 14.9322, Content Loss: 0.4149, PSNR: 17.62 SSIM: 0.66
===> Epoch [10 / 100]: Batch [48 / 159]: Gradient Penalty: 3.1531, D Fake Loss: -14.7673, D Real Loss: 0.6613, Wasserstein Loss: 14.7806, Content Loss: 0.4188, PSNR: 17.53 SSIM: 0.66
===> Epoch [10 / 100]: Batch [64 / 159]: Gradient Penalty: 3.0637, D Fake Loss: -14.7423, D Real Loss: 0.3632, Wasserstein Loss: 14.7459, Content Loss: 0.4154, PSNR: 17.65 SSIM: 0.66
===> Epoch [10 / 100]: Batch [80 / 159]: Gradient Penalty: 3.0237, D Fake Loss: -14.6683, D Real Loss: 0.2937, Wasserstein Loss: 14.6839, Content Loss: 0.4141, PSNR: 17.69 SSIM: 0.67
===> Epoch [10 / 100]: Batch [96 / 159]: Gradient Penalty: 2.9616, D Fake Loss: -14.6290, D Real Loss: 0.0970, Wasserstein Loss: 14.6271, Content Loss: 0.4125, PSNR: 17.79 SSIM: 0.67
===> Epoch [10 / 100]: Batch [112 / 159]: Gradient Penalty: 2.9203, D Fake Loss: -14.5509, D Real Loss: 0.0036, Wasserstein Loss: 14.5478, Content Loss: 0.4091, PSNR: 17.90 SSIM: 0.67
===> Epoch [10 / 100]: Batch [128 / 159]: Gradient Penalty: 2.8883, D Fake Loss: -14.4085, D Real Loss: 0.0342, Wasserstein Loss: 14.4003, Content Loss: 0.4059, PSNR: 17.96 SSIM: 0.68
===> Epoch [10 / 100]: Batch [144 / 159]: Gradient Penalty: 2.8475, D Fake Loss: -14.3835, D Real Loss: -0.1369, Wasserstein Loss: 14.3828, Content Loss: 0.4043, PSNR: 18.10 SSIM: 0.68
Epoch [10 / 100]: Gradient Penalty: 2.8164, D Fake Loss: -14.2994, D Real Loss: -0.1745, Wasserstein Loss: 14.2995, Content Loss: 0.4051, PSNR: 18.15 SSIM: 0.68
Epoch [11 / 100]
===> Epoch [11 / 100]: Batch [16 / 159]: Gradient Penalty: 2.2454, D Fake Loss: -13.9355, D Real Loss: -2.0388, Wasserstein Loss: 13.9711, Content Loss: 0.3747, PSNR: 19.45 SSIM: 0.72
===> Epoch [11 / 100]: Batch [32 / 159]: Gradient Penalty: 2.4089, D Fake Loss: -13.3274, D Real Loss: -0.6334, Wasserstein Loss: 13.3476, Content Loss: 0.3951, PSNR: 18.93 SSIM: 0.71
===> Epoch [11 / 100]: Batch [48 / 159]: Gradient Penalty: 2.4074, D Fake Loss: -13.1770, D Real Loss: -0.5072, Wasserstein Loss: 13.1928, Content Loss: 0.4005, PSNR: 18.81 SSIM: 0.71
===> Epoch [11 / 100]: Batch [64 / 159]: Gradient Penalty: 2.3526, D Fake Loss: -13.1940, D Real Loss: -0.7597, Wasserstein Loss: 13.2029, Content Loss: 0.3990, PSNR: 18.97 SSIM: 0.71
===> Epoch [11 / 100]: Batch [80 / 159]: Gradient Penalty: 2.3192, D Fake Loss: -13.1801, D Real Loss: -0.8702, Wasserstein Loss: 13.1835, Content Loss: 0.3988, PSNR: 18.99 SSIM: 0.72
===> Epoch [11 / 100]: Batch [96 / 159]: Gradient Penalty: 2.2975, D Fake Loss: -13.2100, D Real Loss: -1.0229, Wasserstein Loss: 13.2328, Content Loss: 0.3974, PSNR: 19.06 SSIM: 0.72
===> Epoch [11 / 100]: Batch [112 / 159]: Gradient Penalty: 2.2715, D Fake Loss: -13.1789, D Real Loss: -1.0989, Wasserstein Loss: 13.1856, Content Loss: 0.3947, PSNR: 19.13 SSIM: 0.72
===> Epoch [11 / 100]: Batch [128 / 159]: Gradient Penalty: 2.2405, D Fake Loss: -13.1086, D Real Loss: -1.1519, Wasserstein Loss: 13.1198, Content Loss: 0.3938, PSNR: 19.20 SSIM: 0.72
===> Epoch [11 / 100]: Batch [144 / 159]: Gradient Penalty: 2.2154, D Fake Loss: -13.1235, D Real Loss: -1.3034, Wasserstein Loss: 13.1376, Content Loss: 0.3910, PSNR: 19.35 SSIM: 0.72
Epoch [11 / 100]: Gradient Penalty: 2.1977, D Fake Loss: -13.0588, D Real Loss: -1.2936, Wasserstein Loss: 13.0803, Content Loss: 0.3909, PSNR: 19.37 SSIM: 0.72
Epoch [12 / 100]
===> Epoch [12 / 100]: Batch [16 / 159]: Gradient Penalty: 1.8626, D Fake Loss: -13.1212, D Real Loss: -2.7954, Wasserstein Loss: 13.0527, Content Loss: 0.3744, PSNR: 20.20 SSIM: 0.75
===> Epoch [12 / 100]: Batch [32 / 159]: Gradient Penalty: 1.9129, D Fake Loss: -12.7894, D Real Loss: -2.1432, Wasserstein Loss: 12.8095, Content Loss: 0.3924, PSNR: 19.97 SSIM: 0.74
===> Epoch [12 / 100]: Batch [48 / 159]: Gradient Penalty: 1.9431, D Fake Loss: -12.6001, D Real Loss: -1.7990, Wasserstein Loss: 12.6059, Content Loss: 0.3959, PSNR: 19.83 SSIM: 0.74
===> Epoch [12 / 100]: Batch [64 / 159]: Gradient Penalty: 1.9103, D Fake Loss: -12.6537, D Real Loss: -1.9885, Wasserstein Loss: 12.6242, Content Loss: 0.3923, PSNR: 19.92 SSIM: 0.74
===> Epoch [12 / 100]: Batch [80 / 159]: Gradient Penalty: 1.9002, D Fake Loss: -12.6414, D Real Loss: -2.0199, Wasserstein Loss: 12.6250, Content Loss: 0.3884, PSNR: 19.97 SSIM: 0.74
===> Epoch [12 / 100]: Batch [96 / 159]: Gradient Penalty: 1.8746, D Fake Loss: -12.6318, D Real Loss: -2.1090, Wasserstein Loss: 12.6090, Content Loss: 0.3851, PSNR: 20.02 SSIM: 0.75
===> Epoch [12 / 100]: Batch [112 / 159]: Gradient Penalty: 1.8549, D Fake Loss: -12.6390, D Real Loss: -2.2351, Wasserstein Loss: 12.6357, Content Loss: 0.3830, PSNR: 20.14 SSIM: 0.75
===> Epoch [12 / 100]: Batch [128 / 159]: Gradient Penalty: 1.8396, D Fake Loss: -12.6134, D Real Loss: -2.2764, Wasserstein Loss: 12.6129, Content Loss: 0.3821, PSNR: 20.18 SSIM: 0.75
===> Epoch [12 / 100]: Batch [144 / 159]: Gradient Penalty: 1.8213, D Fake Loss: -12.6760, D Real Loss: -2.4454, Wasserstein Loss: 12.6838, Content Loss: 0.3808, PSNR: 20.32 SSIM: 0.75
Epoch [12 / 100]: Gradient Penalty: 1.8161, D Fake Loss: -12.6120, D Real Loss: -2.3998, Wasserstein Loss: 12.6245, Content Loss: 0.3798, PSNR: 20.33 SSIM: 0.75
Epoch [13 / 100]
===> Epoch [13 / 100]: Batch [16 / 159]: Gradient Penalty: 1.5372, D Fake Loss: -12.5536, D Real Loss: -3.5336, Wasserstein Loss: 12.4855, Content Loss: 0.3571, PSNR: 21.28 SSIM: 0.77
===> Epoch [13 / 100]: Batch [32 / 159]: Gradient Penalty: 1.6112, D Fake Loss: -12.2535, D Real Loss: -2.7887, Wasserstein Loss: 12.2215, Content Loss: 0.3751, PSNR: 20.88 SSIM: 0.76
===> Epoch [13 / 100]: Batch [48 / 159]: Gradient Penalty: 1.6236, D Fake Loss: -12.2306, D Real Loss: -2.6963, Wasserstein Loss: 12.1873, Content Loss: 0.3790, PSNR: 20.76 SSIM: 0.76
===> Epoch [13 / 100]: Batch [64 / 159]: Gradient Penalty: 1.6188, D Fake Loss: -12.2414, D Real Loss: -2.7950, Wasserstein Loss: 12.2303, Content Loss: 0.3759, PSNR: 20.85 SSIM: 0.77
===> Epoch [13 / 100]: Batch [80 / 159]: Gradient Penalty: 1.6058, D Fake Loss: -12.2701, D Real Loss: -2.8724, Wasserstein Loss: 12.2580, Content Loss: 0.3755, PSNR: 20.88 SSIM: 0.77
===> Epoch [13 / 100]: Batch [96 / 159]: Gradient Penalty: 1.5830, D Fake Loss: -12.3318, D Real Loss: -3.0519, Wasserstein Loss: 12.3320, Content Loss: 0.3743, PSNR: 20.96 SSIM: 0.77
===> Epoch [13 / 100]: Batch [112 / 159]: Gradient Penalty: 1.5674, D Fake Loss: -12.3334, D Real Loss: -3.1413, Wasserstein Loss: 12.3220, Content Loss: 0.3720, PSNR: 21.02 SSIM: 0.77
===> Epoch [13 / 100]: Batch [128 / 159]: Gradient Penalty: 1.5468, D Fake Loss: -12.3046, D Real Loss: -3.1978, Wasserstein Loss: 12.2969, Content Loss: 0.3726, PSNR: 21.06 SSIM: 0.77
===> Epoch [13 / 100]: Batch [144 / 159]: Gradient Penalty: 1.5391, D Fake Loss: -12.3207, D Real Loss: -3.2866, Wasserstein Loss: 12.3216, Content Loss: 0.3717, PSNR: 21.15 SSIM: 0.77
Epoch [13 / 100]: Gradient Penalty: 1.5312, D Fake Loss: -12.2604, D Real Loss: -3.2500, Wasserstein Loss: 12.2610, Content Loss: 0.3724, PSNR: 21.16 SSIM: 0.77
Epoch [14 / 100]
===> Epoch [14 / 100]: Batch [16 / 159]: Gradient Penalty: 1.3146, D Fake Loss: -12.2325, D Real Loss: -4.0631, Wasserstein Loss: 12.2179, Content Loss: 0.3702, PSNR: 21.77 SSIM: 0.78
===> Epoch [14 / 100]: Batch [32 / 159]: Gradient Penalty: 1.3720, D Fake Loss: -11.9125, D Real Loss: -3.4675, Wasserstein Loss: 11.8714, Content Loss: 0.3782, PSNR: 21.46 SSIM: 0.78
===> Epoch [14 / 100]: Batch [48 / 159]: Gradient Penalty: 1.3999, D Fake Loss: -11.8089, D Real Loss: -3.2629, Wasserstein Loss: 11.7967, Content Loss: 0.3801, PSNR: 21.32 SSIM: 0.78
===> Epoch [14 / 100]: Batch [64 / 159]: Gradient Penalty: 1.3928, D Fake Loss: -11.7648, D Real Loss: -3.2825, Wasserstein Loss: 11.7686, Content Loss: 0.3746, PSNR: 21.40 SSIM: 0.78
===> Epoch [14 / 100]: Batch [80 / 159]: Gradient Penalty: 1.3756, D Fake Loss: -11.7438, D Real Loss: -3.3331, Wasserstein Loss: 11.7494, Content Loss: 0.3727, PSNR: 21.44 SSIM: 0.78
===> Epoch [14 / 100]: Batch [96 / 159]: Gradient Penalty: 1.3582, D Fake Loss: -11.7394, D Real Loss: -3.4203, Wasserstein Loss: 11.7329, Content Loss: 0.3688, PSNR: 21.52 SSIM: 0.78
===> Epoch [14 / 100]: Batch [112 / 159]: Gradient Penalty: 1.3437, D Fake Loss: -11.7303, D Real Loss: -3.4904, Wasserstein Loss: 11.7224, Content Loss: 0.3678, PSNR: 21.61 SSIM: 0.78
===> Epoch [14 / 100]: Batch [128 / 159]: Gradient Penalty: 1.3267, D Fake Loss: -11.6673, D Real Loss: -3.4939, Wasserstein Loss: 11.6721, Content Loss: 0.3685, PSNR: 21.66 SSIM: 0.78
===> Epoch [14 / 100]: Batch [144 / 159]: Gradient Penalty: 1.3156, D Fake Loss: -11.7498, D Real Loss: -3.6433, Wasserstein Loss: 11.7506, Content Loss: 0.3670, PSNR: 21.77 SSIM: 0.78
Epoch [14 / 100]: Gradient Penalty: 1.3114, D Fake Loss: -11.7230, D Real Loss: -3.6441, Wasserstein Loss: 11.7364, Content Loss: 0.3666, PSNR: 21.79 SSIM: 0.78
Epoch [15 / 100]
===> Epoch [15 / 100]: Batch [16 / 159]: Gradient Penalty: 1.1466, D Fake Loss: -12.1032, D Real Loss: -4.7498, Wasserstein Loss: 12.0344, Content Loss: 0.3395, PSNR: 22.58 SSIM: 0.80
===> Epoch [15 / 100]: Batch [32 / 159]: Gradient Penalty: 1.2396, D Fake Loss: -11.4879, D Real Loss: -3.6895, Wasserstein Loss: 11.4637, Content Loss: 0.3621, PSNR: 22.03 SSIM: 0.79
===> Epoch [15 / 100]: Batch [48 / 159]: Gradient Penalty: 1.2404, D Fake Loss: -11.3793, D Real Loss: -3.5702, Wasserstein Loss: 11.3740, Content Loss: 0.3697, PSNR: 21.97 SSIM: 0.79
===> Epoch [15 / 100]: Batch [64 / 159]: Gradient Penalty: 1.2190, D Fake Loss: -11.4459, D Real Loss: -3.7354, Wasserstein Loss: 11.4324, Content Loss: 0.3676, PSNR: 22.08 SSIM: 0.79
===> Epoch [15 / 100]: Batch [80 / 159]: Gradient Penalty: 1.2207, D Fake Loss: -11.3797, D Real Loss: -3.6649, Wasserstein Loss: 11.3699, Content Loss: 0.3656, PSNR: 22.02 SSIM: 0.79
===> Epoch [15 / 100]: Batch [96 / 159]: Gradient Penalty: 1.2009, D Fake Loss: -11.3923, D Real Loss: -3.7821, Wasserstein Loss: 11.3893, Content Loss: 0.3636, PSNR: 22.13 SSIM: 0.79
===> Epoch [15 / 100]: Batch [112 / 159]: Gradient Penalty: 1.1892, D Fake Loss: -11.3492, D Real Loss: -3.7972, Wasserstein Loss: 11.3431, Content Loss: 0.3618, PSNR: 22.21 SSIM: 0.79
===> Epoch [15 / 100]: Batch [128 / 159]: Gradient Penalty: 1.1801, D Fake Loss: -11.3107, D Real Loss: -3.8017, Wasserstein Loss: 11.3109, Content Loss: 0.3622, PSNR: 22.23 SSIM: 0.79
===> Epoch [15 / 100]: Batch [144 / 159]: Gradient Penalty: 1.1776, D Fake Loss: -11.3595, D Real Loss: -3.8811, Wasserstein Loss: 11.3583, Content Loss: 0.3612, PSNR: 22.29 SSIM: 0.79
Epoch [15 / 100]: Gradient Penalty: 1.1726, D Fake Loss: -11.3211, D Real Loss: -3.8575, Wasserstein Loss: 11.3187, Content Loss: 0.3614, PSNR: 22.28 SSIM: 0.79
Epoch [16 / 100]
===> Epoch [16 / 100]: Batch [16 / 159]: Gradient Penalty: 1.0923, D Fake Loss: -11.1815, D Real Loss: -4.2206, Wasserstein Loss: 11.2102, Content Loss: 0.3347, PSNR: 22.98 SSIM: 0.80
===> Epoch [16 / 100]: Batch [32 / 159]: Gradient Penalty: 1.1008, D Fake Loss: -10.8792, D Real Loss: -3.7395, Wasserstein Loss: 10.9149, Content Loss: 0.3591, PSNR: 22.60 SSIM: 0.80
===> Epoch [16 / 100]: Batch [48 / 159]: Gradient Penalty: 1.1202, D Fake Loss: -10.6799, D Real Loss: -3.4298, Wasserstein Loss: 10.6718, Content Loss: 0.3657, PSNR: 22.38 SSIM: 0.80
===> Epoch [16 / 100]: Batch [64 / 159]: Gradient Penalty: 1.0986, D Fake Loss: -10.6693, D Real Loss: -3.5101, Wasserstein Loss: 10.6494, Content Loss: 0.3653, PSNR: 22.49 SSIM: 0.80
===> Epoch [16 / 100]: Batch [80 / 159]: Gradient Penalty: 1.0970, D Fake Loss: -10.6161, D Real Loss: -3.4825, Wasserstein Loss: 10.6160, Content Loss: 0.3636, PSNR: 22.47 SSIM: 0.80
===> Epoch [16 / 100]: Batch [96 / 159]: Gradient Penalty: 1.0830, D Fake Loss: -10.6535, D Real Loss: -3.5839, Wasserstein Loss: 10.6487, Content Loss: 0.3620, PSNR: 22.53 SSIM: 0.80
===> Epoch [16 / 100]: Batch [112 / 159]: Gradient Penalty: 1.0845, D Fake Loss: -10.5991, D Real Loss: -3.5633, Wasserstein Loss: 10.6056, Content Loss: 0.3593, PSNR: 22.58 SSIM: 0.80
===> Epoch [16 / 100]: Batch [128 / 159]: Gradient Penalty: 1.0695, D Fake Loss: -10.5514, D Real Loss: -3.5672, Wasserstein Loss: 10.5474, Content Loss: 0.3578, PSNR: 22.64 SSIM: 0.80
===> Epoch [16 / 100]: Batch [144 / 159]: Gradient Penalty: 1.0559, D Fake Loss: -10.5819, D Real Loss: -3.6795, Wasserstein Loss: 10.5841, Content Loss: 0.3555, PSNR: 22.76 SSIM: 0.80
Epoch [16 / 100]: Gradient Penalty: 1.0550, D Fake Loss: -10.4803, D Real Loss: -3.5696, Wasserstein Loss: 10.4820, Content Loss: 0.3555, PSNR: 22.74 SSIM: 0.80
Epoch [17 / 100]
===> Epoch [17 / 100]: Batch [16 / 159]: Gradient Penalty: 0.8762, D Fake Loss: -10.6650, D Real Loss: -4.5356, Wasserstein Loss: 10.6473, Content Loss: 0.3407, PSNR: 23.56 SSIM: 0.81
===> Epoch [17 / 100]: Batch [32 / 159]: Gradient Penalty: 0.9623, D Fake Loss: -10.0292, D Real Loss: -3.4678, Wasserstein Loss: 10.0352, Content Loss: 0.3522, PSNR: 23.05 SSIM: 0.80
===> Epoch [17 / 100]: Batch [48 / 159]: Gradient Penalty: 0.9869, D Fake Loss: -9.8788, D Real Loss: -3.1847, Wasserstein Loss: 9.8823, Content Loss: 0.3564, PSNR: 22.86 SSIM: 0.80
===> Epoch [17 / 100]: Batch [64 / 159]: Gradient Penalty: 0.9716, D Fake Loss: -9.9596, D Real Loss: -3.3565, Wasserstein Loss: 9.9552, Content Loss: 0.3528, PSNR: 22.94 SSIM: 0.80
===> Epoch [17 / 100]: Batch [80 / 159]: Gradient Penalty: 0.9581, D Fake Loss: -10.0051, D Real Loss: -3.4668, Wasserstein Loss: 10.0019, Content Loss: 0.3535, PSNR: 22.98 SSIM: 0.80
===> Epoch [17 / 100]: Batch [96 / 159]: Gradient Penalty: 0.9536, D Fake Loss: -9.9902, D Real Loss: -3.4813, Wasserstein Loss: 9.9948, Content Loss: 0.3535, PSNR: 22.99 SSIM: 0.80
===> Epoch [17 / 100]: Batch [112 / 159]: Gradient Penalty: 0.9422, D Fake Loss: -9.9892, D Real Loss: -3.5294, Wasserstein Loss: 9.9921, Content Loss: 0.3500, PSNR: 23.05 SSIM: 0.81
===> Epoch [17 / 100]: Batch [128 / 159]: Gradient Penalty: 0.9368, D Fake Loss: -9.9748, D Real Loss: -3.5515, Wasserstein Loss: 9.9832, Content Loss: 0.3497, PSNR: 23.10 SSIM: 0.81
===> Epoch [17 / 100]: Batch [144 / 159]: Gradient Penalty: 0.9358, D Fake Loss: -10.0410, D Real Loss: -3.6392, Wasserstein Loss: 10.0457, Content Loss: 0.3487, PSNR: 23.17 SSIM: 0.81
Epoch [17 / 100]: Gradient Penalty: 0.9299, D Fake Loss: -9.9918, D Real Loss: -3.6048, Wasserstein Loss: 9.9869, Content Loss: 0.3482, PSNR: 23.18 SSIM: 0.81
Epoch [18 / 100]
===> Epoch [18 / 100]: Batch [16 / 159]: Gradient Penalty: 0.8942, D Fake Loss: -10.1341, D Real Loss: -4.1505, Wasserstein Loss: 10.1338, Content Loss: 0.3325, PSNR: 23.70 SSIM: 0.81
===> Epoch [18 / 100]: Batch [32 / 159]: Gradient Penalty: 0.9077, D Fake Loss: -9.5238, D Real Loss: -3.3190, Wasserstein Loss: 9.5740, Content Loss: 0.3442, PSNR: 23.29 SSIM: 0.81
===> Epoch [18 / 100]: Batch [48 / 159]: Gradient Penalty: 0.9174, D Fake Loss: -9.5105, D Real Loss: -3.2194, Wasserstein Loss: 9.5711, Content Loss: 0.3536, PSNR: 23.13 SSIM: 0.81
===> Epoch [18 / 100]: Batch [64 / 159]: Gradient Penalty: 0.8828, D Fake Loss: -9.6603, D Real Loss: -3.5271, Wasserstein Loss: 9.6823, Content Loss: 0.3500, PSNR: 23.30 SSIM: 0.81
===> Epoch [18 / 100]: Batch [80 / 159]: Gradient Penalty: 0.8669, D Fake Loss: -9.7268, D Real Loss: -3.6478, Wasserstein Loss: 9.7509, Content Loss: 0.3507, PSNR: 23.34 SSIM: 0.81
===> Epoch [18 / 100]: Batch [96 / 159]: Gradient Penalty: 0.8682, D Fake Loss: -9.7045, D Real Loss: -3.6357, Wasserstein Loss: 9.7311, Content Loss: 0.3491, PSNR: 23.38 SSIM: 0.81
===> Epoch [18 / 100]: Batch [112 / 159]: Gradient Penalty: 0.8631, D Fake Loss: -9.6847, D Real Loss: -3.6368, Wasserstein Loss: 9.7124, Content Loss: 0.3476, PSNR: 23.41 SSIM: 0.81
===> Epoch [18 / 100]: Batch [128 / 159]: Gradient Penalty: 0.8566, D Fake Loss: -9.6764, D Real Loss: -3.6591, Wasserstein Loss: 9.6889, Content Loss: 0.3486, PSNR: 23.45 SSIM: 0.81
===> Epoch [18 / 100]: Batch [144 / 159]: Gradient Penalty: 0.8484, D Fake Loss: -9.7467, D Real Loss: -3.7778, Wasserstein Loss: 9.7665, Content Loss: 0.3464, PSNR: 23.54 SSIM: 0.81
Epoch [18 / 100]: Gradient Penalty: 0.8498, D Fake Loss: -9.6982, D Real Loss: -3.7179, Wasserstein Loss: 9.7050, Content Loss: 0.3460, PSNR: 23.53 SSIM: 0.81
Epoch [19 / 100]
===> Epoch [19 / 100]: Batch [16 / 159]: Gradient Penalty: 0.7966, D Fake Loss: -9.4334, D Real Loss: -3.8153, Wasserstein Loss: 9.5413, Content Loss: 0.3228, PSNR: 24.01 SSIM: 0.82
===> Epoch [19 / 100]: Batch [32 / 159]: Gradient Penalty: 0.8283, D Fake Loss: -9.1303, D Real Loss: -3.2427, Wasserstein Loss: 9.2327, Content Loss: 0.3406, PSNR: 23.64 SSIM: 0.81
===> Epoch [19 / 100]: Batch [48 / 159]: Gradient Penalty: 0.8268, D Fake Loss: -9.0179, D Real Loss: -3.0780, Wasserstein Loss: 9.0612, Content Loss: 0.3436, PSNR: 23.49 SSIM: 0.81
===> Epoch [19 / 100]: Batch [64 / 159]: Gradient Penalty: 0.8040, D Fake Loss: -9.2106, D Real Loss: -3.3894, Wasserstein Loss: 9.2250, Content Loss: 0.3435, PSNR: 23.58 SSIM: 0.81
===> Epoch [19 / 100]: Batch [80 / 159]: Gradient Penalty: 0.7953, D Fake Loss: -9.3102, D Real Loss: -3.5419, Wasserstein Loss: 9.3342, Content Loss: 0.3455, PSNR: 23.63 SSIM: 0.81
===> Epoch [19 / 100]: Batch [96 / 159]: Gradient Penalty: 0.7893, D Fake Loss: -9.3286, D Real Loss: -3.6010, Wasserstein Loss: 9.3505, Content Loss: 0.3445, PSNR: 23.68 SSIM: 0.81
===> Epoch [19 / 100]: Batch [112 / 159]: Gradient Penalty: 0.7826, D Fake Loss: -9.3836, D Real Loss: -3.7096, Wasserstein Loss: 9.3985, Content Loss: 0.3431, PSNR: 23.76 SSIM: 0.82
===> Epoch [19 / 100]: Batch [128 / 159]: Gradient Penalty: 0.7834, D Fake Loss: -9.3396, D Real Loss: -3.6650, Wasserstein Loss: 9.3639, Content Loss: 0.3424, PSNR: 23.78 SSIM: 0.82
===> Epoch [19 / 100]: Batch [144 / 159]: Gradient Penalty: 0.7829, D Fake Loss: -9.4201, D Real Loss: -3.7558, Wasserstein Loss: 9.4320, Content Loss: 0.3411, PSNR: 23.82 SSIM: 0.82
Epoch [19 / 100]: Gradient Penalty: 0.7813, D Fake Loss: -9.3814, D Real Loss: -3.7223, Wasserstein Loss: 9.3858, Content Loss: 0.3418, PSNR: 23.82 SSIM: 0.82
Epoch [20 / 100]
===> Epoch [20 / 100]: Batch [16 / 159]: Gradient Penalty: 0.7428, D Fake Loss: -9.4872, D Real Loss: -4.1543, Wasserstein Loss: 9.6221, Content Loss: 0.3202, PSNR: 24.29 SSIM: 0.82
===> Epoch [20 / 100]: Batch [32 / 159]: Gradient Penalty: 0.7507, D Fake Loss: -9.0731, D Real Loss: -3.5777, Wasserstein Loss: 9.1493, Content Loss: 0.3406, PSNR: 23.99 SSIM: 0.82
===> Epoch [20 / 100]: Batch [48 / 159]: Gradient Penalty: 0.7668, D Fake Loss: -8.8849, D Real Loss: -3.2499, Wasserstein Loss: 8.9149, Content Loss: 0.3471, PSNR: 23.73 SSIM: 0.81
===> Epoch [20 / 100]: Batch [64 / 159]: Gradient Penalty: 0.7508, D Fake Loss: -8.9943, D Real Loss: -3.4727, Wasserstein Loss: 9.0220, Content Loss: 0.3454, PSNR: 23.86 SSIM: 0.82
===> Epoch [20 / 100]: Batch [80 / 159]: Gradient Penalty: 0.7446, D Fake Loss: -9.0499, D Real Loss: -3.5424, Wasserstein Loss: 9.0700, Content Loss: 0.3457, PSNR: 23.88 SSIM: 0.82
===> Epoch [20 / 100]: Batch [96 / 159]: Gradient Penalty: 0.7375, D Fake Loss: -9.0447, D Real Loss: -3.5937, Wasserstein Loss: 9.0699, Content Loss: 0.3433, PSNR: 23.95 SSIM: 0.82
===> Epoch [20 / 100]: Batch [112 / 159]: Gradient Penalty: 0.7349, D Fake Loss: -9.0620, D Real Loss: -3.6298, Wasserstein Loss: 9.0917, Content Loss: 0.3420, PSNR: 23.97 SSIM: 0.82
===> Epoch [20 / 100]: Batch [128 / 159]: Gradient Penalty: 0.7295, D Fake Loss: -9.0574, D Real Loss: -3.6455, Wasserstein Loss: 9.0757, Content Loss: 0.3410, PSNR: 23.99 SSIM: 0.82
===> Epoch [20 / 100]: Batch [144 / 159]: Gradient Penalty: 0.7230, D Fake Loss: -9.1600, D Real Loss: -3.8021, Wasserstein Loss: 9.1836, Content Loss: 0.3393, PSNR: 24.11 SSIM: 0.82
Epoch [20 / 100]: Gradient Penalty: 0.7204, D Fake Loss: -9.1354, D Real Loss: -3.7827, Wasserstein Loss: 9.1488, Content Loss: 0.3392, PSNR: 24.11 SSIM: 0.82
Epoch [21 / 100]
===> Epoch [21 / 100]: Batch [16 / 159]: Gradient Penalty: 0.6418, D Fake Loss: -9.3139, D Real Loss: -4.4754, Wasserstein Loss: 9.3548, Content Loss: 0.3149, PSNR: 24.70 SSIM: 0.83
===> Epoch [21 / 100]: Batch [32 / 159]: Gradient Penalty: 0.6911, D Fake Loss: -8.9470, D Real Loss: -3.7709, Wasserstein Loss: 8.9670, Content Loss: 0.3273, PSNR: 24.27 SSIM: 0.82
===> Epoch [21 / 100]: Batch [48 / 159]: Gradient Penalty: 0.6961, D Fake Loss: -8.8260, D Real Loss: -3.6296, Wasserstein Loss: 8.8490, Content Loss: 0.3328, PSNR: 24.15 SSIM: 0.82
===> Epoch [21 / 100]: Batch [64 / 159]: Gradient Penalty: 0.6760, D Fake Loss: -8.9149, D Real Loss: -3.7904, Wasserstein Loss: 8.9066, Content Loss: 0.3337, PSNR: 24.24 SSIM: 0.82
===> Epoch [21 / 100]: Batch [80 / 159]: Gradient Penalty: 0.6734, D Fake Loss: -8.8672, D Real Loss: -3.7622, Wasserstein Loss: 8.8723, Content Loss: 0.3345, PSNR: 24.25 SSIM: 0.82
===> Epoch [21 / 100]: Batch [96 / 159]: Gradient Penalty: 0.6739, D Fake Loss: -8.7960, D Real Loss: -3.6897, Wasserstein Loss: 8.8130, Content Loss: 0.3339, PSNR: 24.25 SSIM: 0.82
===> Epoch [21 / 100]: Batch [112 / 159]: Gradient Penalty: 0.6734, D Fake Loss: -8.8254, D Real Loss: -3.7260, Wasserstein Loss: 8.8367, Content Loss: 0.3340, PSNR: 24.29 SSIM: 0.82
===> Epoch [21 / 100]: Batch [128 / 159]: Gradient Penalty: 0.6733, D Fake Loss: -8.7942, D Real Loss: -3.6982, Wasserstein Loss: 8.8115, Content Loss: 0.3338, PSNR: 24.28 SSIM: 0.82
===> Epoch [21 / 100]: Batch [144 / 159]: Gradient Penalty: 0.6664, D Fake Loss: -8.8795, D Real Loss: -3.8180, Wasserstein Loss: 8.8802, Content Loss: 0.3334, PSNR: 24.36 SSIM: 0.82
Epoch [21 / 100]: Gradient Penalty: 0.6695, D Fake Loss: -8.8432, D Real Loss: -3.7663, Wasserstein Loss: 8.8505, Content Loss: 0.3339, PSNR: 24.34 SSIM: 0.82
Epoch [22 / 100]
===> Epoch [22 / 100]: Batch [16 / 159]: Gradient Penalty: 0.6321, D Fake Loss: -8.7658, D Real Loss: -3.8763, Wasserstein Loss: 8.8230, Content Loss: 0.3122, PSNR: 24.73 SSIM: 0.83
===> Epoch [22 / 100]: Batch [32 / 159]: Gradient Penalty: 0.6448, D Fake Loss: -8.4924, D Real Loss: -3.4791, Wasserstein Loss: 8.5106, Content Loss: 0.3333, PSNR: 24.49 SSIM: 0.83
===> Epoch [22 / 100]: Batch [48 / 159]: Gradient Penalty: 0.6348, D Fake Loss: -8.5743, D Real Loss: -3.5907, Wasserstein Loss: 8.5655, Content Loss: 0.3380, PSNR: 24.38 SSIM: 0.82
===> Epoch [22 / 100]: Batch [64 / 159]: Gradient Penalty: 0.6361, D Fake Loss: -8.6619, D Real Loss: -3.7070, Wasserstein Loss: 8.6931, Content Loss: 0.3345, PSNR: 24.43 SSIM: 0.82
===> Epoch [22 / 100]: Batch [80 / 159]: Gradient Penalty: 0.6262, D Fake Loss: -8.6722, D Real Loss: -3.7624, Wasserstein Loss: 8.6912, Content Loss: 0.3332, PSNR: 24.47 SSIM: 0.83
===> Epoch [22 / 100]: Batch [96 / 159]: Gradient Penalty: 0.6192, D Fake Loss: -8.6739, D Real Loss: -3.7881, Wasserstein Loss: 8.6721, Content Loss: 0.3318, PSNR: 24.47 SSIM: 0.83
===> Epoch [22 / 100]: Batch [112 / 159]: Gradient Penalty: 0.6169, D Fake Loss: -8.6818, D Real Loss: -3.8303, Wasserstein Loss: 8.6953, Content Loss: 0.3310, PSNR: 24.53 SSIM: 0.83
===> Epoch [22 / 100]: Batch [128 / 159]: Gradient Penalty: 0.6154, D Fake Loss: -8.6633, D Real Loss: -3.8183, Wasserstein Loss: 8.6765, Content Loss: 0.3315, PSNR: 24.54 SSIM: 0.83
===> Epoch [22 / 100]: Batch [144 / 159]: Gradient Penalty: 0.6099, D Fake Loss: -8.7330, D Real Loss: -3.9296, Wasserstein Loss: 8.7436, Content Loss: 0.3303, PSNR: 24.62 SSIM: 0.83
Epoch [22 / 100]: Gradient Penalty: 0.6062, D Fake Loss: -8.7032, D Real Loss: -3.9231, Wasserstein Loss: 8.7155, Content Loss: 0.3304, PSNR: 24.64 SSIM: 0.83
Epoch [23 / 100]
===> Epoch [23 / 100]: Batch [16 / 159]: Gradient Penalty: 0.5569, D Fake Loss: -8.6621, D Real Loss: -4.1178, Wasserstein Loss: 8.5439, Content Loss: 0.3124, PSNR: 25.21 SSIM: 0.83
===> Epoch [23 / 100]: Batch [32 / 159]: Gradient Penalty: 0.5914, D Fake Loss: -8.2413, D Real Loss: -3.4643, Wasserstein Loss: 8.2345, Content Loss: 0.3240, PSNR: 24.78 SSIM: 0.83
===> Epoch [23 / 100]: Batch [48 / 159]: Gradient Penalty: 0.6028, D Fake Loss: -8.1905, D Real Loss: -3.3472, Wasserstein Loss: 8.1793, Content Loss: 0.3323, PSNR: 24.58 SSIM: 0.83
===> Epoch [23 / 100]: Batch [64 / 159]: Gradient Penalty: 0.5888, D Fake Loss: -8.3625, D Real Loss: -3.6165, Wasserstein Loss: 8.3746, Content Loss: 0.3277, PSNR: 24.69 SSIM: 0.83
===> Epoch [23 / 100]: Batch [80 / 159]: Gradient Penalty: 0.5773, D Fake Loss: -8.4317, D Real Loss: -3.7464, Wasserstein Loss: 8.4387, Content Loss: 0.3302, PSNR: 24.73 SSIM: 0.83
===> Epoch [23 / 100]: Batch [96 / 159]: Gradient Penalty: 0.5629, D Fake Loss: -8.5513, D Real Loss: -3.9409, Wasserstein Loss: 8.5490, Content Loss: 0.3293, PSNR: 24.78 SSIM: 0.83
===> Epoch [23 / 100]: Batch [112 / 159]: Gradient Penalty: 0.5641, D Fake Loss: -8.5923, D Real Loss: -3.9973, Wasserstein Loss: 8.5894, Content Loss: 0.3288, PSNR: 24.82 SSIM: 0.83
===> Epoch [23 / 100]: Batch [128 / 159]: Gradient Penalty: 0.5633, D Fake Loss: -8.6030, D Real Loss: -4.0157, Wasserstein Loss: 8.6018, Content Loss: 0.3284, PSNR: 24.82 SSIM: 0.83
===> Epoch [23 / 100]: Batch [144 / 159]: Gradient Penalty: 0.5664, D Fake Loss: -8.6443, D Real Loss: -4.0561, Wasserstein Loss: 8.6412, Content Loss: 0.3273, PSNR: 24.85 SSIM: 0.83
Epoch [23 / 100]: Gradient Penalty: 0.5634, D Fake Loss: -8.6157, D Real Loss: -4.0420, Wasserstein Loss: 8.6241, Content Loss: 0.3271, PSNR: 24.85 SSIM: 0.83
Epoch [24 / 100]
===> Epoch [24 / 100]: Batch [16 / 159]: Gradient Penalty: 0.5131, D Fake Loss: -9.0160, D Real Loss: -4.8024, Wasserstein Loss: 9.0244, Content Loss: 0.3162, PSNR: 25.34 SSIM: 0.84
===> Epoch [24 / 100]: Batch [32 / 159]: Gradient Penalty: 0.5226, D Fake Loss: -8.6620, D Real Loss: -4.2420, Wasserstein Loss: 8.6714, Content Loss: 0.3263, PSNR: 24.98 SSIM: 0.83
===> Epoch [24 / 100]: Batch [48 / 159]: Gradient Penalty: 0.5408, D Fake Loss: -8.5750, D Real Loss: -4.0579, Wasserstein Loss: 8.5882, Content Loss: 0.3315, PSNR: 24.75 SSIM: 0.83
===> Epoch [24 / 100]: Batch [64 / 159]: Gradient Penalty: 0.5427, D Fake Loss: -8.6750, D Real Loss: -4.1491, Wasserstein Loss: 8.6847, Content Loss: 0.3279, PSNR: 24.81 SSIM: 0.83
===> Epoch [24 / 100]: Batch [80 / 159]: Gradient Penalty: 0.5371, D Fake Loss: -8.6804, D Real Loss: -4.1894, Wasserstein Loss: 8.6955, Content Loss: 0.3293, PSNR: 24.84 SSIM: 0.83
===> Epoch [24 / 100]: Batch [96 / 159]: Gradient Penalty: 0.5395, D Fake Loss: -8.7013, D Real Loss: -4.2167, Wasserstein Loss: 8.7080, Content Loss: 0.3280, PSNR: 24.86 SSIM: 0.83
===> Epoch [24 / 100]: Batch [112 / 159]: Gradient Penalty: 0.5319, D Fake Loss: -8.7527, D Real Loss: -4.3066, Wasserstein Loss: 8.7499, Content Loss: 0.3271, PSNR: 24.90 SSIM: 0.83
===> Epoch [24 / 100]: Batch [128 / 159]: Gradient Penalty: 0.5282, D Fake Loss: -8.7369, D Real Loss: -4.3249, Wasserstein Loss: 8.7457, Content Loss: 0.3263, PSNR: 24.94 SSIM: 0.83
===> Epoch [24 / 100]: Batch [144 / 159]: Gradient Penalty: 0.5291, D Fake Loss: -8.8014, D Real Loss: -4.4099, Wasserstein Loss: 8.8089, Content Loss: 0.3251, PSNR: 25.03 SSIM: 0.83
Epoch [24 / 100]: Gradient Penalty: 0.5258, D Fake Loss: -8.8013, D Real Loss: -4.4241, Wasserstein Loss: 8.8156, Content Loss: 0.3255, PSNR: 25.03 SSIM: 0.83
Epoch [25 / 100]
===> Epoch [25 / 100]: Batch [16 / 159]: Gradient Penalty: 0.4747, D Fake Loss: -9.4120, D Real Loss: -5.3763, Wasserstein Loss: 9.3670, Content Loss: 0.3130, PSNR: 25.58 SSIM: 0.83
===> Epoch [25 / 100]: Batch [32 / 159]: Gradient Penalty: 0.5164, D Fake Loss: -8.6847, D Real Loss: -4.3467, Wasserstein Loss: 8.6715, Content Loss: 0.3204, PSNR: 25.09 SSIM: 0.83
===> Epoch [25 / 100]: Batch [48 / 159]: Gradient Penalty: 0.5136, D Fake Loss: -8.6325, D Real Loss: -4.2990, Wasserstein Loss: 8.6241, Content Loss: 0.3286, PSNR: 24.99 SSIM: 0.83
===> Epoch [25 / 100]: Batch [64 / 159]: Gradient Penalty: 0.5044, D Fake Loss: -8.7377, D Real Loss: -4.4627, Wasserstein Loss: 8.7427, Content Loss: 0.3260, PSNR: 25.07 SSIM: 0.83
===> Epoch [25 / 100]: Batch [80 / 159]: Gradient Penalty: 0.5013, D Fake Loss: -8.7638, D Real Loss: -4.5132, Wasserstein Loss: 8.7752, Content Loss: 0.3252, PSNR: 25.10 SSIM: 0.83
===> Epoch [25 / 100]: Batch [96 / 159]: Gradient Penalty: 0.4950, D Fake Loss: -8.8642, D Real Loss: -4.6438, Wasserstein Loss: 8.8574, Content Loss: 0.3244, PSNR: 25.14 SSIM: 0.83
===> Epoch [25 / 100]: Batch [112 / 159]: Gradient Penalty: 0.4947, D Fake Loss: -8.8824, D Real Loss: -4.6700, Wasserstein Loss: 8.8662, Content Loss: 0.3233, PSNR: 25.14 SSIM: 0.83
===> Epoch [25 / 100]: Batch [128 / 159]: Gradient Penalty: 0.4953, D Fake Loss: -8.8413, D Real Loss: -4.6314, Wasserstein Loss: 8.8348, Content Loss: 0.3229, PSNR: 25.16 SSIM: 0.83
===> Epoch [25 / 100]: Batch [144 / 159]: Gradient Penalty: 0.4955, D Fake Loss: -8.9031, D Real Loss: -4.7070, Wasserstein Loss: 8.9083, Content Loss: 0.3232, PSNR: 25.23 SSIM: 0.83
Epoch [25 / 100]: Gradient Penalty: 0.4915, D Fake Loss: -8.9063, D Real Loss: -4.7300, Wasserstein Loss: 8.9050, Content Loss: 0.3235, PSNR: 25.25 SSIM: 0.83
Epoch [26 / 100]
===> Epoch [26 / 100]: Batch [16 / 159]: Gradient Penalty: 0.4370, D Fake Loss: -9.6855, D Real Loss: -5.9077, Wasserstein Loss: 9.6329, Content Loss: 0.3058, PSNR: 26.04 SSIM: 0.84
===> Epoch [26 / 100]: Batch [32 / 159]: Gradient Penalty: 0.4828, D Fake Loss: -8.9446, D Real Loss: -4.8220, Wasserstein Loss: 8.9061, Content Loss: 0.3241, PSNR: 25.37 SSIM: 0.83
===> Epoch [26 / 100]: Batch [48 / 159]: Gradient Penalty: 0.4947, D Fake Loss: -8.7700, D Real Loss: -4.5350, Wasserstein Loss: 8.7528, Content Loss: 0.3269, PSNR: 25.12 SSIM: 0.83
===> Epoch [26 / 100]: Batch [64 / 159]: Gradient Penalty: 0.4834, D Fake Loss: -8.8702, D Real Loss: -4.6924, Wasserstein Loss: 8.8507, Content Loss: 0.3251, PSNR: 25.20 SSIM: 0.83
===> Epoch [26 / 100]: Batch [80 / 159]: Gradient Penalty: 0.4774, D Fake Loss: -8.9013, D Real Loss: -4.7573, Wasserstein Loss: 8.9005, Content Loss: 0.3249, PSNR: 25.22 SSIM: 0.83
===> Epoch [26 / 100]: Batch [96 / 159]: Gradient Penalty: 0.4763, D Fake Loss: -8.8517, D Real Loss: -4.7263, Wasserstein Loss: 8.8576, Content Loss: 0.3232, PSNR: 25.22 SSIM: 0.83
===> Epoch [26 / 100]: Batch [112 / 159]: Gradient Penalty: 0.4687, D Fake Loss: -8.8511, D Real Loss: -4.7730, Wasserstein Loss: 8.8491, Content Loss: 0.3218, PSNR: 25.25 SSIM: 0.83
===> Epoch [26 / 100]: Batch [128 / 159]: Gradient Penalty: 0.4682, D Fake Loss: -8.8271, D Real Loss: -4.7516, Wasserstein Loss: 8.8256, Content Loss: 0.3225, PSNR: 25.29 SSIM: 0.83
===> Epoch [26 / 100]: Batch [144 / 159]: Gradient Penalty: 0.4667, D Fake Loss: -8.8775, D Real Loss: -4.8119, Wasserstein Loss: 8.8749, Content Loss: 0.3214, PSNR: 25.34 SSIM: 0.83
Epoch [26 / 100]: Gradient Penalty: 0.4649, D Fake Loss: -8.8146, D Real Loss: -4.7504, Wasserstein Loss: 8.8156, Content Loss: 0.3215, PSNR: 25.31 SSIM: 0.83
Epoch [27 / 100]
===> Epoch [27 / 100]: Batch [16 / 159]: Gradient Penalty: 0.4259, D Fake Loss: -9.1304, D Real Loss: -5.3242, Wasserstein Loss: 9.0836, Content Loss: 0.3021, PSNR: 25.92 SSIM: 0.84
===> Epoch [27 / 100]: Batch [32 / 159]: Gradient Penalty: 0.4686, D Fake Loss: -8.6522, D Real Loss: -4.6236, Wasserstein Loss: 8.6836, Content Loss: 0.3207, PSNR: 25.40 SSIM: 0.84
===> Epoch [27 / 100]: Batch [48 / 159]: Gradient Penalty: 0.4555, D Fake Loss: -8.6390, D Real Loss: -4.6165, Wasserstein Loss: 8.6458, Content Loss: 0.3242, PSNR: 25.29 SSIM: 0.84
===> Epoch [27 / 100]: Batch [64 / 159]: Gradient Penalty: 0.4507, D Fake Loss: -8.7298, D Real Loss: -4.7633, Wasserstein Loss: 8.7622, Content Loss: 0.3226, PSNR: 25.37 SSIM: 0.84
===> Epoch [27 / 100]: Batch [80 / 159]: Gradient Penalty: 0.4417, D Fake Loss: -8.7542, D Real Loss: -4.8187, Wasserstein Loss: 8.7609, Content Loss: 0.3231, PSNR: 25.38 SSIM: 0.84
===> Epoch [27 / 100]: Batch [96 / 159]: Gradient Penalty: 0.4367, D Fake Loss: -8.8194, D Real Loss: -4.9257, Wasserstein Loss: 8.8416, Content Loss: 0.3236, PSNR: 25.43 SSIM: 0.84
===> Epoch [27 / 100]: Batch [112 / 159]: Gradient Penalty: 0.4295, D Fake Loss: -8.8780, D Real Loss: -5.0255, Wasserstein Loss: 8.8779, Content Loss: 0.3215, PSNR: 25.49 SSIM: 0.84
===> Epoch [27 / 100]: Batch [128 / 159]: Gradient Penalty: 0.4304, D Fake Loss: -8.8289, D Real Loss: -4.9717, Wasserstein Loss: 8.8309, Content Loss: 0.3204, PSNR: 25.49 SSIM: 0.84
===> Epoch [27 / 100]: Batch [144 / 159]: Gradient Penalty: 0.4277, D Fake Loss: -8.8776, D Real Loss: -5.0481, Wasserstein Loss: 8.8898, Content Loss: 0.3203, PSNR: 25.58 SSIM: 0.84
Epoch [27 / 100]: Gradient Penalty: 0.4258, D Fake Loss: -8.8487, D Real Loss: -5.0174, Wasserstein Loss: 8.8526, Content Loss: 0.3202, PSNR: 25.56 SSIM: 0.84
Epoch [28 / 100]
===> Epoch [28 / 100]: Batch [16 / 159]: Gradient Penalty: 0.4225, D Fake Loss: -8.9945, D Real Loss: -5.3691, Wasserstein Loss: 9.0909, Content Loss: 0.2967, PSNR: 25.98 SSIM: 0.84
===> Epoch [28 / 100]: Batch [32 / 159]: Gradient Penalty: 0.4105, D Fake Loss: -8.7041, D Real Loss: -4.9616, Wasserstein Loss: 8.7411, Content Loss: 0.3176, PSNR: 25.61 SSIM: 0.84
===> Epoch [28 / 100]: Batch [48 / 159]: Gradient Penalty: 0.4213, D Fake Loss: -8.6095, D Real Loss: -4.7937, Wasserstein Loss: 8.6252, Content Loss: 0.3233, PSNR: 25.47 SSIM: 0.84
===> Epoch [28 / 100]: Batch [64 / 159]: Gradient Penalty: 0.4074, D Fake Loss: -8.8704, D Real Loss: -5.1454, Wasserstein Loss: 8.9019, Content Loss: 0.3218, PSNR: 25.59 SSIM: 0.84
===> Epoch [28 / 100]: Batch [80 / 159]: Gradient Penalty: 0.4085, D Fake Loss: -8.8644, D Real Loss: -5.1226, Wasserstein Loss: 8.8891, Content Loss: 0.3217, PSNR: 25.57 SSIM: 0.84
===> Epoch [28 / 100]: Batch [96 / 159]: Gradient Penalty: 0.4070, D Fake Loss: -8.8355, D Real Loss: -5.1131, Wasserstein Loss: 8.8544, Content Loss: 0.3187, PSNR: 25.62 SSIM: 0.84
===> Epoch [28 / 100]: Batch [112 / 159]: Gradient Penalty: 0.4045, D Fake Loss: -8.8913, D Real Loss: -5.1962, Wasserstein Loss: 8.9119, Content Loss: 0.3175, PSNR: 25.68 SSIM: 0.84
===> Epoch [28 / 100]: Batch [128 / 159]: Gradient Penalty: 0.4003, D Fake Loss: -8.9077, D Real Loss: -5.2359, Wasserstein Loss: 8.9173, Content Loss: 0.3176, PSNR: 25.71 SSIM: 0.84
===> Epoch [28 / 100]: Batch [144 / 159]: Gradient Penalty: 0.3972, D Fake Loss: -8.9920, D Real Loss: -5.3418, Wasserstein Loss: 9.0035, Content Loss: 0.3167, PSNR: 25.78 SSIM: 0.84
Epoch [28 / 100]: Gradient Penalty: 0.3971, D Fake Loss: -8.9500, D Real Loss: -5.2965, Wasserstein Loss: 8.9610, Content Loss: 0.3164, PSNR: 25.76 SSIM: 0.84
Epoch [29 / 100]
===> Epoch [29 / 100]: Batch [16 / 159]: Gradient Penalty: 0.3399, D Fake Loss: -9.2683, D Real Loss: -6.0095, Wasserstein Loss: 9.2666, Content Loss: 0.2986, PSNR: 26.34 SSIM: 0.85
===> Epoch [29 / 100]: Batch [32 / 159]: Gradient Penalty: 0.3611, D Fake Loss: -9.0065, D Real Loss: -5.5352, Wasserstein Loss: 8.9778, Content Loss: 0.3140, PSNR: 25.88 SSIM: 0.84
===> Epoch [29 / 100]: Batch [48 / 159]: Gradient Penalty: 0.3741, D Fake Loss: -8.9362, D Real Loss: -5.3780, Wasserstein Loss: 8.9472, Content Loss: 0.3199, PSNR: 25.70 SSIM: 0.84
===> Epoch [29 / 100]: Batch [64 / 159]: Gradient Penalty: 0.3828, D Fake Loss: -8.9646, D Real Loss: -5.3721, Wasserstein Loss: 8.9675, Content Loss: 0.3200, PSNR: 25.68 SSIM: 0.84
===> Epoch [29 / 100]: Batch [80 / 159]: Gradient Penalty: 0.3795, D Fake Loss: -8.9203, D Real Loss: -5.3437, Wasserstein Loss: 8.9261, Content Loss: 0.3196, PSNR: 25.73 SSIM: 0.84
===> Epoch [29 / 100]: Batch [96 / 159]: Gradient Penalty: 0.3766, D Fake Loss: -8.9482, D Real Loss: -5.3899, Wasserstein Loss: 8.9501, Content Loss: 0.3170, PSNR: 25.76 SSIM: 0.84
===> Epoch [29 / 100]: Batch [112 / 159]: Gradient Penalty: 0.3746, D Fake Loss: -9.0090, D Real Loss: -5.4768, Wasserstein Loss: 9.0042, Content Loss: 0.3150, PSNR: 25.83 SSIM: 0.84
===> Epoch [29 / 100]: Batch [128 / 159]: Gradient Penalty: 0.3729, D Fake Loss: -9.0091, D Real Loss: -5.4854, Wasserstein Loss: 9.0060, Content Loss: 0.3146, PSNR: 25.82 SSIM: 0.84
===> Epoch [29 / 100]: Batch [144 / 159]: Gradient Penalty: 0.3754, D Fake Loss: -9.0387, D Real Loss: -5.5208, Wasserstein Loss: 9.0481, Content Loss: 0.3133, PSNR: 25.90 SSIM: 0.84
Epoch [29 / 100]: Gradient Penalty: 0.3723, D Fake Loss: -9.0342, D Real Loss: -5.5295, Wasserstein Loss: 9.0379, Content Loss: 0.3140, PSNR: 25.91 SSIM: 0.84
Epoch [30 / 100]
===> Epoch [30 / 100]: Batch [16 / 159]: Gradient Penalty: 0.3451, D Fake Loss: -9.3585, D Real Loss: -6.0822, Wasserstein Loss: 9.2278, Content Loss: 0.3017, PSNR: 26.50 SSIM: 0.85
===> Epoch [30 / 100]: Batch [32 / 159]: Gradient Penalty: 0.3734, D Fake Loss: -8.9251, D Real Loss: -5.4240, Wasserstein Loss: 8.9436, Content Loss: 0.3157, PSNR: 25.95 SSIM: 0.84
===> Epoch [30 / 100]: Batch [48 / 159]: Gradient Penalty: 0.3675, D Fake Loss: -9.0290, D Real Loss: -5.5403, Wasserstein Loss: 9.0215, Content Loss: 0.3229, PSNR: 25.88 SSIM: 0.84
===> Epoch [30 / 100]: Batch [64 / 159]: Gradient Penalty: 0.3657, D Fake Loss: -9.0634, D Real Loss: -5.5918, Wasserstein Loss: 9.0801, Content Loss: 0.3170, PSNR: 25.91 SSIM: 0.84
===> Epoch [30 / 100]: Batch [80 / 159]: Gradient Penalty: 0.3640, D Fake Loss: -9.0068, D Real Loss: -5.5413, Wasserstein Loss: 9.0063, Content Loss: 0.3163, PSNR: 25.91 SSIM: 0.84
===> Epoch [30 / 100]: Batch [96 / 159]: Gradient Penalty: 0.3590, D Fake Loss: -9.0273, D Real Loss: -5.5941, Wasserstein Loss: 9.0310, Content Loss: 0.3156, PSNR: 25.94 SSIM: 0.84
===> Epoch [30 / 100]: Batch [112 / 159]: Gradient Penalty: 0.3570, D Fake Loss: -9.0013, D Real Loss: -5.5826, Wasserstein Loss: 8.9977, Content Loss: 0.3141, PSNR: 26.00 SSIM: 0.84
===> Epoch [30 / 100]: Batch [128 / 159]: Gradient Penalty: 0.3546, D Fake Loss: -8.9943, D Real Loss: -5.5965, Wasserstein Loss: 8.9853, Content Loss: 0.3145, PSNR: 26.03 SSIM: 0.84
===> Epoch [30 / 100]: Batch [144 / 159]: Gradient Penalty: 0.3524, D Fake Loss: -9.0601, D Real Loss: -5.6873, Wasserstein Loss: 9.0647, Content Loss: 0.3133, PSNR: 26.09 SSIM: 0.84
Epoch [30 / 100]: Gradient Penalty: 0.3512, D Fake Loss: -9.0156, D Real Loss: -5.6457, Wasserstein Loss: 9.0208, Content Loss: 0.3128, PSNR: 26.08 SSIM: 0.84
Epoch [31 / 100]
===> Epoch [31 / 100]: Batch [16 / 159]: Gradient Penalty: 0.3303, D Fake Loss: -9.2739, D Real Loss: -6.1246, Wasserstein Loss: 9.2785, Content Loss: 0.2989, PSNR: 26.78 SSIM: 0.85
===> Epoch [31 / 100]: Batch [32 / 159]: Gradient Penalty: 0.3357, D Fake Loss: -8.7667, D Real Loss: -5.4670, Wasserstein Loss: 8.7771, Content Loss: 0.3136, PSNR: 26.26 SSIM: 0.84
===> Epoch [31 / 100]: Batch [48 / 159]: Gradient Penalty: 0.3463, D Fake Loss: -8.6854, D Real Loss: -5.3298, Wasserstein Loss: 8.6948, Content Loss: 0.3182, PSNR: 26.08 SSIM: 0.84
===> Epoch [31 / 100]: Batch [64 / 159]: Gradient Penalty: 0.3377, D Fake Loss: -8.8941, D Real Loss: -5.6127, Wasserstein Loss: 8.9074, Content Loss: 0.3157, PSNR: 26.18 SSIM: 0.84
===> Epoch [31 / 100]: Batch [80 / 159]: Gradient Penalty: 0.3377, D Fake Loss: -8.9406, D Real Loss: -5.6666, Wasserstein Loss: 8.9564, Content Loss: 0.3156, PSNR: 26.17 SSIM: 0.84
===> Epoch [31 / 100]: Batch [96 / 159]: Gradient Penalty: 0.3322, D Fake Loss: -9.0021, D Real Loss: -5.7530, Wasserstein Loss: 9.0037, Content Loss: 0.3133, PSNR: 26.19 SSIM: 0.84
===> Epoch [31 / 100]: Batch [112 / 159]: Gradient Penalty: 0.3308, D Fake Loss: -9.0610, D Real Loss: -5.8300, Wasserstein Loss: 9.0722, Content Loss: 0.3126, PSNR: 26.22 SSIM: 0.84
===> Epoch [31 / 100]: Batch [128 / 159]: Gradient Penalty: 0.3291, D Fake Loss: -9.0866, D Real Loss: -5.8739, Wasserstein Loss: 9.0873, Content Loss: 0.3120, PSNR: 26.24 SSIM: 0.84
===> Epoch [31 / 100]: Batch [144 / 159]: Gradient Penalty: 0.3279, D Fake Loss: -9.1529, D Real Loss: -5.9531, Wasserstein Loss: 9.1591, Content Loss: 0.3112, PSNR: 26.30 SSIM: 0.84
Epoch [31 / 100]: Gradient Penalty: 0.3290, D Fake Loss: -9.1026, D Real Loss: -5.8908, Wasserstein Loss: 9.1008, Content Loss: 0.3109, PSNR: 26.27 SSIM: 0.84
Epoch [32 / 100]
===> Epoch [32 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2941, D Fake Loss: -9.3733, D Real Loss: -6.5722, Wasserstein Loss: 9.4397, Content Loss: 0.3015, PSNR: 26.99 SSIM: 0.85
===> Epoch [32 / 100]: Batch [32 / 159]: Gradient Penalty: 0.3078, D Fake Loss: -9.0330, D Real Loss: -5.9837, Wasserstein Loss: 9.0339, Content Loss: 0.3138, PSNR: 26.46 SSIM: 0.84
===> Epoch [32 / 100]: Batch [48 / 159]: Gradient Penalty: 0.3143, D Fake Loss: -8.9749, D Real Loss: -5.8664, Wasserstein Loss: 9.0000, Content Loss: 0.3203, PSNR: 26.24 SSIM: 0.84
===> Epoch [32 / 100]: Batch [64 / 159]: Gradient Penalty: 0.3145, D Fake Loss: -8.9835, D Real Loss: -5.8782, Wasserstein Loss: 9.0181, Content Loss: 0.3174, PSNR: 26.28 SSIM: 0.85
===> Epoch [32 / 100]: Batch [80 / 159]: Gradient Penalty: 0.3168, D Fake Loss: -8.9702, D Real Loss: -5.8461, Wasserstein Loss: 8.9989, Content Loss: 0.3148, PSNR: 26.25 SSIM: 0.85
===> Epoch [32 / 100]: Batch [96 / 159]: Gradient Penalty: 0.3139, D Fake Loss: -8.9819, D Real Loss: -5.8897, Wasserstein Loss: 9.0131, Content Loss: 0.3123, PSNR: 26.32 SSIM: 0.85
===> Epoch [32 / 100]: Batch [112 / 159]: Gradient Penalty: 0.3125, D Fake Loss: -8.9716, D Real Loss: -5.8837, Wasserstein Loss: 8.9967, Content Loss: 0.3112, PSNR: 26.36 SSIM: 0.85
===> Epoch [32 / 100]: Batch [128 / 159]: Gradient Penalty: 0.3116, D Fake Loss: -8.9627, D Real Loss: -5.8820, Wasserstein Loss: 8.9743, Content Loss: 0.3116, PSNR: 26.37 SSIM: 0.85
===> Epoch [32 / 100]: Batch [144 / 159]: Gradient Penalty: 0.3085, D Fake Loss: -9.0215, D Real Loss: -5.9539, Wasserstein Loss: 9.0285, Content Loss: 0.3104, PSNR: 26.43 SSIM: 0.85
Epoch [32 / 100]: Gradient Penalty: 0.3076, D Fake Loss: -8.9783, D Real Loss: -5.9100, Wasserstein Loss: 8.9984, Content Loss: 0.3102, PSNR: 26.41 SSIM: 0.85
Epoch [33 / 100]
===> Epoch [33 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2858, D Fake Loss: -8.9636, D Real Loss: -6.0833, Wasserstein Loss: 8.8469, Content Loss: 0.2905, PSNR: 26.86 SSIM: 0.85
===> Epoch [33 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2855, D Fake Loss: -8.6792, D Real Loss: -5.7470, Wasserstein Loss: 8.6813, Content Loss: 0.3092, PSNR: 26.55 SSIM: 0.85
===> Epoch [33 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2861, D Fake Loss: -8.7684, D Real Loss: -5.7865, Wasserstein Loss: 8.7614, Content Loss: 0.3160, PSNR: 26.40 SSIM: 0.85
===> Epoch [33 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2853, D Fake Loss: -8.8896, D Real Loss: -5.9301, Wasserstein Loss: 8.8957, Content Loss: 0.3129, PSNR: 26.50 SSIM: 0.85
===> Epoch [33 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2840, D Fake Loss: -8.8576, D Real Loss: -5.9065, Wasserstein Loss: 8.8620, Content Loss: 0.3114, PSNR: 26.50 SSIM: 0.85
===> Epoch [33 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2854, D Fake Loss: -8.8982, D Real Loss: -5.9569, Wasserstein Loss: 8.8983, Content Loss: 0.3090, PSNR: 26.53 SSIM: 0.85
===> Epoch [33 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2808, D Fake Loss: -8.8976, D Real Loss: -5.9832, Wasserstein Loss: 8.8929, Content Loss: 0.3082, PSNR: 26.58 SSIM: 0.85
===> Epoch [33 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2811, D Fake Loss: -8.8578, D Real Loss: -5.9398, Wasserstein Loss: 8.8597, Content Loss: 0.3075, PSNR: 26.58 SSIM: 0.85
===> Epoch [33 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2820, D Fake Loss: -8.8723, D Real Loss: -5.9549, Wasserstein Loss: 8.8723, Content Loss: 0.3059, PSNR: 26.62 SSIM: 0.85
Epoch [33 / 100]: Gradient Penalty: 0.2827, D Fake Loss: -8.8471, D Real Loss: -5.9313, Wasserstein Loss: 8.8573, Content Loss: 0.3075, PSNR: 26.62 SSIM: 0.85
Epoch [34 / 100]
===> Epoch [34 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2484, D Fake Loss: -9.2546, D Real Loss: -6.4884, Wasserstein Loss: 9.1404, Content Loss: 0.2927, PSNR: 27.03 SSIM: 0.85
===> Epoch [34 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2608, D Fake Loss: -8.6826, D Real Loss: -5.8320, Wasserstein Loss: 8.6512, Content Loss: 0.3052, PSNR: 26.68 SSIM: 0.85
===> Epoch [34 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2651, D Fake Loss: -8.5665, D Real Loss: -5.7012, Wasserstein Loss: 8.5783, Content Loss: 0.3120, PSNR: 26.57 SSIM: 0.85
===> Epoch [34 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2588, D Fake Loss: -8.7451, D Real Loss: -5.9097, Wasserstein Loss: 8.7268, Content Loss: 0.3104, PSNR: 26.65 SSIM: 0.85
===> Epoch [34 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2613, D Fake Loss: -8.7720, D Real Loss: -5.9364, Wasserstein Loss: 8.7671, Content Loss: 0.3104, PSNR: 26.67 SSIM: 0.85
===> Epoch [34 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2630, D Fake Loss: -8.7774, D Real Loss: -5.9412, Wasserstein Loss: 8.7705, Content Loss: 0.3104, PSNR: 26.67 SSIM: 0.85
===> Epoch [34 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2648, D Fake Loss: -8.7739, D Real Loss: -5.9598, Wasserstein Loss: 8.7752, Content Loss: 0.3086, PSNR: 26.70 SSIM: 0.85
===> Epoch [34 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2656, D Fake Loss: -8.7319, D Real Loss: -5.9020, Wasserstein Loss: 8.7312, Content Loss: 0.3093, PSNR: 26.67 SSIM: 0.85
===> Epoch [34 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2661, D Fake Loss: -8.7760, D Real Loss: -5.9462, Wasserstein Loss: 8.7769, Content Loss: 0.3081, PSNR: 26.74 SSIM: 0.85
Epoch [34 / 100]: Gradient Penalty: 0.2640, D Fake Loss: -8.7674, D Real Loss: -5.9506, Wasserstein Loss: 8.7679, Content Loss: 0.3079, PSNR: 26.75 SSIM: 0.85
Epoch [35 / 100]
===> Epoch [35 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2476, D Fake Loss: -9.1173, D Real Loss: -6.4680, Wasserstein Loss: 9.2020, Content Loss: 0.2921, PSNR: 27.37 SSIM: 0.85
===> Epoch [35 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2483, D Fake Loss: -8.8248, D Real Loss: -6.0784, Wasserstein Loss: 8.8235, Content Loss: 0.3079, PSNR: 26.93 SSIM: 0.85
===> Epoch [35 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2578, D Fake Loss: -8.7860, D Real Loss: -5.9912, Wasserstein Loss: 8.7819, Content Loss: 0.3140, PSNR: 26.74 SSIM: 0.85
===> Epoch [35 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2534, D Fake Loss: -8.8883, D Real Loss: -6.1289, Wasserstein Loss: 8.8831, Content Loss: 0.3120, PSNR: 26.80 SSIM: 0.85
===> Epoch [35 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2534, D Fake Loss: -8.8610, D Real Loss: -6.1024, Wasserstein Loss: 8.8601, Content Loss: 0.3103, PSNR: 26.78 SSIM: 0.85
===> Epoch [35 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2556, D Fake Loss: -8.8638, D Real Loss: -6.1153, Wasserstein Loss: 8.8722, Content Loss: 0.3101, PSNR: 26.79 SSIM: 0.85
===> Epoch [35 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2521, D Fake Loss: -8.8463, D Real Loss: -6.1169, Wasserstein Loss: 8.8563, Content Loss: 0.3077, PSNR: 26.86 SSIM: 0.85
===> Epoch [35 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2514, D Fake Loss: -8.8358, D Real Loss: -6.1067, Wasserstein Loss: 8.8265, Content Loss: 0.3068, PSNR: 26.85 SSIM: 0.85
===> Epoch [35 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2524, D Fake Loss: -8.8867, D Real Loss: -6.1628, Wasserstein Loss: 8.8908, Content Loss: 0.3063, PSNR: 26.90 SSIM: 0.85
Epoch [35 / 100]: Gradient Penalty: 0.2523, D Fake Loss: -8.8411, D Real Loss: -6.1164, Wasserstein Loss: 8.8491, Content Loss: 0.3055, PSNR: 26.90 SSIM: 0.85
Epoch [36 / 100]
===> Epoch [36 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2330, D Fake Loss: -9.1926, D Real Loss: -6.6351, Wasserstein Loss: 9.1494, Content Loss: 0.2884, PSNR: 27.41 SSIM: 0.86
===> Epoch [36 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2527, D Fake Loss: -8.7743, D Real Loss: -6.0473, Wasserstein Loss: 8.7892, Content Loss: 0.3042, PSNR: 26.83 SSIM: 0.85
===> Epoch [36 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2463, D Fake Loss: -8.8293, D Real Loss: -6.1092, Wasserstein Loss: 8.8331, Content Loss: 0.3100, PSNR: 26.82 SSIM: 0.85
===> Epoch [36 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2481, D Fake Loss: -8.9154, D Real Loss: -6.2066, Wasserstein Loss: 8.9210, Content Loss: 0.3062, PSNR: 26.85 SSIM: 0.85
===> Epoch [36 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2454, D Fake Loss: -8.9201, D Real Loss: -6.2196, Wasserstein Loss: 8.8896, Content Loss: 0.3060, PSNR: 26.86 SSIM: 0.85
===> Epoch [36 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2489, D Fake Loss: -8.8272, D Real Loss: -6.1263, Wasserstein Loss: 8.8226, Content Loss: 0.3036, PSNR: 26.88 SSIM: 0.85
===> Epoch [36 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2471, D Fake Loss: -8.8503, D Real Loss: -6.1672, Wasserstein Loss: 8.8548, Content Loss: 0.3030, PSNR: 26.94 SSIM: 0.85
===> Epoch [36 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2462, D Fake Loss: -8.8375, D Real Loss: -6.1649, Wasserstein Loss: 8.8344, Content Loss: 0.3036, PSNR: 26.94 SSIM: 0.85
===> Epoch [36 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2464, D Fake Loss: -8.8506, D Real Loss: -6.1775, Wasserstein Loss: 8.8523, Content Loss: 0.3034, PSNR: 26.98 SSIM: 0.85
Epoch [36 / 100]: Gradient Penalty: 0.2453, D Fake Loss: -8.8186, D Real Loss: -6.1431, Wasserstein Loss: 8.8188, Content Loss: 0.3036, PSNR: 26.98 SSIM: 0.85
Epoch [37 / 100]
===> Epoch [37 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2180, D Fake Loss: -8.9640, D Real Loss: -6.5393, Wasserstein Loss: 8.9649, Content Loss: 0.2889, PSNR: 27.66 SSIM: 0.86
===> Epoch [37 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2288, D Fake Loss: -8.6679, D Real Loss: -6.0980, Wasserstein Loss: 8.6818, Content Loss: 0.2998, PSNR: 27.17 SSIM: 0.85
===> Epoch [37 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2308, D Fake Loss: -8.7624, D Real Loss: -6.1597, Wasserstein Loss: 8.7716, Content Loss: 0.3051, PSNR: 26.97 SSIM: 0.85
===> Epoch [37 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2352, D Fake Loss: -8.8410, D Real Loss: -6.2273, Wasserstein Loss: 8.8604, Content Loss: 0.3022, PSNR: 27.01 SSIM: 0.85
===> Epoch [37 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2341, D Fake Loss: -8.8490, D Real Loss: -6.2409, Wasserstein Loss: 8.8765, Content Loss: 0.3032, PSNR: 27.01 SSIM: 0.85
===> Epoch [37 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2316, D Fake Loss: -8.9025, D Real Loss: -6.2993, Wasserstein Loss: 8.9201, Content Loss: 0.3041, PSNR: 27.02 SSIM: 0.85
===> Epoch [37 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2279, D Fake Loss: -8.9943, D Real Loss: -6.4162, Wasserstein Loss: 8.9993, Content Loss: 0.3037, PSNR: 27.07 SSIM: 0.85
===> Epoch [37 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2297, D Fake Loss: -8.9765, D Real Loss: -6.3838, Wasserstein Loss: 8.9785, Content Loss: 0.3026, PSNR: 27.06 SSIM: 0.85
===> Epoch [37 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2284, D Fake Loss: -9.0667, D Real Loss: -6.4962, Wasserstein Loss: 9.0826, Content Loss: 0.3017, PSNR: 27.12 SSIM: 0.85
Epoch [37 / 100]: Gradient Penalty: 0.2286, D Fake Loss: -9.0706, D Real Loss: -6.4929, Wasserstein Loss: 9.0782, Content Loss: 0.3015, PSNR: 27.10 SSIM: 0.85
Epoch [38 / 100]
===> Epoch [38 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2044, D Fake Loss: -9.9781, D Real Loss: -7.6243, Wasserstein Loss: 9.9719, Content Loss: 0.2964, PSNR: 27.57 SSIM: 0.85
===> Epoch [38 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2085, D Fake Loss: -9.7672, D Real Loss: -7.3142, Wasserstein Loss: 9.7996, Content Loss: 0.3086, PSNR: 27.24 SSIM: 0.85
===> Epoch [38 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2154, D Fake Loss: -9.6480, D Real Loss: -7.1522, Wasserstein Loss: 9.6877, Content Loss: 0.3113, PSNR: 27.06 SSIM: 0.85
===> Epoch [38 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2177, D Fake Loss: -9.7739, D Real Loss: -7.2652, Wasserstein Loss: 9.8033, Content Loss: 0.3073, PSNR: 27.11 SSIM: 0.85
===> Epoch [38 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2169, D Fake Loss: -9.7415, D Real Loss: -7.2195, Wasserstein Loss: 9.7523, Content Loss: 0.3055, PSNR: 27.11 SSIM: 0.85
===> Epoch [38 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2191, D Fake Loss: -9.6851, D Real Loss: -7.1498, Wasserstein Loss: 9.6959, Content Loss: 0.3035, PSNR: 27.11 SSIM: 0.85
===> Epoch [38 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2156, D Fake Loss: -9.7810, D Real Loss: -7.2688, Wasserstein Loss: 9.7904, Content Loss: 0.3019, PSNR: 27.19 SSIM: 0.85
===> Epoch [38 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2152, D Fake Loss: -9.7915, D Real Loss: -7.2828, Wasserstein Loss: 9.8049, Content Loss: 0.3004, PSNR: 27.21 SSIM: 0.85
===> Epoch [38 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2127, D Fake Loss: -9.9229, D Real Loss: -7.4384, Wasserstein Loss: 9.9336, Content Loss: 0.3003, PSNR: 27.30 SSIM: 0.85
Epoch [38 / 100]: Gradient Penalty: 0.2132, D Fake Loss: -9.9224, D Real Loss: -7.4358, Wasserstein Loss: 9.9424, Content Loss: 0.3006, PSNR: 27.27 SSIM: 0.85
Epoch [39 / 100]
===> Epoch [39 / 100]: Batch [16 / 159]: Gradient Penalty: 0.2080, D Fake Loss: -10.6670, D Real Loss: -8.1914, Wasserstein Loss: 10.6104, Content Loss: 0.2863, PSNR: 27.68 SSIM: 0.86
===> Epoch [39 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2205, D Fake Loss: -10.2248, D Real Loss: -7.6777, Wasserstein Loss: 10.2194, Content Loss: 0.2981, PSNR: 27.29 SSIM: 0.85
===> Epoch [39 / 100]: Batch [48 / 159]: Gradient Penalty: 0.2109, D Fake Loss: -10.3339, D Real Loss: -7.8090, Wasserstein Loss: 10.3219, Content Loss: 0.3021, PSNR: 27.19 SSIM: 0.85
===> Epoch [39 / 100]: Batch [64 / 159]: Gradient Penalty: 0.2060, D Fake Loss: -10.5108, D Real Loss: -8.0344, Wasserstein Loss: 10.5224, Content Loss: 0.3009, PSNR: 27.30 SSIM: 0.85
===> Epoch [39 / 100]: Batch [80 / 159]: Gradient Penalty: 0.2074, D Fake Loss: -10.5439, D Real Loss: -8.0648, Wasserstein Loss: 10.5361, Content Loss: 0.3015, PSNR: 27.25 SSIM: 0.85
===> Epoch [39 / 100]: Batch [96 / 159]: Gradient Penalty: 0.2032, D Fake Loss: -10.6245, D Real Loss: -8.1736, Wasserstein Loss: 10.6245, Content Loss: 0.3025, PSNR: 27.30 SSIM: 0.85
===> Epoch [39 / 100]: Batch [112 / 159]: Gradient Penalty: 0.2018, D Fake Loss: -10.7012, D Real Loss: -8.2660, Wasserstein Loss: 10.7177, Content Loss: 0.3017, PSNR: 27.33 SSIM: 0.85
===> Epoch [39 / 100]: Batch [128 / 159]: Gradient Penalty: 0.2014, D Fake Loss: -10.7594, D Real Loss: -8.3220, Wasserstein Loss: 10.7657, Content Loss: 0.3018, PSNR: 27.34 SSIM: 0.85
===> Epoch [39 / 100]: Batch [144 / 159]: Gradient Penalty: 0.2011, D Fake Loss: -10.8378, D Real Loss: -8.4101, Wasserstein Loss: 10.8470, Content Loss: 0.3009, PSNR: 27.39 SSIM: 0.85
Epoch [39 / 100]: Gradient Penalty: 0.2019, D Fake Loss: -10.8386, D Real Loss: -8.4037, Wasserstein Loss: 10.8380, Content Loss: 0.3005, PSNR: 27.38 SSIM: 0.85
Epoch [40 / 100]
===> Epoch [40 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1871, D Fake Loss: -11.3948, D Real Loss: -9.1342, Wasserstein Loss: 11.5140, Content Loss: 0.2861, PSNR: 27.98 SSIM: 0.86
===> Epoch [40 / 100]: Batch [32 / 159]: Gradient Penalty: 0.2011, D Fake Loss: -11.0608, D Real Loss: -8.6308, Wasserstein Loss: 11.0653, Content Loss: 0.2974, PSNR: 27.37 SSIM: 0.86
===> Epoch [40 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1948, D Fake Loss: -11.0943, D Real Loss: -8.6766, Wasserstein Loss: 11.1232, Content Loss: 0.3035, PSNR: 27.30 SSIM: 0.85
===> Epoch [40 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1919, D Fake Loss: -11.2275, D Real Loss: -8.8423, Wasserstein Loss: 11.2551, Content Loss: 0.3010, PSNR: 27.41 SSIM: 0.85
===> Epoch [40 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1915, D Fake Loss: -11.2458, D Real Loss: -8.8641, Wasserstein Loss: 11.2730, Content Loss: 0.2997, PSNR: 27.41 SSIM: 0.86
===> Epoch [40 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1887, D Fake Loss: -11.4138, D Real Loss: -9.0555, Wasserstein Loss: 11.4335, Content Loss: 0.2981, PSNR: 27.48 SSIM: 0.86
===> Epoch [40 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1857, D Fake Loss: -11.5312, D Real Loss: -9.1902, Wasserstein Loss: 11.5394, Content Loss: 0.2972, PSNR: 27.53 SSIM: 0.86
===> Epoch [40 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1886, D Fake Loss: -11.4979, D Real Loss: -9.1438, Wasserstein Loss: 11.5122, Content Loss: 0.2961, PSNR: 27.53 SSIM: 0.86
===> Epoch [40 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1903, D Fake Loss: -11.5490, D Real Loss: -9.1888, Wasserstein Loss: 11.5617, Content Loss: 0.2958, PSNR: 27.56 SSIM: 0.86
Epoch [40 / 100]: Gradient Penalty: 0.1896, D Fake Loss: -11.5553, D Real Loss: -9.1990, Wasserstein Loss: 11.5772, Content Loss: 0.2965, PSNR: 27.54 SSIM: 0.86
Epoch [41 / 100]
===> Epoch [41 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1822, D Fake Loss: -12.0624, D Real Loss: -9.8632, Wasserstein Loss: 11.9834, Content Loss: 0.2816, PSNR: 27.99 SSIM: 0.86
===> Epoch [41 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1941, D Fake Loss: -11.5297, D Real Loss: -9.1624, Wasserstein Loss: 11.5453, Content Loss: 0.2954, PSNR: 27.53 SSIM: 0.86
===> Epoch [41 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1857, D Fake Loss: -11.6809, D Real Loss: -9.3185, Wasserstein Loss: 11.6628, Content Loss: 0.3014, PSNR: 27.43 SSIM: 0.86
===> Epoch [41 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1793, D Fake Loss: -11.8991, D Real Loss: -9.5806, Wasserstein Loss: 11.8824, Content Loss: 0.2996, PSNR: 27.52 SSIM: 0.86
===> Epoch [41 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1873, D Fake Loss: -11.8128, D Real Loss: -9.4712, Wasserstein Loss: 11.8126, Content Loss: 0.2995, PSNR: 27.48 SSIM: 0.86
===> Epoch [41 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1846, D Fake Loss: -11.8788, D Real Loss: -9.5498, Wasserstein Loss: 11.8566, Content Loss: 0.2977, PSNR: 27.54 SSIM: 0.86
===> Epoch [41 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1817, D Fake Loss: -12.0234, D Real Loss: -9.7170, Wasserstein Loss: 12.0118, Content Loss: 0.2978, PSNR: 27.57 SSIM: 0.86
===> Epoch [41 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1823, D Fake Loss: -12.0665, D Real Loss: -9.7584, Wasserstein Loss: 12.0577, Content Loss: 0.2978, PSNR: 27.58 SSIM: 0.86
===> Epoch [41 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1827, D Fake Loss: -12.1333, D Real Loss: -9.8279, Wasserstein Loss: 12.1266, Content Loss: 0.2970, PSNR: 27.63 SSIM: 0.86
Epoch [41 / 100]: Gradient Penalty: 0.1829, D Fake Loss: -12.1229, D Real Loss: -9.8113, Wasserstein Loss: 12.1183, Content Loss: 0.2976, PSNR: 27.62 SSIM: 0.86
Epoch [42 / 100]
===> Epoch [42 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1913, D Fake Loss: -12.7775, D Real Loss: -10.4900, Wasserstein Loss: 12.8310, Content Loss: 0.2887, PSNR: 27.81 SSIM: 0.86
===> Epoch [42 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1816, D Fake Loss: -12.5684, D Real Loss: -10.2448, Wasserstein Loss: 12.6345, Content Loss: 0.3024, PSNR: 27.56 SSIM: 0.86
===> Epoch [42 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1801, D Fake Loss: -12.5697, D Real Loss: -10.2247, Wasserstein Loss: 12.5758, Content Loss: 0.3038, PSNR: 27.48 SSIM: 0.86
===> Epoch [42 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1789, D Fake Loss: -12.6078, D Real Loss: -10.2891, Wasserstein Loss: 12.6240, Content Loss: 0.3017, PSNR: 27.57 SSIM: 0.86
===> Epoch [42 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1782, D Fake Loss: -12.7075, D Real Loss: -10.4005, Wasserstein Loss: 12.7122, Content Loss: 0.3022, PSNR: 27.59 SSIM: 0.86
===> Epoch [42 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1801, D Fake Loss: -12.7050, D Real Loss: -10.3985, Wasserstein Loss: 12.7212, Content Loss: 0.3022, PSNR: 27.55 SSIM: 0.86
===> Epoch [42 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1821, D Fake Loss: -12.6906, D Real Loss: -10.3782, Wasserstein Loss: 12.7039, Content Loss: 0.2997, PSNR: 27.58 SSIM: 0.86
===> Epoch [42 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1839, D Fake Loss: -12.6617, D Real Loss: -10.3452, Wasserstein Loss: 12.6777, Content Loss: 0.2993, PSNR: 27.60 SSIM: 0.86
===> Epoch [42 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1836, D Fake Loss: -12.7169, D Real Loss: -10.4130, Wasserstein Loss: 12.7400, Content Loss: 0.2988, PSNR: 27.67 SSIM: 0.86
Epoch [42 / 100]: Gradient Penalty: 0.1840, D Fake Loss: -12.7109, D Real Loss: -10.4036, Wasserstein Loss: 12.7294, Content Loss: 0.2980, PSNR: 27.65 SSIM: 0.86
Epoch [43 / 100]
===> Epoch [43 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1608, D Fake Loss: -13.1783, D Real Loss: -11.0609, Wasserstein Loss: 13.1363, Content Loss: 0.2864, PSNR: 28.03 SSIM: 0.86
===> Epoch [43 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1690, D Fake Loss: -12.8476, D Real Loss: -10.5887, Wasserstein Loss: 12.8698, Content Loss: 0.2967, PSNR: 27.60 SSIM: 0.86
===> Epoch [43 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1674, D Fake Loss: -12.9113, D Real Loss: -10.6486, Wasserstein Loss: 12.9534, Content Loss: 0.2980, PSNR: 27.54 SSIM: 0.86
===> Epoch [43 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1663, D Fake Loss: -12.9829, D Real Loss: -10.7346, Wasserstein Loss: 13.0040, Content Loss: 0.2952, PSNR: 27.63 SSIM: 0.86
===> Epoch [43 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1729, D Fake Loss: -12.8885, D Real Loss: -10.6119, Wasserstein Loss: 12.9015, Content Loss: 0.2946, PSNR: 27.59 SSIM: 0.86
===> Epoch [43 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1726, D Fake Loss: -12.9301, D Real Loss: -10.6725, Wasserstein Loss: 12.9278, Content Loss: 0.2946, PSNR: 27.65 SSIM: 0.86
===> Epoch [43 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1737, D Fake Loss: -12.8986, D Real Loss: -10.6434, Wasserstein Loss: 12.9141, Content Loss: 0.2932, PSNR: 27.67 SSIM: 0.86
===> Epoch [43 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1737, D Fake Loss: -12.8854, D Real Loss: -10.6339, Wasserstein Loss: 12.8883, Content Loss: 0.2936, PSNR: 27.67 SSIM: 0.86
===> Epoch [43 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1736, D Fake Loss: -12.9029, D Real Loss: -10.6586, Wasserstein Loss: 12.9072, Content Loss: 0.2943, PSNR: 27.74 SSIM: 0.86
Epoch [43 / 100]: Gradient Penalty: 0.1756, D Fake Loss: -12.8737, D Real Loss: -10.6223, Wasserstein Loss: 12.8748, Content Loss: 0.2959, PSNR: 27.75 SSIM: 0.86
Epoch [44 / 100]
===> Epoch [44 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1646, D Fake Loss: -13.0286, D Real Loss: -10.9288, Wasserstein Loss: 13.1023, Content Loss: 0.2805, PSNR: 28.29 SSIM: 0.86
===> Epoch [44 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1732, D Fake Loss: -12.7772, D Real Loss: -10.5488, Wasserstein Loss: 12.8490, Content Loss: 0.2919, PSNR: 27.84 SSIM: 0.86
===> Epoch [44 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1732, D Fake Loss: -12.7689, D Real Loss: -10.5003, Wasserstein Loss: 12.7999, Content Loss: 0.2955, PSNR: 27.75 SSIM: 0.86
===> Epoch [44 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1735, D Fake Loss: -12.8252, D Real Loss: -10.5762, Wasserstein Loss: 12.8409, Content Loss: 0.2955, PSNR: 27.78 SSIM: 0.86
===> Epoch [44 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1726, D Fake Loss: -12.8673, D Real Loss: -10.6313, Wasserstein Loss: 12.8935, Content Loss: 0.2958, PSNR: 27.80 SSIM: 0.86
===> Epoch [44 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1709, D Fake Loss: -12.8983, D Real Loss: -10.6734, Wasserstein Loss: 12.9134, Content Loss: 0.2956, PSNR: 27.81 SSIM: 0.86
===> Epoch [44 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1691, D Fake Loss: -12.9357, D Real Loss: -10.7316, Wasserstein Loss: 12.9428, Content Loss: 0.2949, PSNR: 27.85 SSIM: 0.86
===> Epoch [44 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1703, D Fake Loss: -12.9078, D Real Loss: -10.6922, Wasserstein Loss: 12.9159, Content Loss: 0.2940, PSNR: 27.86 SSIM: 0.86
===> Epoch [44 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1688, D Fake Loss: -12.9727, D Real Loss: -10.7698, Wasserstein Loss: 12.9773, Content Loss: 0.2942, PSNR: 27.90 SSIM: 0.86
Epoch [44 / 100]: Gradient Penalty: 0.1698, D Fake Loss: -12.9349, D Real Loss: -10.7337, Wasserstein Loss: 12.9478, Content Loss: 0.2951, PSNR: 27.89 SSIM: 0.86
Epoch [45 / 100]
===> Epoch [45 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1600, D Fake Loss: -12.9239, D Real Loss: -10.8059, Wasserstein Loss: 12.9284, Content Loss: 0.2838, PSNR: 28.28 SSIM: 0.86
===> Epoch [45 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1728, D Fake Loss: -12.6818, D Real Loss: -10.4053, Wasserstein Loss: 12.6909, Content Loss: 0.2944, PSNR: 27.88 SSIM: 0.86
===> Epoch [45 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1692, D Fake Loss: -12.8241, D Real Loss: -10.5775, Wasserstein Loss: 12.8180, Content Loss: 0.2984, PSNR: 27.72 SSIM: 0.86
===> Epoch [45 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1694, D Fake Loss: -12.8979, D Real Loss: -10.6670, Wasserstein Loss: 12.9060, Content Loss: 0.2985, PSNR: 27.76 SSIM: 0.86
===> Epoch [45 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1711, D Fake Loss: -12.8612, D Real Loss: -10.6303, Wasserstein Loss: 12.8750, Content Loss: 0.2980, PSNR: 27.76 SSIM: 0.86
===> Epoch [45 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1697, D Fake Loss: -12.8143, D Real Loss: -10.6011, Wasserstein Loss: 12.8201, Content Loss: 0.2958, PSNR: 27.80 SSIM: 0.86
===> Epoch [45 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1681, D Fake Loss: -12.8402, D Real Loss: -10.6481, Wasserstein Loss: 12.8379, Content Loss: 0.2944, PSNR: 27.83 SSIM: 0.86
===> Epoch [45 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1674, D Fake Loss: -12.8029, D Real Loss: -10.6029, Wasserstein Loss: 12.8052, Content Loss: 0.2944, PSNR: 27.83 SSIM: 0.86
===> Epoch [45 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1665, D Fake Loss: -12.8305, D Real Loss: -10.6430, Wasserstein Loss: 12.8289, Content Loss: 0.2939, PSNR: 27.90 SSIM: 0.86
Epoch [45 / 100]: Gradient Penalty: 0.1676, D Fake Loss: -12.8020, D Real Loss: -10.6077, Wasserstein Loss: 12.8044, Content Loss: 0.2939, PSNR: 27.89 SSIM: 0.86
Epoch [46 / 100]
===> Epoch [46 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1568, D Fake Loss: -12.9444, D Real Loss: -10.9378, Wasserstein Loss: 12.9481, Content Loss: 0.2754, PSNR: 28.47 SSIM: 0.87
===> Epoch [46 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1593, D Fake Loss: -12.6424, D Real Loss: -10.5533, Wasserstein Loss: 12.6276, Content Loss: 0.2901, PSNR: 28.05 SSIM: 0.86
===> Epoch [46 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1603, D Fake Loss: -12.5744, D Real Loss: -10.4404, Wasserstein Loss: 12.5659, Content Loss: 0.2952, PSNR: 27.92 SSIM: 0.86
===> Epoch [46 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1600, D Fake Loss: -12.5823, D Real Loss: -10.4569, Wasserstein Loss: 12.5899, Content Loss: 0.2901, PSNR: 28.03 SSIM: 0.86
===> Epoch [46 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1585, D Fake Loss: -12.5435, D Real Loss: -10.4177, Wasserstein Loss: 12.5407, Content Loss: 0.2897, PSNR: 28.01 SSIM: 0.86
===> Epoch [46 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1589, D Fake Loss: -12.5919, D Real Loss: -10.4640, Wasserstein Loss: 12.5958, Content Loss: 0.2905, PSNR: 27.99 SSIM: 0.86
===> Epoch [46 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1608, D Fake Loss: -12.5927, D Real Loss: -10.4561, Wasserstein Loss: 12.5882, Content Loss: 0.2906, PSNR: 28.00 SSIM: 0.86
===> Epoch [46 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1604, D Fake Loss: -12.5987, D Real Loss: -10.4648, Wasserstein Loss: 12.5947, Content Loss: 0.2913, PSNR: 27.99 SSIM: 0.86
===> Epoch [46 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1611, D Fake Loss: -12.5954, D Real Loss: -10.4701, Wasserstein Loss: 12.5934, Content Loss: 0.2904, PSNR: 28.07 SSIM: 0.86
Epoch [46 / 100]: Gradient Penalty: 0.1602, D Fake Loss: -12.5563, D Real Loss: -10.4365, Wasserstein Loss: 12.5554, Content Loss: 0.2918, PSNR: 28.07 SSIM: 0.86
Epoch [47 / 100]
===> Epoch [47 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1578, D Fake Loss: -12.4132, D Real Loss: -10.3188, Wasserstein Loss: 12.4549, Content Loss: 0.2782, PSNR: 28.34 SSIM: 0.86
===> Epoch [47 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1570, D Fake Loss: -12.2896, D Real Loss: -10.1518, Wasserstein Loss: 12.3438, Content Loss: 0.2931, PSNR: 28.06 SSIM: 0.86
===> Epoch [47 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1561, D Fake Loss: -12.2843, D Real Loss: -10.1469, Wasserstein Loss: 12.3103, Content Loss: 0.2984, PSNR: 27.92 SSIM: 0.86
===> Epoch [47 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1579, D Fake Loss: -12.3324, D Real Loss: -10.2025, Wasserstein Loss: 12.3463, Content Loss: 0.2945, PSNR: 28.03 SSIM: 0.86
===> Epoch [47 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1593, D Fake Loss: -12.3450, D Real Loss: -10.2002, Wasserstein Loss: 12.3541, Content Loss: 0.2944, PSNR: 27.95 SSIM: 0.86
===> Epoch [47 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1566, D Fake Loss: -12.2947, D Real Loss: -10.1710, Wasserstein Loss: 12.3090, Content Loss: 0.2913, PSNR: 27.99 SSIM: 0.86
===> Epoch [47 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1570, D Fake Loss: -12.3144, D Real Loss: -10.1907, Wasserstein Loss: 12.3253, Content Loss: 0.2909, PSNR: 28.03 SSIM: 0.86
===> Epoch [47 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1572, D Fake Loss: -12.3336, D Real Loss: -10.2174, Wasserstein Loss: 12.3411, Content Loss: 0.2912, PSNR: 28.04 SSIM: 0.86
===> Epoch [47 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1571, D Fake Loss: -12.3683, D Real Loss: -10.2594, Wasserstein Loss: 12.3827, Content Loss: 0.2908, PSNR: 28.10 SSIM: 0.86
Epoch [47 / 100]: Gradient Penalty: 0.1561, D Fake Loss: -12.3448, D Real Loss: -10.2451, Wasserstein Loss: 12.3683, Content Loss: 0.2919, PSNR: 28.11 SSIM: 0.86
Epoch [48 / 100]
===> Epoch [48 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1544, D Fake Loss: -12.8100, D Real Loss: -10.8029, Wasserstein Loss: 12.6025, Content Loss: 0.2827, PSNR: 28.19 SSIM: 0.86
===> Epoch [48 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1597, D Fake Loss: -12.2883, D Real Loss: -10.2036, Wasserstein Loss: 12.2110, Content Loss: 0.2943, PSNR: 27.99 SSIM: 0.86
===> Epoch [48 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1558, D Fake Loss: -12.1292, D Real Loss: -10.0392, Wasserstein Loss: 12.1301, Content Loss: 0.3016, PSNR: 27.90 SSIM: 0.86
===> Epoch [48 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1561, D Fake Loss: -12.1905, D Real Loss: -10.0932, Wasserstein Loss: 12.1589, Content Loss: 0.2982, PSNR: 28.00 SSIM: 0.86
===> Epoch [48 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1543, D Fake Loss: -12.2241, D Real Loss: -10.1453, Wasserstein Loss: 12.1956, Content Loss: 0.2974, PSNR: 28.04 SSIM: 0.86
===> Epoch [48 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1541, D Fake Loss: -12.2445, D Real Loss: -10.1661, Wasserstein Loss: 12.2231, Content Loss: 0.2968, PSNR: 28.07 SSIM: 0.86
===> Epoch [48 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1545, D Fake Loss: -12.2749, D Real Loss: -10.1978, Wasserstein Loss: 12.2615, Content Loss: 0.2946, PSNR: 28.13 SSIM: 0.86
===> Epoch [48 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1540, D Fake Loss: -12.2637, D Real Loss: -10.1836, Wasserstein Loss: 12.2533, Content Loss: 0.2934, PSNR: 28.12 SSIM: 0.86
===> Epoch [48 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1550, D Fake Loss: -12.2706, D Real Loss: -10.1969, Wasserstein Loss: 12.2721, Content Loss: 0.2920, PSNR: 28.17 SSIM: 0.86
Epoch [48 / 100]: Gradient Penalty: 0.1549, D Fake Loss: -12.2231, D Real Loss: -10.1447, Wasserstein Loss: 12.2293, Content Loss: 0.2919, PSNR: 28.15 SSIM: 0.86
Epoch [49 / 100]
===> Epoch [49 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1488, D Fake Loss: -12.1919, D Real Loss: -10.1982, Wasserstein Loss: 12.0044, Content Loss: 0.2697, PSNR: 28.60 SSIM: 0.86
===> Epoch [49 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1512, D Fake Loss: -11.9111, D Real Loss: -9.8652, Wasserstein Loss: 11.9246, Content Loss: 0.2933, PSNR: 28.21 SSIM: 0.86
===> Epoch [49 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1503, D Fake Loss: -12.0719, D Real Loss: -9.9817, Wasserstein Loss: 12.0881, Content Loss: 0.2978, PSNR: 28.04 SSIM: 0.86
===> Epoch [49 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1523, D Fake Loss: -12.1845, D Real Loss: -10.0943, Wasserstein Loss: 12.1676, Content Loss: 0.2954, PSNR: 28.12 SSIM: 0.86
===> Epoch [49 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1546, D Fake Loss: -12.1931, D Real Loss: -10.0981, Wasserstein Loss: 12.1795, Content Loss: 0.2946, PSNR: 28.10 SSIM: 0.86
===> Epoch [49 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1533, D Fake Loss: -12.1619, D Real Loss: -10.0790, Wasserstein Loss: 12.1431, Content Loss: 0.2926, PSNR: 28.14 SSIM: 0.86
===> Epoch [49 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1522, D Fake Loss: -12.1895, D Real Loss: -10.1248, Wasserstein Loss: 12.1935, Content Loss: 0.2918, PSNR: 28.19 SSIM: 0.86
===> Epoch [49 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1508, D Fake Loss: -12.1956, D Real Loss: -10.1378, Wasserstein Loss: 12.1928, Content Loss: 0.2910, PSNR: 28.21 SSIM: 0.86
===> Epoch [49 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1522, D Fake Loss: -12.2185, D Real Loss: -10.1665, Wasserstein Loss: 12.2055, Content Loss: 0.2898, PSNR: 28.27 SSIM: 0.86
Epoch [49 / 100]: Gradient Penalty: 0.1528, D Fake Loss: -12.1377, D Real Loss: -10.0806, Wasserstein Loss: 12.1352, Content Loss: 0.2895, PSNR: 28.24 SSIM: 0.86
Epoch [50 / 100]
===> Epoch [50 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1602, D Fake Loss: -12.0584, D Real Loss: -10.0933, Wasserstein Loss: 12.0725, Content Loss: 0.2716, PSNR: 28.66 SSIM: 0.86
===> Epoch [50 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1662, D Fake Loss: -11.6331, D Real Loss: -9.5290, Wasserstein Loss: 11.6434, Content Loss: 0.2903, PSNR: 28.13 SSIM: 0.86
===> Epoch [50 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1677, D Fake Loss: -11.4384, D Real Loss: -9.2943, Wasserstein Loss: 11.4343, Content Loss: 0.2936, PSNR: 28.00 SSIM: 0.86
===> Epoch [50 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1594, D Fake Loss: -11.5827, D Real Loss: -9.4924, Wasserstein Loss: 11.5954, Content Loss: 0.2906, PSNR: 28.17 SSIM: 0.86
===> Epoch [50 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1600, D Fake Loss: -11.5779, D Real Loss: -9.4913, Wasserstein Loss: 11.6022, Content Loss: 0.2918, PSNR: 28.12 SSIM: 0.86
===> Epoch [50 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1582, D Fake Loss: -11.5426, D Real Loss: -9.4694, Wasserstein Loss: 11.5502, Content Loss: 0.2905, PSNR: 28.17 SSIM: 0.86
===> Epoch [50 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1564, D Fake Loss: -11.6022, D Real Loss: -9.5518, Wasserstein Loss: 11.6156, Content Loss: 0.2898, PSNR: 28.22 SSIM: 0.86
===> Epoch [50 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1556, D Fake Loss: -11.5805, D Real Loss: -9.5292, Wasserstein Loss: 11.5845, Content Loss: 0.2900, PSNR: 28.23 SSIM: 0.86
===> Epoch [50 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1544, D Fake Loss: -11.6036, D Real Loss: -9.5619, Wasserstein Loss: 11.6118, Content Loss: 0.2894, PSNR: 28.29 SSIM: 0.86
Epoch [50 / 100]: Gradient Penalty: 0.1538, D Fake Loss: -11.5438, D Real Loss: -9.5045, Wasserstein Loss: 11.5537, Content Loss: 0.2891, PSNR: 28.28 SSIM: 0.86
Epoch [51 / 100]
===> Epoch [51 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1200, D Fake Loss: -11.6639, D Real Loss: -9.8499, Wasserstein Loss: 11.6231, Content Loss: 0.2751, PSNR: 29.01 SSIM: 0.87
===> Epoch [51 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1354, D Fake Loss: -11.3513, D Real Loss: -9.4312, Wasserstein Loss: 11.3928, Content Loss: 0.2903, PSNR: 28.58 SSIM: 0.87
===> Epoch [51 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1457, D Fake Loss: -11.2597, D Real Loss: -9.2526, Wasserstein Loss: 11.2756, Content Loss: 0.2964, PSNR: 28.29 SSIM: 0.86
===> Epoch [51 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1457, D Fake Loss: -11.2865, D Real Loss: -9.2879, Wasserstein Loss: 11.3047, Content Loss: 0.2953, PSNR: 28.28 SSIM: 0.86
===> Epoch [51 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1447, D Fake Loss: -11.3034, D Real Loss: -9.2991, Wasserstein Loss: 11.2971, Content Loss: 0.2955, PSNR: 28.24 SSIM: 0.86
===> Epoch [51 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1468, D Fake Loss: -11.3068, D Real Loss: -9.2981, Wasserstein Loss: 11.3115, Content Loss: 0.2932, PSNR: 28.22 SSIM: 0.86
===> Epoch [51 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1474, D Fake Loss: -11.2868, D Real Loss: -9.2894, Wasserstein Loss: 11.2978, Content Loss: 0.2911, PSNR: 28.27 SSIM: 0.86
===> Epoch [51 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1463, D Fake Loss: -11.3075, D Real Loss: -9.3064, Wasserstein Loss: 11.3116, Content Loss: 0.2903, PSNR: 28.27 SSIM: 0.86
===> Epoch [51 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1451, D Fake Loss: -11.3395, D Real Loss: -9.3490, Wasserstein Loss: 11.3425, Content Loss: 0.2894, PSNR: 28.33 SSIM: 0.86
Epoch [51 / 100]: Gradient Penalty: 0.1463, D Fake Loss: -11.2885, D Real Loss: -9.2901, Wasserstein Loss: 11.2890, Content Loss: 0.2900, PSNR: 28.31 SSIM: 0.86
Epoch [52 / 100]
===> Epoch [52 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1464, D Fake Loss: -11.2806, D Real Loss: -9.3639, Wasserstein Loss: 11.3087, Content Loss: 0.2717, PSNR: 28.60 SSIM: 0.87
===> Epoch [52 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1461, D Fake Loss: -11.0556, D Real Loss: -9.0394, Wasserstein Loss: 11.1106, Content Loss: 0.2868, PSNR: 28.27 SSIM: 0.86
===> Epoch [52 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1432, D Fake Loss: -11.0230, D Real Loss: -9.0076, Wasserstein Loss: 11.0525, Content Loss: 0.2930, PSNR: 28.19 SSIM: 0.86
===> Epoch [52 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1415, D Fake Loss: -11.0409, D Real Loss: -9.0434, Wasserstein Loss: 11.0617, Content Loss: 0.2897, PSNR: 28.28 SSIM: 0.86
===> Epoch [52 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1405, D Fake Loss: -11.0169, D Real Loss: -9.0298, Wasserstein Loss: 11.0311, Content Loss: 0.2898, PSNR: 28.30 SSIM: 0.86
===> Epoch [52 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1432, D Fake Loss: -11.0267, D Real Loss: -9.0316, Wasserstein Loss: 11.0446, Content Loss: 0.2901, PSNR: 28.31 SSIM: 0.86
===> Epoch [52 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1441, D Fake Loss: -11.0280, D Real Loss: -9.0387, Wasserstein Loss: 11.0528, Content Loss: 0.2894, PSNR: 28.35 SSIM: 0.86
===> Epoch [52 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1446, D Fake Loss: -11.0379, D Real Loss: -9.0419, Wasserstein Loss: 11.0524, Content Loss: 0.2903, PSNR: 28.34 SSIM: 0.86
===> Epoch [52 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1434, D Fake Loss: -11.0643, D Real Loss: -9.0807, Wasserstein Loss: 11.0768, Content Loss: 0.2898, PSNR: 28.41 SSIM: 0.86
Epoch [52 / 100]: Gradient Penalty: 0.1434, D Fake Loss: -11.0403, D Real Loss: -9.0511, Wasserstein Loss: 11.0471, Content Loss: 0.2894, PSNR: 28.40 SSIM: 0.86
Epoch [53 / 100]
===> Epoch [53 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1352, D Fake Loss: -10.9883, D Real Loss: -9.1213, Wasserstein Loss: 11.0403, Content Loss: 0.2777, PSNR: 28.65 SSIM: 0.87
===> Epoch [53 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1352, D Fake Loss: -10.7572, D Real Loss: -8.8231, Wasserstein Loss: 10.7789, Content Loss: 0.2915, PSNR: 28.43 SSIM: 0.86
===> Epoch [53 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1381, D Fake Loss: -10.7588, D Real Loss: -8.8211, Wasserstein Loss: 10.8093, Content Loss: 0.2978, PSNR: 28.34 SSIM: 0.86
===> Epoch [53 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1400, D Fake Loss: -10.9011, D Real Loss: -8.9554, Wasserstein Loss: 10.9243, Content Loss: 0.2938, PSNR: 28.39 SSIM: 0.86
===> Epoch [53 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1416, D Fake Loss: -10.9277, D Real Loss: -8.9704, Wasserstein Loss: 10.9417, Content Loss: 0.2933, PSNR: 28.34 SSIM: 0.86
===> Epoch [53 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1430, D Fake Loss: -10.8931, D Real Loss: -8.9200, Wasserstein Loss: 10.9043, Content Loss: 0.2917, PSNR: 28.35 SSIM: 0.86
===> Epoch [53 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1417, D Fake Loss: -10.9141, D Real Loss: -8.9578, Wasserstein Loss: 10.9320, Content Loss: 0.2900, PSNR: 28.41 SSIM: 0.86
===> Epoch [53 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1423, D Fake Loss: -10.9082, D Real Loss: -8.9578, Wasserstein Loss: 10.9268, Content Loss: 0.2906, PSNR: 28.40 SSIM: 0.86
===> Epoch [53 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1436, D Fake Loss: -10.8837, D Real Loss: -8.9264, Wasserstein Loss: 10.8966, Content Loss: 0.2894, PSNR: 28.43 SSIM: 0.86
Epoch [53 / 100]: Gradient Penalty: 0.1433, D Fake Loss: -10.8514, D Real Loss: -8.8932, Wasserstein Loss: 10.8690, Content Loss: 0.2891, PSNR: 28.43 SSIM: 0.86
Epoch [54 / 100]
===> Epoch [54 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1308, D Fake Loss: -11.1076, D Real Loss: -9.3396, Wasserstein Loss: 11.0462, Content Loss: 0.2703, PSNR: 29.31 SSIM: 0.87
===> Epoch [54 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1310, D Fake Loss: -10.7667, D Real Loss: -8.9040, Wasserstein Loss: 10.7484, Content Loss: 0.2897, PSNR: 28.84 SSIM: 0.87
===> Epoch [54 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1316, D Fake Loss: -10.6447, D Real Loss: -8.7715, Wasserstein Loss: 10.6364, Content Loss: 0.2904, PSNR: 28.66 SSIM: 0.87
===> Epoch [54 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1370, D Fake Loss: -10.6484, D Real Loss: -8.7419, Wasserstein Loss: 10.6462, Content Loss: 0.2892, PSNR: 28.59 SSIM: 0.87
===> Epoch [54 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1370, D Fake Loss: -10.6156, D Real Loss: -8.7022, Wasserstein Loss: 10.6201, Content Loss: 0.2888, PSNR: 28.54 SSIM: 0.86
===> Epoch [54 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1366, D Fake Loss: -10.6713, D Real Loss: -8.7488, Wasserstein Loss: 10.6694, Content Loss: 0.2871, PSNR: 28.49 SSIM: 0.86
===> Epoch [54 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1364, D Fake Loss: -10.7107, D Real Loss: -8.7912, Wasserstein Loss: 10.7086, Content Loss: 0.2868, PSNR: 28.53 SSIM: 0.86
===> Epoch [54 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1378, D Fake Loss: -10.6364, D Real Loss: -8.7179, Wasserstein Loss: 10.6410, Content Loss: 0.2865, PSNR: 28.52 SSIM: 0.86
===> Epoch [54 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1380, D Fake Loss: -10.6220, D Real Loss: -8.7083, Wasserstein Loss: 10.6231, Content Loss: 0.2857, PSNR: 28.56 SSIM: 0.86
Epoch [54 / 100]: Gradient Penalty: 0.1384, D Fake Loss: -10.5878, D Real Loss: -8.6689, Wasserstein Loss: 10.5929, Content Loss: 0.2860, PSNR: 28.54 SSIM: 0.86
Epoch [55 / 100]
===> Epoch [55 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1322, D Fake Loss: -10.5373, D Real Loss: -8.6527, Wasserstein Loss: 10.5074, Content Loss: 0.2722, PSNR: 28.86 SSIM: 0.87
===> Epoch [55 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1378, D Fake Loss: -10.2782, D Real Loss: -8.2981, Wasserstein Loss: 10.2671, Content Loss: 0.2826, PSNR: 28.46 SSIM: 0.87
===> Epoch [55 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1380, D Fake Loss: -10.2034, D Real Loss: -8.2043, Wasserstein Loss: 10.2042, Content Loss: 0.2867, PSNR: 28.37 SSIM: 0.86
===> Epoch [55 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1346, D Fake Loss: -10.3396, D Real Loss: -8.3981, Wasserstein Loss: 10.3456, Content Loss: 0.2867, PSNR: 28.53 SSIM: 0.86
===> Epoch [55 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1359, D Fake Loss: -10.3099, D Real Loss: -8.3622, Wasserstein Loss: 10.2988, Content Loss: 0.2873, PSNR: 28.52 SSIM: 0.86
===> Epoch [55 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1362, D Fake Loss: -10.2680, D Real Loss: -8.3281, Wasserstein Loss: 10.2543, Content Loss: 0.2883, PSNR: 28.51 SSIM: 0.86
===> Epoch [55 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1376, D Fake Loss: -10.2026, D Real Loss: -8.2762, Wasserstein Loss: 10.1992, Content Loss: 0.2858, PSNR: 28.53 SSIM: 0.86
===> Epoch [55 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1375, D Fake Loss: -10.1605, D Real Loss: -8.2281, Wasserstein Loss: 10.1580, Content Loss: 0.2851, PSNR: 28.52 SSIM: 0.86
===> Epoch [55 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1363, D Fake Loss: -10.2032, D Real Loss: -8.2834, Wasserstein Loss: 10.2039, Content Loss: 0.2845, PSNR: 28.61 SSIM: 0.86
Epoch [55 / 100]: Gradient Penalty: 0.1366, D Fake Loss: -10.2146, D Real Loss: -8.2905, Wasserstein Loss: 10.2124, Content Loss: 0.2859, PSNR: 28.60 SSIM: 0.86
Epoch [56 / 100]
===> Epoch [56 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1450, D Fake Loss: -10.5894, D Real Loss: -8.6904, Wasserstein Loss: 10.6179, Content Loss: 0.2756, PSNR: 28.96 SSIM: 0.87
===> Epoch [56 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1514, D Fake Loss: -10.3111, D Real Loss: -8.3018, Wasserstein Loss: 10.3180, Content Loss: 0.2860, PSNR: 28.48 SSIM: 0.87
===> Epoch [56 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1470, D Fake Loss: -10.3579, D Real Loss: -8.3783, Wasserstein Loss: 10.3671, Content Loss: 0.2900, PSNR: 28.40 SSIM: 0.86
===> Epoch [56 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1443, D Fake Loss: -10.3799, D Real Loss: -8.4259, Wasserstein Loss: 10.3955, Content Loss: 0.2885, PSNR: 28.47 SSIM: 0.87
===> Epoch [56 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1426, D Fake Loss: -10.3725, D Real Loss: -8.4343, Wasserstein Loss: 10.3908, Content Loss: 0.2887, PSNR: 28.50 SSIM: 0.87
===> Epoch [56 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1439, D Fake Loss: -10.3168, D Real Loss: -8.3798, Wasserstein Loss: 10.3291, Content Loss: 0.2875, PSNR: 28.51 SSIM: 0.87
===> Epoch [56 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1428, D Fake Loss: -10.2858, D Real Loss: -8.3624, Wasserstein Loss: 10.2982, Content Loss: 0.2863, PSNR: 28.55 SSIM: 0.87
===> Epoch [56 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1419, D Fake Loss: -10.2155, D Real Loss: -8.2853, Wasserstein Loss: 10.2221, Content Loss: 0.2853, PSNR: 28.55 SSIM: 0.87
===> Epoch [56 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1394, D Fake Loss: -10.2289, D Real Loss: -8.3149, Wasserstein Loss: 10.2317, Content Loss: 0.2845, PSNR: 28.64 SSIM: 0.87
Epoch [56 / 100]: Gradient Penalty: 0.1393, D Fake Loss: -10.1580, D Real Loss: -8.2429, Wasserstein Loss: 10.1645, Content Loss: 0.2848, PSNR: 28.62 SSIM: 0.87
Epoch [57 / 100]
===> Epoch [57 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1121, D Fake Loss: -10.1436, D Real Loss: -8.3881, Wasserstein Loss: 10.1060, Content Loss: 0.2719, PSNR: 29.05 SSIM: 0.87
===> Epoch [57 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1246, D Fake Loss: -9.8933, D Real Loss: -8.0439, Wasserstein Loss: 9.9185, Content Loss: 0.2825, PSNR: 28.72 SSIM: 0.87
===> Epoch [57 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1284, D Fake Loss: -9.9172, D Real Loss: -8.0501, Wasserstein Loss: 9.9580, Content Loss: 0.2894, PSNR: 28.59 SSIM: 0.87
===> Epoch [57 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1292, D Fake Loss: -10.0557, D Real Loss: -8.1868, Wasserstein Loss: 10.0680, Content Loss: 0.2861, PSNR: 28.68 SSIM: 0.87
===> Epoch [57 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1286, D Fake Loss: -10.0706, D Real Loss: -8.2006, Wasserstein Loss: 10.0620, Content Loss: 0.2859, PSNR: 28.68 SSIM: 0.87
===> Epoch [57 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1317, D Fake Loss: -9.9529, D Real Loss: -8.0747, Wasserstein Loss: 9.9481, Content Loss: 0.2848, PSNR: 28.65 SSIM: 0.87
===> Epoch [57 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1313, D Fake Loss: -9.8835, D Real Loss: -8.0088, Wasserstein Loss: 9.8828, Content Loss: 0.2845, PSNR: 28.68 SSIM: 0.87
===> Epoch [57 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1310, D Fake Loss: -9.9028, D Real Loss: -8.0296, Wasserstein Loss: 9.9063, Content Loss: 0.2849, PSNR: 28.67 SSIM: 0.87
===> Epoch [57 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1310, D Fake Loss: -9.9312, D Real Loss: -8.0580, Wasserstein Loss: 9.9348, Content Loss: 0.2844, PSNR: 28.72 SSIM: 0.87
Epoch [57 / 100]: Gradient Penalty: 0.1322, D Fake Loss: -9.8794, D Real Loss: -7.9966, Wasserstein Loss: 9.8843, Content Loss: 0.2844, PSNR: 28.69 SSIM: 0.87
Epoch [58 / 100]
===> Epoch [58 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1284, D Fake Loss: -9.7536, D Real Loss: -7.9334, Wasserstein Loss: 9.7144, Content Loss: 0.2705, PSNR: 28.87 SSIM: 0.87
===> Epoch [58 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1288, D Fake Loss: -9.4623, D Real Loss: -7.5972, Wasserstein Loss: 9.4750, Content Loss: 0.2811, PSNR: 28.61 SSIM: 0.87
===> Epoch [58 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1315, D Fake Loss: -9.4413, D Real Loss: -7.5326, Wasserstein Loss: 9.4559, Content Loss: 0.2872, PSNR: 28.48 SSIM: 0.86
===> Epoch [58 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1312, D Fake Loss: -9.5296, D Real Loss: -7.6416, Wasserstein Loss: 9.5314, Content Loss: 0.2838, PSNR: 28.55 SSIM: 0.87
===> Epoch [58 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1311, D Fake Loss: -9.5402, D Real Loss: -7.6497, Wasserstein Loss: 9.5397, Content Loss: 0.2846, PSNR: 28.57 SSIM: 0.87
===> Epoch [58 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1302, D Fake Loss: -9.6055, D Real Loss: -7.7191, Wasserstein Loss: 9.6118, Content Loss: 0.2841, PSNR: 28.59 SSIM: 0.87
===> Epoch [58 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1296, D Fake Loss: -9.6253, D Real Loss: -7.7464, Wasserstein Loss: 9.6262, Content Loss: 0.2830, PSNR: 28.65 SSIM: 0.87
===> Epoch [58 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1287, D Fake Loss: -9.6246, D Real Loss: -7.7519, Wasserstein Loss: 9.6289, Content Loss: 0.2846, PSNR: 28.66 SSIM: 0.87
===> Epoch [58 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1283, D Fake Loss: -9.7125, D Real Loss: -7.8513, Wasserstein Loss: 9.7220, Content Loss: 0.2832, PSNR: 28.71 SSIM: 0.87
Epoch [58 / 100]: Gradient Penalty: 0.1281, D Fake Loss: -9.7494, D Real Loss: -7.8845, Wasserstein Loss: 9.7546, Content Loss: 0.2830, PSNR: 28.70 SSIM: 0.87
Epoch [59 / 100]
===> Epoch [59 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1289, D Fake Loss: -9.8856, D Real Loss: -8.0583, Wasserstein Loss: 9.8412, Content Loss: 0.2681, PSNR: 29.02 SSIM: 0.87
===> Epoch [59 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1281, D Fake Loss: -9.7317, D Real Loss: -7.8756, Wasserstein Loss: 9.7762, Content Loss: 0.2825, PSNR: 28.67 SSIM: 0.87
===> Epoch [59 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1300, D Fake Loss: -9.7827, D Real Loss: -7.8985, Wasserstein Loss: 9.7882, Content Loss: 0.2876, PSNR: 28.55 SSIM: 0.87
===> Epoch [59 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1274, D Fake Loss: -9.8933, D Real Loss: -8.0257, Wasserstein Loss: 9.9001, Content Loss: 0.2881, PSNR: 28.61 SSIM: 0.87
===> Epoch [59 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1302, D Fake Loss: -9.8457, D Real Loss: -7.9762, Wasserstein Loss: 9.8548, Content Loss: 0.2877, PSNR: 28.59 SSIM: 0.87
===> Epoch [59 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1294, D Fake Loss: -9.8681, D Real Loss: -8.0037, Wasserstein Loss: 9.8665, Content Loss: 0.2851, PSNR: 28.63 SSIM: 0.87
===> Epoch [59 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1284, D Fake Loss: -9.9435, D Real Loss: -8.0802, Wasserstein Loss: 9.9544, Content Loss: 0.2823, PSNR: 28.67 SSIM: 0.87
===> Epoch [59 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1288, D Fake Loss: -10.0091, D Real Loss: -8.1547, Wasserstein Loss: 10.0181, Content Loss: 0.2829, PSNR: 28.71 SSIM: 0.87
===> Epoch [59 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1278, D Fake Loss: -10.0914, D Real Loss: -8.2497, Wasserstein Loss: 10.0967, Content Loss: 0.2822, PSNR: 28.78 SSIM: 0.87
Epoch [59 / 100]: Gradient Penalty: 0.1288, D Fake Loss: -10.1003, D Real Loss: -8.2562, Wasserstein Loss: 10.1103, Content Loss: 0.2824, PSNR: 28.80 SSIM: 0.87
Epoch [60 / 100]
===> Epoch [60 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1187, D Fake Loss: -10.3465, D Real Loss: -8.6561, Wasserstein Loss: 10.3603, Content Loss: 0.2701, PSNR: 29.45 SSIM: 0.87
===> Epoch [60 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1197, D Fake Loss: -10.1487, D Real Loss: -8.3674, Wasserstein Loss: 10.1503, Content Loss: 0.2850, PSNR: 28.83 SSIM: 0.87
===> Epoch [60 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1225, D Fake Loss: -10.0210, D Real Loss: -8.1721, Wasserstein Loss: 10.0269, Content Loss: 0.2877, PSNR: 28.67 SSIM: 0.87
===> Epoch [60 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1236, D Fake Loss: -10.1072, D Real Loss: -8.2645, Wasserstein Loss: 10.1210, Content Loss: 0.2850, PSNR: 28.65 SSIM: 0.87
===> Epoch [60 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1262, D Fake Loss: -10.1079, D Real Loss: -8.2570, Wasserstein Loss: 10.1079, Content Loss: 0.2843, PSNR: 28.65 SSIM: 0.87
===> Epoch [60 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1263, D Fake Loss: -10.0954, D Real Loss: -8.2486, Wasserstein Loss: 10.1017, Content Loss: 0.2834, PSNR: 28.67 SSIM: 0.87
===> Epoch [60 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1257, D Fake Loss: -10.0972, D Real Loss: -8.2635, Wasserstein Loss: 10.1039, Content Loss: 0.2829, PSNR: 28.73 SSIM: 0.87
===> Epoch [60 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1249, D Fake Loss: -10.1362, D Real Loss: -8.3081, Wasserstein Loss: 10.1445, Content Loss: 0.2830, PSNR: 28.78 SSIM: 0.87
===> Epoch [60 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1262, D Fake Loss: -10.1319, D Real Loss: -8.3038, Wasserstein Loss: 10.1432, Content Loss: 0.2825, PSNR: 28.82 SSIM: 0.87
Epoch [60 / 100]: Gradient Penalty: 0.1263, D Fake Loss: -10.1167, D Real Loss: -8.2932, Wasserstein Loss: 10.1238, Content Loss: 0.2827, PSNR: 28.82 SSIM: 0.87
Epoch [61 / 100]
===> Epoch [61 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1222, D Fake Loss: -10.3105, D Real Loss: -8.6045, Wasserstein Loss: 10.3091, Content Loss: 0.2683, PSNR: 29.10 SSIM: 0.87
===> Epoch [61 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1209, D Fake Loss: -10.0693, D Real Loss: -8.2584, Wasserstein Loss: 10.0933, Content Loss: 0.2826, PSNR: 28.76 SSIM: 0.87
===> Epoch [61 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1251, D Fake Loss: -9.9836, D Real Loss: -8.1308, Wasserstein Loss: 9.9927, Content Loss: 0.2887, PSNR: 28.66 SSIM: 0.87
===> Epoch [61 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1217, D Fake Loss: -10.0816, D Real Loss: -8.2786, Wasserstein Loss: 10.0930, Content Loss: 0.2877, PSNR: 28.79 SSIM: 0.87
===> Epoch [61 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1221, D Fake Loss: -10.0307, D Real Loss: -8.2305, Wasserstein Loss: 10.0320, Content Loss: 0.2863, PSNR: 28.81 SSIM: 0.87
===> Epoch [61 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1205, D Fake Loss: -10.0084, D Real Loss: -8.2163, Wasserstein Loss: 10.0188, Content Loss: 0.2847, PSNR: 28.79 SSIM: 0.87
===> Epoch [61 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1210, D Fake Loss: -10.0012, D Real Loss: -8.2044, Wasserstein Loss: 10.0109, Content Loss: 0.2847, PSNR: 28.82 SSIM: 0.87
===> Epoch [61 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1211, D Fake Loss: -9.9699, D Real Loss: -8.1728, Wasserstein Loss: 9.9794, Content Loss: 0.2843, PSNR: 28.83 SSIM: 0.87
===> Epoch [61 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1221, D Fake Loss: -9.9889, D Real Loss: -8.1935, Wasserstein Loss: 10.0007, Content Loss: 0.2829, PSNR: 28.88 SSIM: 0.87
Epoch [61 / 100]: Gradient Penalty: 0.1221, D Fake Loss: -9.9347, D Real Loss: -8.1303, Wasserstein Loss: 9.9382, Content Loss: 0.2818, PSNR: 28.87 SSIM: 0.87
Epoch [62 / 100]
===> Epoch [62 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1230, D Fake Loss: -9.6930, D Real Loss: -7.9289, Wasserstein Loss: 9.7491, Content Loss: 0.2676, PSNR: 29.20 SSIM: 0.87
===> Epoch [62 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1198, D Fake Loss: -9.4928, D Real Loss: -7.7225, Wasserstein Loss: 9.5232, Content Loss: 0.2798, PSNR: 28.97 SSIM: 0.87
===> Epoch [62 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1211, D Fake Loss: -9.5252, D Real Loss: -7.7422, Wasserstein Loss: 9.5431, Content Loss: 0.2851, PSNR: 28.81 SSIM: 0.87
===> Epoch [62 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1196, D Fake Loss: -9.6835, D Real Loss: -7.8901, Wasserstein Loss: 9.7069, Content Loss: 0.2851, PSNR: 28.79 SSIM: 0.87
===> Epoch [62 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1213, D Fake Loss: -9.7021, D Real Loss: -7.8940, Wasserstein Loss: 9.7224, Content Loss: 0.2845, PSNR: 28.72 SSIM: 0.87
===> Epoch [62 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1211, D Fake Loss: -9.6850, D Real Loss: -7.8922, Wasserstein Loss: 9.7002, Content Loss: 0.2841, PSNR: 28.78 SSIM: 0.87
===> Epoch [62 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1204, D Fake Loss: -9.6537, D Real Loss: -7.8694, Wasserstein Loss: 9.6607, Content Loss: 0.2823, PSNR: 28.82 SSIM: 0.87
===> Epoch [62 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1202, D Fake Loss: -9.6232, D Real Loss: -7.8360, Wasserstein Loss: 9.6292, Content Loss: 0.2823, PSNR: 28.84 SSIM: 0.87
===> Epoch [62 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1191, D Fake Loss: -9.6340, D Real Loss: -7.8566, Wasserstein Loss: 9.6524, Content Loss: 0.2817, PSNR: 28.91 SSIM: 0.87
Epoch [62 / 100]: Gradient Penalty: 0.1187, D Fake Loss: -9.6300, D Real Loss: -7.8505, Wasserstein Loss: 9.6343, Content Loss: 0.2820, PSNR: 28.90 SSIM: 0.87
Epoch [63 / 100]
===> Epoch [63 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1333, D Fake Loss: -9.7284, D Real Loss: -7.9132, Wasserstein Loss: 9.8134, Content Loss: 0.2662, PSNR: 29.11 SSIM: 0.87
===> Epoch [63 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1236, D Fake Loss: -9.4474, D Real Loss: -7.6374, Wasserstein Loss: 9.4920, Content Loss: 0.2765, PSNR: 28.88 SSIM: 0.87
===> Epoch [63 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1233, D Fake Loss: -9.5179, D Real Loss: -7.6784, Wasserstein Loss: 9.5459, Content Loss: 0.2819, PSNR: 28.71 SSIM: 0.87
===> Epoch [63 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1205, D Fake Loss: -9.5722, D Real Loss: -7.7713, Wasserstein Loss: 9.6061, Content Loss: 0.2815, PSNR: 28.81 SSIM: 0.87
===> Epoch [63 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1169, D Fake Loss: -9.6085, D Real Loss: -7.8291, Wasserstein Loss: 9.6292, Content Loss: 0.2818, PSNR: 28.85 SSIM: 0.87
===> Epoch [63 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1169, D Fake Loss: -9.6919, D Real Loss: -7.9191, Wasserstein Loss: 9.7172, Content Loss: 0.2814, PSNR: 28.89 SSIM: 0.87
===> Epoch [63 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1178, D Fake Loss: -9.7120, D Real Loss: -7.9426, Wasserstein Loss: 9.7287, Content Loss: 0.2804, PSNR: 28.91 SSIM: 0.87
===> Epoch [63 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1196, D Fake Loss: -9.6690, D Real Loss: -7.8979, Wasserstein Loss: 9.6833, Content Loss: 0.2808, PSNR: 28.91 SSIM: 0.87
===> Epoch [63 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1183, D Fake Loss: -9.7188, D Real Loss: -7.9584, Wasserstein Loss: 9.7312, Content Loss: 0.2797, PSNR: 28.97 SSIM: 0.87
Epoch [63 / 100]: Gradient Penalty: 0.1179, D Fake Loss: -9.6905, D Real Loss: -7.9263, Wasserstein Loss: 9.7051, Content Loss: 0.2805, PSNR: 28.94 SSIM: 0.87
Epoch [64 / 100]
===> Epoch [64 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1210, D Fake Loss: -10.1018, D Real Loss: -8.3824, Wasserstein Loss: 10.0753, Content Loss: 0.2686, PSNR: 29.24 SSIM: 0.87
===> Epoch [64 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1194, D Fake Loss: -9.6344, D Real Loss: -7.8612, Wasserstein Loss: 9.6598, Content Loss: 0.2806, PSNR: 28.91 SSIM: 0.87
===> Epoch [64 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1212, D Fake Loss: -9.5955, D Real Loss: -7.7866, Wasserstein Loss: 9.5957, Content Loss: 0.2845, PSNR: 28.76 SSIM: 0.87
===> Epoch [64 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1182, D Fake Loss: -9.6197, D Real Loss: -7.8293, Wasserstein Loss: 9.6170, Content Loss: 0.2822, PSNR: 28.83 SSIM: 0.87
===> Epoch [64 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1163, D Fake Loss: -9.6331, D Real Loss: -7.8603, Wasserstein Loss: 9.6446, Content Loss: 0.2813, PSNR: 28.85 SSIM: 0.87
===> Epoch [64 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1162, D Fake Loss: -9.6427, D Real Loss: -7.8746, Wasserstein Loss: 9.6519, Content Loss: 0.2808, PSNR: 28.85 SSIM: 0.87
===> Epoch [64 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1161, D Fake Loss: -9.6268, D Real Loss: -7.8651, Wasserstein Loss: 9.6307, Content Loss: 0.2797, PSNR: 28.90 SSIM: 0.87
===> Epoch [64 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1161, D Fake Loss: -9.6284, D Real Loss: -7.8740, Wasserstein Loss: 9.6369, Content Loss: 0.2803, PSNR: 28.93 SSIM: 0.87
===> Epoch [64 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1156, D Fake Loss: -9.6721, D Real Loss: -7.9230, Wasserstein Loss: 9.6792, Content Loss: 0.2796, PSNR: 28.98 SSIM: 0.87
Epoch [64 / 100]: Gradient Penalty: 0.1157, D Fake Loss: -9.6345, D Real Loss: -7.8879, Wasserstein Loss: 9.6419, Content Loss: 0.2804, PSNR: 28.97 SSIM: 0.87
Epoch [65 / 100]
===> Epoch [65 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1017, D Fake Loss: -9.7521, D Real Loss: -8.0647, Wasserstein Loss: 9.7521, Content Loss: 0.2654, PSNR: 29.22 SSIM: 0.87
===> Epoch [65 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1124, D Fake Loss: -9.5761, D Real Loss: -7.7937, Wasserstein Loss: 9.5929, Content Loss: 0.2785, PSNR: 28.93 SSIM: 0.87
===> Epoch [65 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1152, D Fake Loss: -9.6174, D Real Loss: -7.8412, Wasserstein Loss: 9.6364, Content Loss: 0.2828, PSNR: 28.80 SSIM: 0.87
===> Epoch [65 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1122, D Fake Loss: -9.7961, D Real Loss: -8.0353, Wasserstein Loss: 9.7998, Content Loss: 0.2829, PSNR: 28.90 SSIM: 0.87
===> Epoch [65 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1140, D Fake Loss: -9.9039, D Real Loss: -8.1448, Wasserstein Loss: 9.9211, Content Loss: 0.2824, PSNR: 28.90 SSIM: 0.87
===> Epoch [65 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1142, D Fake Loss: -10.0100, D Real Loss: -8.2554, Wasserstein Loss: 10.0193, Content Loss: 0.2814, PSNR: 28.93 SSIM: 0.87
===> Epoch [65 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1136, D Fake Loss: -10.0686, D Real Loss: -8.3341, Wasserstein Loss: 10.0769, Content Loss: 0.2799, PSNR: 28.98 SSIM: 0.87
===> Epoch [65 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1144, D Fake Loss: -10.0230, D Real Loss: -8.2794, Wasserstein Loss: 10.0291, Content Loss: 0.2813, PSNR: 28.96 SSIM: 0.87
===> Epoch [65 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1156, D Fake Loss: -9.9921, D Real Loss: -8.2500, Wasserstein Loss: 9.9965, Content Loss: 0.2799, PSNR: 29.03 SSIM: 0.87
Epoch [65 / 100]: Gradient Penalty: 0.1151, D Fake Loss: -9.9681, D Real Loss: -8.2237, Wasserstein Loss: 9.9696, Content Loss: 0.2807, PSNR: 29.01 SSIM: 0.87
Epoch [66 / 100]
===> Epoch [66 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1124, D Fake Loss: -9.9196, D Real Loss: -8.2217, Wasserstein Loss: 9.9616, Content Loss: 0.2638, PSNR: 29.52 SSIM: 0.87
===> Epoch [66 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1105, D Fake Loss: -9.8834, D Real Loss: -8.1907, Wasserstein Loss: 9.9281, Content Loss: 0.2809, PSNR: 29.20 SSIM: 0.87
===> Epoch [66 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1143, D Fake Loss: -9.8671, D Real Loss: -8.1105, Wasserstein Loss: 9.8942, Content Loss: 0.2861, PSNR: 28.92 SSIM: 0.87
===> Epoch [66 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1130, D Fake Loss: -9.9099, D Real Loss: -8.1554, Wasserstein Loss: 9.9189, Content Loss: 0.2829, PSNR: 28.95 SSIM: 0.87
===> Epoch [66 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1146, D Fake Loss: -9.8956, D Real Loss: -8.1312, Wasserstein Loss: 9.9160, Content Loss: 0.2843, PSNR: 28.90 SSIM: 0.87
===> Epoch [66 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1153, D Fake Loss: -9.9196, D Real Loss: -8.1641, Wasserstein Loss: 9.9390, Content Loss: 0.2827, PSNR: 28.92 SSIM: 0.87
===> Epoch [66 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1157, D Fake Loss: -9.8791, D Real Loss: -8.1206, Wasserstein Loss: 9.8963, Content Loss: 0.2810, PSNR: 28.98 SSIM: 0.87
===> Epoch [66 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1161, D Fake Loss: -9.8682, D Real Loss: -8.1175, Wasserstein Loss: 9.8808, Content Loss: 0.2804, PSNR: 29.01 SSIM: 0.87
===> Epoch [66 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1163, D Fake Loss: -9.8649, D Real Loss: -8.1171, Wasserstein Loss: 9.8755, Content Loss: 0.2801, PSNR: 29.03 SSIM: 0.87
Epoch [66 / 100]: Gradient Penalty: 0.1159, D Fake Loss: -9.8295, D Real Loss: -8.0849, Wasserstein Loss: 9.8445, Content Loss: 0.2795, PSNR: 29.02 SSIM: 0.87
Epoch [67 / 100]
===> Epoch [67 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1094, D Fake Loss: -9.7242, D Real Loss: -8.0598, Wasserstein Loss: 9.7000, Content Loss: 0.2655, PSNR: 29.48 SSIM: 0.87
===> Epoch [67 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1141, D Fake Loss: -9.5978, D Real Loss: -7.8445, Wasserstein Loss: 9.6079, Content Loss: 0.2756, PSNR: 29.06 SSIM: 0.87
===> Epoch [67 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1158, D Fake Loss: -9.7121, D Real Loss: -7.9572, Wasserstein Loss: 9.7276, Content Loss: 0.2812, PSNR: 29.01 SSIM: 0.87
===> Epoch [67 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1135, D Fake Loss: -9.8714, D Real Loss: -8.1330, Wasserstein Loss: 9.8698, Content Loss: 0.2809, PSNR: 29.07 SSIM: 0.87
===> Epoch [67 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1112, D Fake Loss: -9.9243, D Real Loss: -8.1960, Wasserstein Loss: 9.9333, Content Loss: 0.2808, PSNR: 29.07 SSIM: 0.87
===> Epoch [67 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1133, D Fake Loss: -9.9138, D Real Loss: -8.1673, Wasserstein Loss: 9.9166, Content Loss: 0.2806, PSNR: 29.03 SSIM: 0.87
===> Epoch [67 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1106, D Fake Loss: -9.9532, D Real Loss: -8.2335, Wasserstein Loss: 9.9552, Content Loss: 0.2789, PSNR: 29.12 SSIM: 0.87
===> Epoch [67 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1112, D Fake Loss: -9.9741, D Real Loss: -8.2560, Wasserstein Loss: 9.9787, Content Loss: 0.2791, PSNR: 29.14 SSIM: 0.87
===> Epoch [67 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1107, D Fake Loss: -10.0166, D Real Loss: -8.3021, Wasserstein Loss: 10.0236, Content Loss: 0.2793, PSNR: 29.17 SSIM: 0.87
Epoch [67 / 100]: Gradient Penalty: 0.1094, D Fake Loss: -10.0244, D Real Loss: -8.3148, Wasserstein Loss: 10.0283, Content Loss: 0.2792, PSNR: 29.16 SSIM: 0.87
Epoch [68 / 100]
===> Epoch [68 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1132, D Fake Loss: -10.1577, D Real Loss: -8.5143, Wasserstein Loss: 10.1974, Content Loss: 0.2671, PSNR: 29.37 SSIM: 0.87
===> Epoch [68 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1171, D Fake Loss: -10.0419, D Real Loss: -8.2954, Wasserstein Loss: 10.0795, Content Loss: 0.2822, PSNR: 29.02 SSIM: 0.87
===> Epoch [68 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1175, D Fake Loss: -10.0399, D Real Loss: -8.2710, Wasserstein Loss: 10.0482, Content Loss: 0.2870, PSNR: 28.86 SSIM: 0.87
===> Epoch [68 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1174, D Fake Loss: -10.1518, D Real Loss: -8.3972, Wasserstein Loss: 10.1771, Content Loss: 0.2819, PSNR: 28.94 SSIM: 0.87
===> Epoch [68 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1154, D Fake Loss: -10.1460, D Real Loss: -8.4034, Wasserstein Loss: 10.1422, Content Loss: 0.2821, PSNR: 28.97 SSIM: 0.87
===> Epoch [68 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1131, D Fake Loss: -10.1565, D Real Loss: -8.4476, Wasserstein Loss: 10.1669, Content Loss: 0.2804, PSNR: 29.02 SSIM: 0.87
===> Epoch [68 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1111, D Fake Loss: -10.1299, D Real Loss: -8.4312, Wasserstein Loss: 10.1364, Content Loss: 0.2793, PSNR: 29.09 SSIM: 0.87
===> Epoch [68 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1104, D Fake Loss: -10.0923, D Real Loss: -8.3977, Wasserstein Loss: 10.0994, Content Loss: 0.2788, PSNR: 29.12 SSIM: 0.87
===> Epoch [68 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1097, D Fake Loss: -10.0987, D Real Loss: -8.4059, Wasserstein Loss: 10.1092, Content Loss: 0.2785, PSNR: 29.17 SSIM: 0.87
Epoch [68 / 100]: Gradient Penalty: 0.1112, D Fake Loss: -10.0776, D Real Loss: -8.3774, Wasserstein Loss: 10.0866, Content Loss: 0.2790, PSNR: 29.15 SSIM: 0.87
Epoch [69 / 100]
===> Epoch [69 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1021, D Fake Loss: -9.8776, D Real Loss: -8.2994, Wasserstein Loss: 9.8491, Content Loss: 0.2654, PSNR: 29.47 SSIM: 0.87
===> Epoch [69 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1047, D Fake Loss: -9.7185, D Real Loss: -8.0376, Wasserstein Loss: 9.7085, Content Loss: 0.2780, PSNR: 29.11 SSIM: 0.87
===> Epoch [69 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1110, D Fake Loss: -9.7047, D Real Loss: -7.9770, Wasserstein Loss: 9.7050, Content Loss: 0.2858, PSNR: 28.92 SSIM: 0.87
===> Epoch [69 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1086, D Fake Loss: -9.7956, D Real Loss: -8.0849, Wasserstein Loss: 9.8031, Content Loss: 0.2848, PSNR: 28.92 SSIM: 0.87
===> Epoch [69 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1093, D Fake Loss: -9.8308, D Real Loss: -8.1239, Wasserstein Loss: 9.8246, Content Loss: 0.2829, PSNR: 28.95 SSIM: 0.87
===> Epoch [69 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1093, D Fake Loss: -9.8383, D Real Loss: -8.1376, Wasserstein Loss: 9.8514, Content Loss: 0.2810, PSNR: 29.01 SSIM: 0.87
===> Epoch [69 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1088, D Fake Loss: -9.8770, D Real Loss: -8.1868, Wasserstein Loss: 9.8826, Content Loss: 0.2794, PSNR: 29.11 SSIM: 0.87
===> Epoch [69 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1093, D Fake Loss: -9.8895, D Real Loss: -8.1895, Wasserstein Loss: 9.8930, Content Loss: 0.2786, PSNR: 29.11 SSIM: 0.87
===> Epoch [69 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1090, D Fake Loss: -9.9375, D Real Loss: -8.2454, Wasserstein Loss: 9.9425, Content Loss: 0.2775, PSNR: 29.18 SSIM: 0.87
Epoch [69 / 100]: Gradient Penalty: 0.1101, D Fake Loss: -9.9054, D Real Loss: -8.1985, Wasserstein Loss: 9.9108, Content Loss: 0.2776, PSNR: 29.16 SSIM: 0.87
Epoch [70 / 100]
===> Epoch [70 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1080, D Fake Loss: -9.8677, D Real Loss: -8.2313, Wasserstein Loss: 9.8478, Content Loss: 0.2654, PSNR: 29.59 SSIM: 0.87
===> Epoch [70 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1173, D Fake Loss: -9.6590, D Real Loss: -7.9531, Wasserstein Loss: 9.6858, Content Loss: 0.2769, PSNR: 29.17 SSIM: 0.87
===> Epoch [70 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1110, D Fake Loss: -9.7852, D Real Loss: -8.0748, Wasserstein Loss: 9.7910, Content Loss: 0.2812, PSNR: 29.02 SSIM: 0.87
===> Epoch [70 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1114, D Fake Loss: -9.8289, D Real Loss: -8.1172, Wasserstein Loss: 9.8342, Content Loss: 0.2792, PSNR: 29.04 SSIM: 0.87
===> Epoch [70 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1095, D Fake Loss: -9.8827, D Real Loss: -8.1747, Wasserstein Loss: 9.8917, Content Loss: 0.2798, PSNR: 29.04 SSIM: 0.87
===> Epoch [70 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1093, D Fake Loss: -9.8487, D Real Loss: -8.1532, Wasserstein Loss: 9.8630, Content Loss: 0.2793, PSNR: 29.08 SSIM: 0.87
===> Epoch [70 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1100, D Fake Loss: -9.8519, D Real Loss: -8.1512, Wasserstein Loss: 9.8465, Content Loss: 0.2787, PSNR: 29.11 SSIM: 0.87
===> Epoch [70 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1094, D Fake Loss: -9.8580, D Real Loss: -8.1642, Wasserstein Loss: 9.8599, Content Loss: 0.2793, PSNR: 29.13 SSIM: 0.87
===> Epoch [70 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1077, D Fake Loss: -9.9102, D Real Loss: -8.2315, Wasserstein Loss: 9.9192, Content Loss: 0.2785, PSNR: 29.19 SSIM: 0.87
Epoch [70 / 100]: Gradient Penalty: 0.1080, D Fake Loss: -9.9233, D Real Loss: -8.2452, Wasserstein Loss: 9.9276, Content Loss: 0.2783, PSNR: 29.18 SSIM: 0.87
Epoch [71 / 100]
===> Epoch [71 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1064, D Fake Loss: -10.1046, D Real Loss: -8.5003, Wasserstein Loss: 10.1511, Content Loss: 0.2626, PSNR: 29.53 SSIM: 0.87
===> Epoch [71 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1063, D Fake Loss: -10.1266, D Real Loss: -8.4573, Wasserstein Loss: 10.1479, Content Loss: 0.2792, PSNR: 29.11 SSIM: 0.87
===> Epoch [71 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1083, D Fake Loss: -10.1191, D Real Loss: -8.4067, Wasserstein Loss: 10.1344, Content Loss: 0.2842, PSNR: 28.99 SSIM: 0.87
===> Epoch [71 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1101, D Fake Loss: -10.1691, D Real Loss: -8.4573, Wasserstein Loss: 10.1948, Content Loss: 0.2821, PSNR: 29.06 SSIM: 0.87
===> Epoch [71 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1103, D Fake Loss: -10.1812, D Real Loss: -8.4726, Wasserstein Loss: 10.1892, Content Loss: 0.2818, PSNR: 29.06 SSIM: 0.87
===> Epoch [71 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1080, D Fake Loss: -10.1845, D Real Loss: -8.4905, Wasserstein Loss: 10.1924, Content Loss: 0.2812, PSNR: 29.07 SSIM: 0.87
===> Epoch [71 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1068, D Fake Loss: -10.1875, D Real Loss: -8.5073, Wasserstein Loss: 10.1998, Content Loss: 0.2786, PSNR: 29.15 SSIM: 0.87
===> Epoch [71 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1075, D Fake Loss: -10.1700, D Real Loss: -8.4871, Wasserstein Loss: 10.1811, Content Loss: 0.2779, PSNR: 29.15 SSIM: 0.87
===> Epoch [71 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1068, D Fake Loss: -10.1946, D Real Loss: -8.5220, Wasserstein Loss: 10.2049, Content Loss: 0.2777, PSNR: 29.20 SSIM: 0.87
Epoch [71 / 100]: Gradient Penalty: 0.1072, D Fake Loss: -10.1433, D Real Loss: -8.4651, Wasserstein Loss: 10.1498, Content Loss: 0.2767, PSNR: 29.21 SSIM: 0.87
Epoch [72 / 100]
===> Epoch [72 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0911, D Fake Loss: -10.1050, D Real Loss: -8.5718, Wasserstein Loss: 10.1654, Content Loss: 0.2616, PSNR: 29.97 SSIM: 0.88
===> Epoch [72 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0999, D Fake Loss: -10.1924, D Real Loss: -8.5580, Wasserstein Loss: 10.2089, Content Loss: 0.2809, PSNR: 29.40 SSIM: 0.87
===> Epoch [72 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1035, D Fake Loss: -10.3404, D Real Loss: -8.6815, Wasserstein Loss: 10.3649, Content Loss: 0.2843, PSNR: 29.20 SSIM: 0.87
===> Epoch [72 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1043, D Fake Loss: -10.4751, D Real Loss: -8.8137, Wasserstein Loss: 10.4957, Content Loss: 0.2827, PSNR: 29.16 SSIM: 0.87
===> Epoch [72 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1060, D Fake Loss: -10.4808, D Real Loss: -8.8237, Wasserstein Loss: 10.4938, Content Loss: 0.2833, PSNR: 29.12 SSIM: 0.87
===> Epoch [72 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1051, D Fake Loss: -10.4958, D Real Loss: -8.8414, Wasserstein Loss: 10.5090, Content Loss: 0.2817, PSNR: 29.12 SSIM: 0.87
===> Epoch [72 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1066, D Fake Loss: -10.4927, D Real Loss: -8.8298, Wasserstein Loss: 10.5029, Content Loss: 0.2802, PSNR: 29.18 SSIM: 0.87
===> Epoch [72 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1069, D Fake Loss: -10.4613, D Real Loss: -8.7999, Wasserstein Loss: 10.4670, Content Loss: 0.2794, PSNR: 29.18 SSIM: 0.87
===> Epoch [72 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1065, D Fake Loss: -10.4242, D Real Loss: -8.7716, Wasserstein Loss: 10.4311, Content Loss: 0.2781, PSNR: 29.27 SSIM: 0.87
Epoch [72 / 100]: Gradient Penalty: 0.1061, D Fake Loss: -10.4088, D Real Loss: -8.7508, Wasserstein Loss: 10.4203, Content Loss: 0.2789, PSNR: 29.26 SSIM: 0.87
Epoch [73 / 100]
===> Epoch [73 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1170, D Fake Loss: -10.4956, D Real Loss: -8.8298, Wasserstein Loss: 10.4219, Content Loss: 0.2611, PSNR: 29.39 SSIM: 0.87
===> Epoch [73 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1099, D Fake Loss: -10.3797, D Real Loss: -8.7097, Wasserstein Loss: 10.3701, Content Loss: 0.2779, PSNR: 29.19 SSIM: 0.87
===> Epoch [73 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1114, D Fake Loss: -10.3182, D Real Loss: -8.6204, Wasserstein Loss: 10.3132, Content Loss: 0.2815, PSNR: 29.07 SSIM: 0.87
===> Epoch [73 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1081, D Fake Loss: -10.3763, D Real Loss: -8.7250, Wasserstein Loss: 10.3869, Content Loss: 0.2810, PSNR: 29.13 SSIM: 0.87
===> Epoch [73 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1089, D Fake Loss: -10.3129, D Real Loss: -8.6367, Wasserstein Loss: 10.3075, Content Loss: 0.2813, PSNR: 29.06 SSIM: 0.87
===> Epoch [73 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1084, D Fake Loss: -10.3055, D Real Loss: -8.6255, Wasserstein Loss: 10.3044, Content Loss: 0.2803, PSNR: 29.07 SSIM: 0.87
===> Epoch [73 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1096, D Fake Loss: -10.2895, D Real Loss: -8.6043, Wasserstein Loss: 10.2886, Content Loss: 0.2785, PSNR: 29.11 SSIM: 0.87
===> Epoch [73 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1093, D Fake Loss: -10.2494, D Real Loss: -8.5633, Wasserstein Loss: 10.2451, Content Loss: 0.2778, PSNR: 29.12 SSIM: 0.87
===> Epoch [73 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1081, D Fake Loss: -10.2391, D Real Loss: -8.5674, Wasserstein Loss: 10.2393, Content Loss: 0.2769, PSNR: 29.19 SSIM: 0.87
Epoch [73 / 100]: Gradient Penalty: 0.1085, D Fake Loss: -10.2095, D Real Loss: -8.5410, Wasserstein Loss: 10.2085, Content Loss: 0.2760, PSNR: 29.23 SSIM: 0.87
Epoch [74 / 100]
===> Epoch [74 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0927, D Fake Loss: -10.2082, D Real Loss: -8.6512, Wasserstein Loss: 10.2854, Content Loss: 0.2635, PSNR: 29.72 SSIM: 0.87
===> Epoch [74 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0994, D Fake Loss: -10.2328, D Real Loss: -8.6183, Wasserstein Loss: 10.2658, Content Loss: 0.2734, PSNR: 29.47 SSIM: 0.87
===> Epoch [74 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1024, D Fake Loss: -10.2952, D Real Loss: -8.6606, Wasserstein Loss: 10.3142, Content Loss: 0.2795, PSNR: 29.25 SSIM: 0.87
===> Epoch [74 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1009, D Fake Loss: -10.3572, D Real Loss: -8.7286, Wasserstein Loss: 10.3742, Content Loss: 0.2787, PSNR: 29.29 SSIM: 0.87
===> Epoch [74 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1011, D Fake Loss: -10.3201, D Real Loss: -8.6950, Wasserstein Loss: 10.3338, Content Loss: 0.2781, PSNR: 29.29 SSIM: 0.87
===> Epoch [74 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1018, D Fake Loss: -10.2743, D Real Loss: -8.6556, Wasserstein Loss: 10.2852, Content Loss: 0.2758, PSNR: 29.30 SSIM: 0.87
===> Epoch [74 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1015, D Fake Loss: -10.2568, D Real Loss: -8.6384, Wasserstein Loss: 10.2639, Content Loss: 0.2757, PSNR: 29.31 SSIM: 0.87
===> Epoch [74 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1014, D Fake Loss: -10.2081, D Real Loss: -8.5968, Wasserstein Loss: 10.2165, Content Loss: 0.2765, PSNR: 29.35 SSIM: 0.87
===> Epoch [74 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1022, D Fake Loss: -10.2219, D Real Loss: -8.6077, Wasserstein Loss: 10.2305, Content Loss: 0.2758, PSNR: 29.37 SSIM: 0.87
Epoch [74 / 100]: Gradient Penalty: 0.1024, D Fake Loss: -10.1773, D Real Loss: -8.5604, Wasserstein Loss: 10.1871, Content Loss: 0.2766, PSNR: 29.35 SSIM: 0.87
Epoch [75 / 100]
===> Epoch [75 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0863, D Fake Loss: -10.4887, D Real Loss: -8.9569, Wasserstein Loss: 10.4505, Content Loss: 0.2622, PSNR: 29.74 SSIM: 0.87
===> Epoch [75 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0987, D Fake Loss: -10.1870, D Real Loss: -8.5647, Wasserstein Loss: 10.2079, Content Loss: 0.2779, PSNR: 29.34 SSIM: 0.87
===> Epoch [75 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1048, D Fake Loss: -10.2501, D Real Loss: -8.5968, Wasserstein Loss: 10.2504, Content Loss: 0.2835, PSNR: 29.10 SSIM: 0.87
===> Epoch [75 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1049, D Fake Loss: -10.3041, D Real Loss: -8.6478, Wasserstein Loss: 10.3024, Content Loss: 0.2816, PSNR: 29.18 SSIM: 0.87
===> Epoch [75 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1038, D Fake Loss: -10.3170, D Real Loss: -8.6711, Wasserstein Loss: 10.3270, Content Loss: 0.2795, PSNR: 29.25 SSIM: 0.87
===> Epoch [75 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1044, D Fake Loss: -10.3315, D Real Loss: -8.6803, Wasserstein Loss: 10.3323, Content Loss: 0.2786, PSNR: 29.24 SSIM: 0.87
===> Epoch [75 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1052, D Fake Loss: -10.3340, D Real Loss: -8.6865, Wasserstein Loss: 10.3392, Content Loss: 0.2771, PSNR: 29.28 SSIM: 0.87
===> Epoch [75 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1050, D Fake Loss: -10.3295, D Real Loss: -8.6823, Wasserstein Loss: 10.3319, Content Loss: 0.2774, PSNR: 29.28 SSIM: 0.87
===> Epoch [75 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1041, D Fake Loss: -10.3408, D Real Loss: -8.7092, Wasserstein Loss: 10.3400, Content Loss: 0.2762, PSNR: 29.36 SSIM: 0.87
Epoch [75 / 100]: Gradient Penalty: 0.1042, D Fake Loss: -10.3317, D Real Loss: -8.6934, Wasserstein Loss: 10.3358, Content Loss: 0.2763, PSNR: 29.32 SSIM: 0.87
Epoch [76 / 100]
===> Epoch [76 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0969, D Fake Loss: -10.3712, D Real Loss: -8.8435, Wasserstein Loss: 10.3630, Content Loss: 0.2536, PSNR: 29.92 SSIM: 0.87
===> Epoch [76 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0959, D Fake Loss: -10.0833, D Real Loss: -8.5082, Wasserstein Loss: 10.0923, Content Loss: 0.2661, PSNR: 29.61 SSIM: 0.87
===> Epoch [76 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0951, D Fake Loss: -10.0823, D Real Loss: -8.5054, Wasserstein Loss: 10.1047, Content Loss: 0.2722, PSNR: 29.54 SSIM: 0.87
===> Epoch [76 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0985, D Fake Loss: -10.1805, D Real Loss: -8.5889, Wasserstein Loss: 10.1829, Content Loss: 0.2721, PSNR: 29.49 SSIM: 0.87
===> Epoch [76 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1000, D Fake Loss: -10.2147, D Real Loss: -8.6194, Wasserstein Loss: 10.2201, Content Loss: 0.2743, PSNR: 29.42 SSIM: 0.87
===> Epoch [76 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1006, D Fake Loss: -10.2388, D Real Loss: -8.6369, Wasserstein Loss: 10.2371, Content Loss: 0.2748, PSNR: 29.40 SSIM: 0.87
===> Epoch [76 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1028, D Fake Loss: -10.2449, D Real Loss: -8.6273, Wasserstein Loss: 10.2519, Content Loss: 0.2741, PSNR: 29.42 SSIM: 0.87
===> Epoch [76 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1035, D Fake Loss: -10.2470, D Real Loss: -8.6292, Wasserstein Loss: 10.2582, Content Loss: 0.2751, PSNR: 29.40 SSIM: 0.87
===> Epoch [76 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1032, D Fake Loss: -10.2936, D Real Loss: -8.6773, Wasserstein Loss: 10.3018, Content Loss: 0.2746, PSNR: 29.42 SSIM: 0.87
Epoch [76 / 100]: Gradient Penalty: 0.1037, D Fake Loss: -10.2623, D Real Loss: -8.6403, Wasserstein Loss: 10.2702, Content Loss: 0.2742, PSNR: 29.40 SSIM: 0.87
Epoch [77 / 100]
===> Epoch [77 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0973, D Fake Loss: -10.3686, D Real Loss: -8.8470, Wasserstein Loss: 10.3949, Content Loss: 0.2660, PSNR: 29.74 SSIM: 0.87
===> Epoch [77 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1045, D Fake Loss: -10.1552, D Real Loss: -8.5485, Wasserstein Loss: 10.1694, Content Loss: 0.2794, PSNR: 29.32 SSIM: 0.87
===> Epoch [77 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1019, D Fake Loss: -10.1003, D Real Loss: -8.5068, Wasserstein Loss: 10.1165, Content Loss: 0.2814, PSNR: 29.34 SSIM: 0.87
===> Epoch [77 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1020, D Fake Loss: -10.1180, D Real Loss: -8.5070, Wasserstein Loss: 10.1153, Content Loss: 0.2797, PSNR: 29.31 SSIM: 0.87
===> Epoch [77 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1003, D Fake Loss: -10.1198, D Real Loss: -8.5187, Wasserstein Loss: 10.1204, Content Loss: 0.2764, PSNR: 29.33 SSIM: 0.87
===> Epoch [77 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0994, D Fake Loss: -10.1564, D Real Loss: -8.5659, Wasserstein Loss: 10.1652, Content Loss: 0.2758, PSNR: 29.35 SSIM: 0.87
===> Epoch [77 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1001, D Fake Loss: -10.1486, D Real Loss: -8.5530, Wasserstein Loss: 10.1475, Content Loss: 0.2738, PSNR: 29.38 SSIM: 0.87
===> Epoch [77 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1003, D Fake Loss: -10.1484, D Real Loss: -8.5498, Wasserstein Loss: 10.1551, Content Loss: 0.2741, PSNR: 29.39 SSIM: 0.87
===> Epoch [77 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1010, D Fake Loss: -10.1733, D Real Loss: -8.5816, Wasserstein Loss: 10.1828, Content Loss: 0.2733, PSNR: 29.45 SSIM: 0.87
Epoch [77 / 100]: Gradient Penalty: 0.1008, D Fake Loss: -10.1647, D Real Loss: -8.5667, Wasserstein Loss: 10.1760, Content Loss: 0.2742, PSNR: 29.42 SSIM: 0.87
Epoch [78 / 100]
===> Epoch [78 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1031, D Fake Loss: -10.3405, D Real Loss: -8.8084, Wasserstein Loss: 10.3180, Content Loss: 0.2598, PSNR: 29.90 SSIM: 0.87
===> Epoch [78 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1043, D Fake Loss: -10.1327, D Real Loss: -8.5117, Wasserstein Loss: 10.1514, Content Loss: 0.2781, PSNR: 29.41 SSIM: 0.87
===> Epoch [78 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1032, D Fake Loss: -10.0226, D Real Loss: -8.4063, Wasserstein Loss: 10.0093, Content Loss: 0.2823, PSNR: 29.25 SSIM: 0.87
===> Epoch [78 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0995, D Fake Loss: -10.0533, D Real Loss: -8.4557, Wasserstein Loss: 10.0430, Content Loss: 0.2789, PSNR: 29.35 SSIM: 0.87
===> Epoch [78 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0991, D Fake Loss: -10.0302, D Real Loss: -8.4312, Wasserstein Loss: 10.0228, Content Loss: 0.2777, PSNR: 29.33 SSIM: 0.87
===> Epoch [78 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1010, D Fake Loss: -9.9792, D Real Loss: -8.3703, Wasserstein Loss: 9.9808, Content Loss: 0.2764, PSNR: 29.34 SSIM: 0.87
===> Epoch [78 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1006, D Fake Loss: -9.9908, D Real Loss: -8.3868, Wasserstein Loss: 9.9920, Content Loss: 0.2753, PSNR: 29.37 SSIM: 0.87
===> Epoch [78 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1018, D Fake Loss: -10.0066, D Real Loss: -8.3958, Wasserstein Loss: 10.0038, Content Loss: 0.2755, PSNR: 29.36 SSIM: 0.87
===> Epoch [78 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1011, D Fake Loss: -10.0175, D Real Loss: -8.4150, Wasserstein Loss: 10.0211, Content Loss: 0.2739, PSNR: 29.44 SSIM: 0.87
Epoch [78 / 100]: Gradient Penalty: 0.1007, D Fake Loss: -10.0569, D Real Loss: -8.4558, Wasserstein Loss: 10.0620, Content Loss: 0.2744, PSNR: 29.42 SSIM: 0.87
Epoch [79 / 100]
===> Epoch [79 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1079, D Fake Loss: -10.2928, D Real Loss: -8.7418, Wasserstein Loss: 10.2725, Content Loss: 0.2561, PSNR: 29.97 SSIM: 0.88
===> Epoch [79 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1077, D Fake Loss: -10.1159, D Real Loss: -8.5284, Wasserstein Loss: 10.1299, Content Loss: 0.2669, PSNR: 29.58 SSIM: 0.87
===> Epoch [79 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1030, D Fake Loss: -10.0261, D Real Loss: -8.4293, Wasserstein Loss: 10.0308, Content Loss: 0.2711, PSNR: 29.45 SSIM: 0.87
===> Epoch [79 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1025, D Fake Loss: -10.1093, D Real Loss: -8.5141, Wasserstein Loss: 10.1169, Content Loss: 0.2734, PSNR: 29.49 SSIM: 0.87
===> Epoch [79 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1026, D Fake Loss: -10.1617, D Real Loss: -8.5593, Wasserstein Loss: 10.1615, Content Loss: 0.2745, PSNR: 29.43 SSIM: 0.87
===> Epoch [79 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1021, D Fake Loss: -10.1886, D Real Loss: -8.5863, Wasserstein Loss: 10.1942, Content Loss: 0.2744, PSNR: 29.40 SSIM: 0.87
===> Epoch [79 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1029, D Fake Loss: -10.1977, D Real Loss: -8.6003, Wasserstein Loss: 10.2056, Content Loss: 0.2730, PSNR: 29.43 SSIM: 0.87
===> Epoch [79 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1026, D Fake Loss: -10.1784, D Real Loss: -8.5848, Wasserstein Loss: 10.1829, Content Loss: 0.2726, PSNR: 29.44 SSIM: 0.87
===> Epoch [79 / 100]: Batch [144 / 159]: Gradient Penalty: 0.1016, D Fake Loss: -10.1746, D Real Loss: -8.5909, Wasserstein Loss: 10.1808, Content Loss: 0.2725, PSNR: 29.50 SSIM: 0.87
Epoch [79 / 100]: Gradient Penalty: 0.1020, D Fake Loss: -10.1297, D Real Loss: -8.5412, Wasserstein Loss: 10.1350, Content Loss: 0.2726, PSNR: 29.49 SSIM: 0.87
Epoch [80 / 100]
===> Epoch [80 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1029, D Fake Loss: -9.6496, D Real Loss: -8.0573, Wasserstein Loss: 9.6670, Content Loss: 0.2581, PSNR: 29.67 SSIM: 0.88
===> Epoch [80 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1006, D Fake Loss: -9.5815, D Real Loss: -7.9872, Wasserstein Loss: 9.5977, Content Loss: 0.2751, PSNR: 29.54 SSIM: 0.87
===> Epoch [80 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1006, D Fake Loss: -9.5997, D Real Loss: -7.9932, Wasserstein Loss: 9.6117, Content Loss: 0.2789, PSNR: 29.36 SSIM: 0.87
===> Epoch [80 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0987, D Fake Loss: -9.7175, D Real Loss: -8.1291, Wasserstein Loss: 9.7303, Content Loss: 0.2757, PSNR: 29.45 SSIM: 0.87
===> Epoch [80 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0987, D Fake Loss: -9.7333, D Real Loss: -8.1475, Wasserstein Loss: 9.7381, Content Loss: 0.2743, PSNR: 29.43 SSIM: 0.87
===> Epoch [80 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0978, D Fake Loss: -9.7767, D Real Loss: -8.1993, Wasserstein Loss: 9.7868, Content Loss: 0.2733, PSNR: 29.46 SSIM: 0.87
===> Epoch [80 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0979, D Fake Loss: -9.8282, D Real Loss: -8.2631, Wasserstein Loss: 9.8407, Content Loss: 0.2738, PSNR: 29.50 SSIM: 0.87
===> Epoch [80 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0975, D Fake Loss: -9.8372, D Real Loss: -8.2710, Wasserstein Loss: 9.8462, Content Loss: 0.2739, PSNR: 29.48 SSIM: 0.87
===> Epoch [80 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0972, D Fake Loss: -9.8459, D Real Loss: -8.2868, Wasserstein Loss: 9.8561, Content Loss: 0.2730, PSNR: 29.55 SSIM: 0.87
Epoch [80 / 100]: Gradient Penalty: 0.0975, D Fake Loss: -9.8373, D Real Loss: -8.2718, Wasserstein Loss: 9.8466, Content Loss: 0.2735, PSNR: 29.53 SSIM: 0.87
Epoch [81 / 100]
===> Epoch [81 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1001, D Fake Loss: -9.8663, D Real Loss: -8.3526, Wasserstein Loss: 9.8811, Content Loss: 0.2657, PSNR: 29.83 SSIM: 0.87
===> Epoch [81 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0984, D Fake Loss: -9.7551, D Real Loss: -8.1604, Wasserstein Loss: 9.7680, Content Loss: 0.2768, PSNR: 29.53 SSIM: 0.87
===> Epoch [81 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1037, D Fake Loss: -9.7782, D Real Loss: -8.1515, Wasserstein Loss: 9.7984, Content Loss: 0.2805, PSNR: 29.32 SSIM: 0.87
===> Epoch [81 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1017, D Fake Loss: -9.9039, D Real Loss: -8.2963, Wasserstein Loss: 9.9127, Content Loss: 0.2790, PSNR: 29.41 SSIM: 0.87
===> Epoch [81 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1001, D Fake Loss: -9.8935, D Real Loss: -8.3019, Wasserstein Loss: 9.9059, Content Loss: 0.2756, PSNR: 29.45 SSIM: 0.87
===> Epoch [81 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0989, D Fake Loss: -9.8729, D Real Loss: -8.3017, Wasserstein Loss: 9.8835, Content Loss: 0.2730, PSNR: 29.51 SSIM: 0.87
===> Epoch [81 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0971, D Fake Loss: -9.8928, D Real Loss: -8.3300, Wasserstein Loss: 9.8992, Content Loss: 0.2712, PSNR: 29.53 SSIM: 0.87
===> Epoch [81 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0966, D Fake Loss: -9.9213, D Real Loss: -8.3643, Wasserstein Loss: 9.9274, Content Loss: 0.2716, PSNR: 29.53 SSIM: 0.87
===> Epoch [81 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0963, D Fake Loss: -9.9351, D Real Loss: -8.3799, Wasserstein Loss: 9.9410, Content Loss: 0.2717, PSNR: 29.56 SSIM: 0.87
Epoch [81 / 100]: Gradient Penalty: 0.0981, D Fake Loss: -9.9362, D Real Loss: -8.3659, Wasserstein Loss: 9.9425, Content Loss: 0.2718, PSNR: 29.52 SSIM: 0.87
Epoch [82 / 100]
===> Epoch [82 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1010, D Fake Loss: -10.2222, D Real Loss: -8.6274, Wasserstein Loss: 10.2496, Content Loss: 0.2756, PSNR: 29.45 SSIM: 0.87
===> Epoch [82 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1043, D Fake Loss: -10.1006, D Real Loss: -8.4659, Wasserstein Loss: 10.1275, Content Loss: 0.2815, PSNR: 29.28 SSIM: 0.87
===> Epoch [82 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1041, D Fake Loss: -10.0804, D Real Loss: -8.4477, Wasserstein Loss: 10.0817, Content Loss: 0.2804, PSNR: 29.28 SSIM: 0.87
===> Epoch [82 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1011, D Fake Loss: -10.0527, D Real Loss: -8.4564, Wasserstein Loss: 10.0583, Content Loss: 0.2769, PSNR: 29.39 SSIM: 0.87
===> Epoch [82 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1005, D Fake Loss: -10.0380, D Real Loss: -8.4433, Wasserstein Loss: 10.0428, Content Loss: 0.2758, PSNR: 29.40 SSIM: 0.87
===> Epoch [82 / 100]: Batch [96 / 159]: Gradient Penalty: 0.1010, D Fake Loss: -10.0749, D Real Loss: -8.4841, Wasserstein Loss: 10.0880, Content Loss: 0.2754, PSNR: 29.40 SSIM: 0.87
===> Epoch [82 / 100]: Batch [112 / 159]: Gradient Penalty: 0.1000, D Fake Loss: -10.0897, D Real Loss: -8.5062, Wasserstein Loss: 10.0963, Content Loss: 0.2733, PSNR: 29.46 SSIM: 0.87
===> Epoch [82 / 100]: Batch [128 / 159]: Gradient Penalty: 0.1012, D Fake Loss: -10.0759, D Real Loss: -8.4885, Wasserstein Loss: 10.0830, Content Loss: 0.2735, PSNR: 29.46 SSIM: 0.87
===> Epoch [82 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0992, D Fake Loss: -10.1245, D Real Loss: -8.5542, Wasserstein Loss: 10.1360, Content Loss: 0.2725, PSNR: 29.55 SSIM: 0.87
Epoch [82 / 100]: Gradient Penalty: 0.0991, D Fake Loss: -10.1277, D Real Loss: -8.5559, Wasserstein Loss: 10.1353, Content Loss: 0.2731, PSNR: 29.53 SSIM: 0.87
Epoch [83 / 100]
===> Epoch [83 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0930, D Fake Loss: -10.3963, D Real Loss: -8.9497, Wasserstein Loss: 10.4241, Content Loss: 0.2638, PSNR: 30.09 SSIM: 0.88
===> Epoch [83 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0924, D Fake Loss: -10.1538, D Real Loss: -8.6344, Wasserstein Loss: 10.1798, Content Loss: 0.2739, PSNR: 29.73 SSIM: 0.87
===> Epoch [83 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0947, D Fake Loss: -10.2092, D Real Loss: -8.6599, Wasserstein Loss: 10.2244, Content Loss: 0.2787, PSNR: 29.50 SSIM: 0.87
===> Epoch [83 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0946, D Fake Loss: -10.2220, D Real Loss: -8.6690, Wasserstein Loss: 10.2363, Content Loss: 0.2762, PSNR: 29.52 SSIM: 0.87
===> Epoch [83 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0955, D Fake Loss: -10.2255, D Real Loss: -8.6560, Wasserstein Loss: 10.2360, Content Loss: 0.2746, PSNR: 29.45 SSIM: 0.87
===> Epoch [83 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0960, D Fake Loss: -10.1796, D Real Loss: -8.6192, Wasserstein Loss: 10.1887, Content Loss: 0.2739, PSNR: 29.48 SSIM: 0.87
===> Epoch [83 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0960, D Fake Loss: -10.1846, D Real Loss: -8.6295, Wasserstein Loss: 10.1983, Content Loss: 0.2725, PSNR: 29.54 SSIM: 0.87
===> Epoch [83 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0972, D Fake Loss: -10.1830, D Real Loss: -8.6279, Wasserstein Loss: 10.1888, Content Loss: 0.2725, PSNR: 29.54 SSIM: 0.87
===> Epoch [83 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0980, D Fake Loss: -10.2173, D Real Loss: -8.6664, Wasserstein Loss: 10.2254, Content Loss: 0.2728, PSNR: 29.57 SSIM: 0.87
Epoch [83 / 100]: Gradient Penalty: 0.0981, D Fake Loss: -10.2060, D Real Loss: -8.6532, Wasserstein Loss: 10.2182, Content Loss: 0.2738, PSNR: 29.57 SSIM: 0.87
Epoch [84 / 100]
===> Epoch [84 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1031, D Fake Loss: -10.4954, D Real Loss: -9.0220, Wasserstein Loss: 10.5315, Content Loss: 0.2589, PSNR: 29.94 SSIM: 0.88
===> Epoch [84 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1005, D Fake Loss: -10.3420, D Real Loss: -8.7880, Wasserstein Loss: 10.3705, Content Loss: 0.2716, PSNR: 29.57 SSIM: 0.87
===> Epoch [84 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1012, D Fake Loss: -10.2866, D Real Loss: -8.7012, Wasserstein Loss: 10.2802, Content Loss: 0.2755, PSNR: 29.43 SSIM: 0.87
===> Epoch [84 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1006, D Fake Loss: -10.3206, D Real Loss: -8.7510, Wasserstein Loss: 10.3315, Content Loss: 0.2737, PSNR: 29.43 SSIM: 0.87
===> Epoch [84 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0995, D Fake Loss: -10.3108, D Real Loss: -8.7482, Wasserstein Loss: 10.3168, Content Loss: 0.2734, PSNR: 29.43 SSIM: 0.87
===> Epoch [84 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0984, D Fake Loss: -10.2470, D Real Loss: -8.7025, Wasserstein Loss: 10.2518, Content Loss: 0.2714, PSNR: 29.51 SSIM: 0.87
===> Epoch [84 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -10.2316, D Real Loss: -8.6928, Wasserstein Loss: 10.2374, Content Loss: 0.2715, PSNR: 29.57 SSIM: 0.87
===> Epoch [84 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0969, D Fake Loss: -10.2154, D Real Loss: -8.6725, Wasserstein Loss: 10.2276, Content Loss: 0.2735, PSNR: 29.52 SSIM: 0.87
===> Epoch [84 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0977, D Fake Loss: -10.2033, D Real Loss: -8.6567, Wasserstein Loss: 10.2109, Content Loss: 0.2725, PSNR: 29.57 SSIM: 0.87
Epoch [84 / 100]: Gradient Penalty: 0.0976, D Fake Loss: -10.1582, D Real Loss: -8.6067, Wasserstein Loss: 10.1646, Content Loss: 0.2726, PSNR: 29.56 SSIM: 0.87
Epoch [85 / 100]
===> Epoch [85 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0962, D Fake Loss: -10.2382, D Real Loss: -8.7723, Wasserstein Loss: 10.2309, Content Loss: 0.2577, PSNR: 30.20 SSIM: 0.88
===> Epoch [85 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0956, D Fake Loss: -9.9433, D Real Loss: -8.4394, Wasserstein Loss: 9.9698, Content Loss: 0.2689, PSNR: 29.80 SSIM: 0.87
===> Epoch [85 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0999, D Fake Loss: -9.8817, D Real Loss: -8.3002, Wasserstein Loss: 9.9171, Content Loss: 0.2727, PSNR: 29.57 SSIM: 0.87
===> Epoch [85 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0966, D Fake Loss: -9.9437, D Real Loss: -8.3982, Wasserstein Loss: 9.9545, Content Loss: 0.2733, PSNR: 29.63 SSIM: 0.87
===> Epoch [85 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -9.9404, D Real Loss: -8.3851, Wasserstein Loss: 9.9385, Content Loss: 0.2729, PSNR: 29.55 SSIM: 0.87
===> Epoch [85 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0958, D Fake Loss: -9.9228, D Real Loss: -8.3803, Wasserstein Loss: 9.9366, Content Loss: 0.2726, PSNR: 29.59 SSIM: 0.87
===> Epoch [85 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0958, D Fake Loss: -9.9096, D Real Loss: -8.3632, Wasserstein Loss: 9.9148, Content Loss: 0.2715, PSNR: 29.62 SSIM: 0.87
===> Epoch [85 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0949, D Fake Loss: -9.8896, D Real Loss: -8.3505, Wasserstein Loss: 9.8960, Content Loss: 0.2713, PSNR: 29.62 SSIM: 0.87
===> Epoch [85 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0951, D Fake Loss: -9.9036, D Real Loss: -8.3726, Wasserstein Loss: 9.9098, Content Loss: 0.2704, PSNR: 29.67 SSIM: 0.87
Epoch [85 / 100]: Gradient Penalty: 0.0951, D Fake Loss: -9.8612, D Real Loss: -8.3326, Wasserstein Loss: 9.8703, Content Loss: 0.2702, PSNR: 29.69 SSIM: 0.87
Epoch [86 / 100]
===> Epoch [86 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0892, D Fake Loss: -9.2367, D Real Loss: -7.8014, Wasserstein Loss: 9.2218, Content Loss: 0.2507, PSNR: 30.34 SSIM: 0.88
===> Epoch [86 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0926, D Fake Loss: -9.2322, D Real Loss: -7.7274, Wasserstein Loss: 9.2450, Content Loss: 0.2686, PSNR: 29.77 SSIM: 0.88
===> Epoch [86 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0962, D Fake Loss: -9.2138, D Real Loss: -7.6915, Wasserstein Loss: 9.2097, Content Loss: 0.2702, PSNR: 29.62 SSIM: 0.88
===> Epoch [86 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0954, D Fake Loss: -9.2371, D Real Loss: -7.7258, Wasserstein Loss: 9.2465, Content Loss: 0.2696, PSNR: 29.68 SSIM: 0.87
===> Epoch [86 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0954, D Fake Loss: -9.2636, D Real Loss: -7.7439, Wasserstein Loss: 9.2626, Content Loss: 0.2694, PSNR: 29.62 SSIM: 0.87
===> Epoch [86 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0954, D Fake Loss: -9.2613, D Real Loss: -7.7408, Wasserstein Loss: 9.2667, Content Loss: 0.2698, PSNR: 29.61 SSIM: 0.87
===> Epoch [86 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0952, D Fake Loss: -9.3460, D Real Loss: -7.8321, Wasserstein Loss: 9.3511, Content Loss: 0.2697, PSNR: 29.66 SSIM: 0.87
===> Epoch [86 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0956, D Fake Loss: -9.3464, D Real Loss: -7.8236, Wasserstein Loss: 9.3520, Content Loss: 0.2698, PSNR: 29.66 SSIM: 0.88
===> Epoch [86 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0970, D Fake Loss: -9.3805, D Real Loss: -7.8557, Wasserstein Loss: 9.3841, Content Loss: 0.2698, PSNR: 29.69 SSIM: 0.87
Epoch [86 / 100]: Gradient Penalty: 0.0976, D Fake Loss: -9.3625, D Real Loss: -7.8316, Wasserstein Loss: 9.3725, Content Loss: 0.2702, PSNR: 29.67 SSIM: 0.87
Epoch [87 / 100]
===> Epoch [87 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0929, D Fake Loss: -9.5141, D Real Loss: -8.0168, Wasserstein Loss: 9.4887, Content Loss: 0.2549, PSNR: 29.92 SSIM: 0.88
===> Epoch [87 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0980, D Fake Loss: -9.1919, D Real Loss: -7.6401, Wasserstein Loss: 9.1995, Content Loss: 0.2696, PSNR: 29.64 SSIM: 0.87
===> Epoch [87 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0981, D Fake Loss: -9.1899, D Real Loss: -7.6361, Wasserstein Loss: 9.2039, Content Loss: 0.2747, PSNR: 29.53 SSIM: 0.87
===> Epoch [87 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0990, D Fake Loss: -9.2738, D Real Loss: -7.7056, Wasserstein Loss: 9.2720, Content Loss: 0.2736, PSNR: 29.53 SSIM: 0.87
===> Epoch [87 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0981, D Fake Loss: -9.3082, D Real Loss: -7.7494, Wasserstein Loss: 9.3085, Content Loss: 0.2722, PSNR: 29.52 SSIM: 0.87
===> Epoch [87 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0976, D Fake Loss: -9.2993, D Real Loss: -7.7476, Wasserstein Loss: 9.3038, Content Loss: 0.2710, PSNR: 29.57 SSIM: 0.88
===> Epoch [87 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0963, D Fake Loss: -9.3237, D Real Loss: -7.7852, Wasserstein Loss: 9.3303, Content Loss: 0.2699, PSNR: 29.64 SSIM: 0.88
===> Epoch [87 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0964, D Fake Loss: -9.3074, D Real Loss: -7.7712, Wasserstein Loss: 9.3175, Content Loss: 0.2703, PSNR: 29.65 SSIM: 0.88
===> Epoch [87 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0965, D Fake Loss: -9.3353, D Real Loss: -7.8083, Wasserstein Loss: 9.3420, Content Loss: 0.2698, PSNR: 29.71 SSIM: 0.87
Epoch [87 / 100]: Gradient Penalty: 0.0957, D Fake Loss: -9.3274, D Real Loss: -7.7971, Wasserstein Loss: 9.3358, Content Loss: 0.2706, PSNR: 29.68 SSIM: 0.87
Epoch [88 / 100]
===> Epoch [88 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1054, D Fake Loss: -9.2688, D Real Loss: -7.7239, Wasserstein Loss: 9.2409, Content Loss: 0.2597, PSNR: 30.03 SSIM: 0.88
===> Epoch [88 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1003, D Fake Loss: -9.1085, D Real Loss: -7.5657, Wasserstein Loss: 9.1261, Content Loss: 0.2703, PSNR: 29.73 SSIM: 0.87
===> Epoch [88 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0986, D Fake Loss: -9.1363, D Real Loss: -7.6056, Wasserstein Loss: 9.1472, Content Loss: 0.2726, PSNR: 29.64 SSIM: 0.87
===> Epoch [88 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0995, D Fake Loss: -9.1952, D Real Loss: -7.6674, Wasserstein Loss: 9.1956, Content Loss: 0.2719, PSNR: 29.67 SSIM: 0.88
===> Epoch [88 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0980, D Fake Loss: -9.1657, D Real Loss: -7.6353, Wasserstein Loss: 9.1681, Content Loss: 0.2733, PSNR: 29.58 SSIM: 0.87
===> Epoch [88 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0975, D Fake Loss: -9.1595, D Real Loss: -7.6294, Wasserstein Loss: 9.1632, Content Loss: 0.2718, PSNR: 29.59 SSIM: 0.87
===> Epoch [88 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -9.1522, D Real Loss: -7.6257, Wasserstein Loss: 9.1533, Content Loss: 0.2703, PSNR: 29.62 SSIM: 0.87
===> Epoch [88 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0970, D Fake Loss: -9.1154, D Real Loss: -7.5918, Wasserstein Loss: 9.1217, Content Loss: 0.2706, PSNR: 29.64 SSIM: 0.88
===> Epoch [88 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0963, D Fake Loss: -9.1592, D Real Loss: -7.6421, Wasserstein Loss: 9.1654, Content Loss: 0.2700, PSNR: 29.68 SSIM: 0.87
Epoch [88 / 100]: Gradient Penalty: 0.0962, D Fake Loss: -9.1488, D Real Loss: -7.6228, Wasserstein Loss: 9.1566, Content Loss: 0.2702, PSNR: 29.66 SSIM: 0.87
Epoch [89 / 100]
===> Epoch [89 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0970, D Fake Loss: -9.3330, D Real Loss: -7.9130, Wasserstein Loss: 9.3321, Content Loss: 0.2595, PSNR: 30.18 SSIM: 0.87
===> Epoch [89 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1007, D Fake Loss: -9.1229, D Real Loss: -7.6017, Wasserstein Loss: 9.1275, Content Loss: 0.2730, PSNR: 29.79 SSIM: 0.87
===> Epoch [89 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0974, D Fake Loss: -9.1356, D Real Loss: -7.6218, Wasserstein Loss: 9.1527, Content Loss: 0.2764, PSNR: 29.64 SSIM: 0.87
===> Epoch [89 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0964, D Fake Loss: -9.1922, D Real Loss: -7.6735, Wasserstein Loss: 9.2047, Content Loss: 0.2735, PSNR: 29.66 SSIM: 0.87
===> Epoch [89 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0983, D Fake Loss: -9.1746, D Real Loss: -7.6417, Wasserstein Loss: 9.1845, Content Loss: 0.2727, PSNR: 29.65 SSIM: 0.87
===> Epoch [89 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0972, D Fake Loss: -9.1817, D Real Loss: -7.6585, Wasserstein Loss: 9.1876, Content Loss: 0.2723, PSNR: 29.66 SSIM: 0.87
===> Epoch [89 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0970, D Fake Loss: -9.1963, D Real Loss: -7.6746, Wasserstein Loss: 9.2039, Content Loss: 0.2709, PSNR: 29.69 SSIM: 0.87
===> Epoch [89 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0963, D Fake Loss: -9.1901, D Real Loss: -7.6726, Wasserstein Loss: 9.1946, Content Loss: 0.2701, PSNR: 29.69 SSIM: 0.87
===> Epoch [89 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0960, D Fake Loss: -9.2226, D Real Loss: -7.7082, Wasserstein Loss: 9.2288, Content Loss: 0.2695, PSNR: 29.75 SSIM: 0.87
Epoch [89 / 100]: Gradient Penalty: 0.0961, D Fake Loss: -9.2226, D Real Loss: -7.7091, Wasserstein Loss: 9.2285, Content Loss: 0.2706, PSNR: 29.73 SSIM: 0.87
Epoch [90 / 100]
===> Epoch [90 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0937, D Fake Loss: -9.2693, D Real Loss: -7.8433, Wasserstein Loss: 9.2880, Content Loss: 0.2618, PSNR: 30.10 SSIM: 0.88
===> Epoch [90 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0947, D Fake Loss: -9.1314, D Real Loss: -7.6371, Wasserstein Loss: 9.1440, Content Loss: 0.2712, PSNR: 29.79 SSIM: 0.87
===> Epoch [90 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0935, D Fake Loss: -9.2269, D Real Loss: -7.7314, Wasserstein Loss: 9.2379, Content Loss: 0.2751, PSNR: 29.66 SSIM: 0.87
===> Epoch [90 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0934, D Fake Loss: -9.2662, D Real Loss: -7.7691, Wasserstein Loss: 9.2683, Content Loss: 0.2722, PSNR: 29.67 SSIM: 0.87
===> Epoch [90 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0938, D Fake Loss: -9.2515, D Real Loss: -7.7475, Wasserstein Loss: 9.2467, Content Loss: 0.2713, PSNR: 29.63 SSIM: 0.87
===> Epoch [90 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0944, D Fake Loss: -9.2432, D Real Loss: -7.7392, Wasserstein Loss: 9.2479, Content Loss: 0.2700, PSNR: 29.66 SSIM: 0.87
===> Epoch [90 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0939, D Fake Loss: -9.2489, D Real Loss: -7.7535, Wasserstein Loss: 9.2488, Content Loss: 0.2694, PSNR: 29.71 SSIM: 0.87
===> Epoch [90 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0945, D Fake Loss: -9.2401, D Real Loss: -7.7348, Wasserstein Loss: 9.2397, Content Loss: 0.2703, PSNR: 29.69 SSIM: 0.87
===> Epoch [90 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0948, D Fake Loss: -9.2330, D Real Loss: -7.7308, Wasserstein Loss: 9.2353, Content Loss: 0.2697, PSNR: 29.73 SSIM: 0.87
Epoch [90 / 100]: Gradient Penalty: 0.0954, D Fake Loss: -9.1927, D Real Loss: -7.6803, Wasserstein Loss: 9.1982, Content Loss: 0.2699, PSNR: 29.70 SSIM: 0.87
Epoch [91 / 100]
===> Epoch [91 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0977, D Fake Loss: -9.2589, D Real Loss: -7.8075, Wasserstein Loss: 9.2931, Content Loss: 0.2594, PSNR: 30.01 SSIM: 0.88
===> Epoch [91 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0983, D Fake Loss: -9.2058, D Real Loss: -7.6740, Wasserstein Loss: 9.2388, Content Loss: 0.2717, PSNR: 29.58 SSIM: 0.88
===> Epoch [91 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0980, D Fake Loss: -9.2144, D Real Loss: -7.6589, Wasserstein Loss: 9.2223, Content Loss: 0.2760, PSNR: 29.45 SSIM: 0.88
===> Epoch [91 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0989, D Fake Loss: -9.2411, D Real Loss: -7.6856, Wasserstein Loss: 9.2465, Content Loss: 0.2734, PSNR: 29.50 SSIM: 0.87
===> Epoch [91 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0982, D Fake Loss: -9.2493, D Real Loss: -7.6975, Wasserstein Loss: 9.2525, Content Loss: 0.2712, PSNR: 29.52 SSIM: 0.88
===> Epoch [91 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0978, D Fake Loss: -9.2448, D Real Loss: -7.7044, Wasserstein Loss: 9.2561, Content Loss: 0.2704, PSNR: 29.58 SSIM: 0.88
===> Epoch [91 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0977, D Fake Loss: -9.2755, D Real Loss: -7.7390, Wasserstein Loss: 9.2832, Content Loss: 0.2690, PSNR: 29.59 SSIM: 0.88
===> Epoch [91 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0977, D Fake Loss: -9.2707, D Real Loss: -7.7388, Wasserstein Loss: 9.2792, Content Loss: 0.2690, PSNR: 29.62 SSIM: 0.88
===> Epoch [91 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0973, D Fake Loss: -9.2754, D Real Loss: -7.7457, Wasserstein Loss: 9.2797, Content Loss: 0.2692, PSNR: 29.69 SSIM: 0.87
Epoch [91 / 100]: Gradient Penalty: 0.0972, D Fake Loss: -9.2519, D Real Loss: -7.7299, Wasserstein Loss: 9.2596, Content Loss: 0.2697, PSNR: 29.69 SSIM: 0.87
Epoch [92 / 100]
===> Epoch [92 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0804, D Fake Loss: -9.3969, D Real Loss: -8.0929, Wasserstein Loss: 9.3939, Content Loss: 0.2561, PSNR: 30.31 SSIM: 0.87
===> Epoch [92 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0902, D Fake Loss: -9.1675, D Real Loss: -7.7102, Wasserstein Loss: 9.1748, Content Loss: 0.2695, PSNR: 29.88 SSIM: 0.87
===> Epoch [92 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0924, D Fake Loss: -9.1560, D Real Loss: -7.6653, Wasserstein Loss: 9.1797, Content Loss: 0.2743, PSNR: 29.72 SSIM: 0.87
===> Epoch [92 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0931, D Fake Loss: -9.3045, D Real Loss: -7.8012, Wasserstein Loss: 9.3186, Content Loss: 0.2760, PSNR: 29.65 SSIM: 0.87
===> Epoch [92 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0947, D Fake Loss: -9.3053, D Real Loss: -7.7981, Wasserstein Loss: 9.3205, Content Loss: 0.2750, PSNR: 29.63 SSIM: 0.87
===> Epoch [92 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0940, D Fake Loss: -9.3175, D Real Loss: -7.8134, Wasserstein Loss: 9.3292, Content Loss: 0.2718, PSNR: 29.67 SSIM: 0.87
===> Epoch [92 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0951, D Fake Loss: -9.3305, D Real Loss: -7.8203, Wasserstein Loss: 9.3417, Content Loss: 0.2699, PSNR: 29.70 SSIM: 0.88
===> Epoch [92 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0946, D Fake Loss: -9.3435, D Real Loss: -7.8354, Wasserstein Loss: 9.3561, Content Loss: 0.2694, PSNR: 29.71 SSIM: 0.88
===> Epoch [92 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0944, D Fake Loss: -9.3958, D Real Loss: -7.8949, Wasserstein Loss: 9.4073, Content Loss: 0.2692, PSNR: 29.76 SSIM: 0.87
Epoch [92 / 100]: Gradient Penalty: 0.0952, D Fake Loss: -9.3905, D Real Loss: -7.8865, Wasserstein Loss: 9.4016, Content Loss: 0.2697, PSNR: 29.74 SSIM: 0.88
Epoch [93 / 100]
===> Epoch [93 / 100]: Batch [16 / 159]: Gradient Penalty: 0.1017, D Fake Loss: -9.6257, D Real Loss: -8.0673, Wasserstein Loss: 9.6143, Content Loss: 0.2623, PSNR: 29.82 SSIM: 0.87
===> Epoch [93 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1016, D Fake Loss: -9.5711, D Real Loss: -8.0182, Wasserstein Loss: 9.5944, Content Loss: 0.2736, PSNR: 29.55 SSIM: 0.87
===> Epoch [93 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1010, D Fake Loss: -9.5011, D Real Loss: -7.9548, Wasserstein Loss: 9.5220, Content Loss: 0.2769, PSNR: 29.49 SSIM: 0.87
===> Epoch [93 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0982, D Fake Loss: -9.5235, D Real Loss: -7.9786, Wasserstein Loss: 9.5250, Content Loss: 0.2729, PSNR: 29.58 SSIM: 0.87
===> Epoch [93 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0989, D Fake Loss: -9.4841, D Real Loss: -7.9420, Wasserstein Loss: 9.4892, Content Loss: 0.2725, PSNR: 29.58 SSIM: 0.87
===> Epoch [93 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0977, D Fake Loss: -9.4564, D Real Loss: -7.9321, Wasserstein Loss: 9.4660, Content Loss: 0.2707, PSNR: 29.61 SSIM: 0.87
===> Epoch [93 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0980, D Fake Loss: -9.4396, D Real Loss: -7.9106, Wasserstein Loss: 9.4389, Content Loss: 0.2696, PSNR: 29.62 SSIM: 0.88
===> Epoch [93 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0979, D Fake Loss: -9.3676, D Real Loss: -7.8414, Wasserstein Loss: 9.3763, Content Loss: 0.2696, PSNR: 29.64 SSIM: 0.88
===> Epoch [93 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0969, D Fake Loss: -9.3761, D Real Loss: -7.8665, Wasserstein Loss: 9.3830, Content Loss: 0.2684, PSNR: 29.73 SSIM: 0.88
Epoch [93 / 100]: Gradient Penalty: 0.0971, D Fake Loss: -9.3492, D Real Loss: -7.8393, Wasserstein Loss: 9.3548, Content Loss: 0.2696, PSNR: 29.72 SSIM: 0.88
Epoch [94 / 100]
===> Epoch [94 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0936, D Fake Loss: -9.4579, D Real Loss: -7.9541, Wasserstein Loss: 9.4860, Content Loss: 0.2573, PSNR: 30.04 SSIM: 0.88
===> Epoch [94 / 100]: Batch [32 / 159]: Gradient Penalty: 0.1018, D Fake Loss: -9.2260, D Real Loss: -7.6550, Wasserstein Loss: 9.2443, Content Loss: 0.2699, PSNR: 29.55 SSIM: 0.88
===> Epoch [94 / 100]: Batch [48 / 159]: Gradient Penalty: 0.1009, D Fake Loss: -9.3071, D Real Loss: -7.7403, Wasserstein Loss: 9.3249, Content Loss: 0.2749, PSNR: 29.44 SSIM: 0.88
===> Epoch [94 / 100]: Batch [64 / 159]: Gradient Penalty: 0.1021, D Fake Loss: -9.3637, D Real Loss: -7.7977, Wasserstein Loss: 9.3708, Content Loss: 0.2727, PSNR: 29.49 SSIM: 0.88
===> Epoch [94 / 100]: Batch [80 / 159]: Gradient Penalty: 0.1004, D Fake Loss: -9.3656, D Real Loss: -7.8235, Wasserstein Loss: 9.3728, Content Loss: 0.2733, PSNR: 29.56 SSIM: 0.87
===> Epoch [94 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0994, D Fake Loss: -9.3527, D Real Loss: -7.8193, Wasserstein Loss: 9.3576, Content Loss: 0.2713, PSNR: 29.61 SSIM: 0.88
===> Epoch [94 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0996, D Fake Loss: -9.3515, D Real Loss: -7.8106, Wasserstein Loss: 9.3572, Content Loss: 0.2703, PSNR: 29.65 SSIM: 0.88
===> Epoch [94 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0992, D Fake Loss: -9.3298, D Real Loss: -7.7955, Wasserstein Loss: 9.3371, Content Loss: 0.2703, PSNR: 29.67 SSIM: 0.88
===> Epoch [94 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0981, D Fake Loss: -9.3758, D Real Loss: -7.8536, Wasserstein Loss: 9.3827, Content Loss: 0.2686, PSNR: 29.76 SSIM: 0.88
Epoch [94 / 100]: Gradient Penalty: 0.0989, D Fake Loss: -9.3625, D Real Loss: -7.8448, Wasserstein Loss: 9.3725, Content Loss: 0.2694, PSNR: 29.76 SSIM: 0.88
Epoch [95 / 100]
===> Epoch [95 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0911, D Fake Loss: -9.4528, D Real Loss: -8.0812, Wasserstein Loss: 9.4462, Content Loss: 0.2501, PSNR: 30.32 SSIM: 0.88
===> Epoch [95 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0931, D Fake Loss: -9.2517, D Real Loss: -7.7651, Wasserstein Loss: 9.2489, Content Loss: 0.2686, PSNR: 29.81 SSIM: 0.88
===> Epoch [95 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0947, D Fake Loss: -9.2642, D Real Loss: -7.7548, Wasserstein Loss: 9.2735, Content Loss: 0.2717, PSNR: 29.66 SSIM: 0.88
===> Epoch [95 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0921, D Fake Loss: -9.3901, D Real Loss: -7.9187, Wasserstein Loss: 9.4020, Content Loss: 0.2689, PSNR: 29.80 SSIM: 0.88
===> Epoch [95 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0929, D Fake Loss: -9.4659, D Real Loss: -7.9900, Wasserstein Loss: 9.4672, Content Loss: 0.2687, PSNR: 29.76 SSIM: 0.88
===> Epoch [95 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0943, D Fake Loss: -9.4640, D Real Loss: -7.9809, Wasserstein Loss: 9.4723, Content Loss: 0.2687, PSNR: 29.76 SSIM: 0.88
===> Epoch [95 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0945, D Fake Loss: -9.5194, D Real Loss: -8.0365, Wasserstein Loss: 9.5300, Content Loss: 0.2684, PSNR: 29.78 SSIM: 0.88
===> Epoch [95 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0954, D Fake Loss: -9.5446, D Real Loss: -8.0560, Wasserstein Loss: 9.5483, Content Loss: 0.2678, PSNR: 29.76 SSIM: 0.88
===> Epoch [95 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0949, D Fake Loss: -9.5971, D Real Loss: -8.1164, Wasserstein Loss: 9.5974, Content Loss: 0.2668, PSNR: 29.82 SSIM: 0.88
Epoch [95 / 100]: Gradient Penalty: 0.0959, D Fake Loss: -9.5969, D Real Loss: -8.1094, Wasserstein Loss: 9.6000, Content Loss: 0.2673, PSNR: 29.80 SSIM: 0.88
Epoch [96 / 100]
===> Epoch [96 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0945, D Fake Loss: -9.7111, D Real Loss: -8.2908, Wasserstein Loss: 9.7362, Content Loss: 0.2544, PSNR: 30.27 SSIM: 0.88
===> Epoch [96 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0998, D Fake Loss: -9.5475, D Real Loss: -8.0310, Wasserstein Loss: 9.5707, Content Loss: 0.2712, PSNR: 29.82 SSIM: 0.87
===> Epoch [96 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0997, D Fake Loss: -9.5357, D Real Loss: -8.0189, Wasserstein Loss: 9.5538, Content Loss: 0.2740, PSNR: 29.66 SSIM: 0.87
===> Epoch [96 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0969, D Fake Loss: -9.6116, D Real Loss: -8.1288, Wasserstein Loss: 9.6198, Content Loss: 0.2714, PSNR: 29.78 SSIM: 0.87
===> Epoch [96 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -9.6299, D Real Loss: -8.1435, Wasserstein Loss: 9.6392, Content Loss: 0.2710, PSNR: 29.75 SSIM: 0.87
===> Epoch [96 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -9.6327, D Real Loss: -8.1476, Wasserstein Loss: 9.6430, Content Loss: 0.2701, PSNR: 29.79 SSIM: 0.88
===> Epoch [96 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0970, D Fake Loss: -9.6610, D Real Loss: -8.1689, Wasserstein Loss: 9.6709, Content Loss: 0.2692, PSNR: 29.79 SSIM: 0.88
===> Epoch [96 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0974, D Fake Loss: -9.6280, D Real Loss: -8.1371, Wasserstein Loss: 9.6393, Content Loss: 0.2685, PSNR: 29.81 SSIM: 0.88
===> Epoch [96 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0967, D Fake Loss: -9.6225, D Real Loss: -8.1418, Wasserstein Loss: 9.6308, Content Loss: 0.2674, PSNR: 29.88 SSIM: 0.88
Epoch [96 / 100]: Gradient Penalty: 0.0967, D Fake Loss: -9.6147, D Real Loss: -8.1308, Wasserstein Loss: 9.6245, Content Loss: 0.2676, PSNR: 29.87 SSIM: 0.88
Epoch [97 / 100]
===> Epoch [97 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0899, D Fake Loss: -9.6079, D Real Loss: -8.1772, Wasserstein Loss: 9.6013, Content Loss: 0.2595, PSNR: 30.23 SSIM: 0.88
===> Epoch [97 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0951, D Fake Loss: -9.4170, D Real Loss: -7.9263, Wasserstein Loss: 9.4134, Content Loss: 0.2703, PSNR: 29.93 SSIM: 0.88
===> Epoch [97 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0950, D Fake Loss: -9.3678, D Real Loss: -7.8626, Wasserstein Loss: 9.3693, Content Loss: 0.2728, PSNR: 29.82 SSIM: 0.88
===> Epoch [97 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0949, D Fake Loss: -9.4480, D Real Loss: -7.9467, Wasserstein Loss: 9.4410, Content Loss: 0.2709, PSNR: 29.82 SSIM: 0.88
===> Epoch [97 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0947, D Fake Loss: -9.4330, D Real Loss: -7.9516, Wasserstein Loss: 9.4331, Content Loss: 0.2686, PSNR: 29.81 SSIM: 0.88
===> Epoch [97 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0951, D Fake Loss: -9.4531, D Real Loss: -7.9699, Wasserstein Loss: 9.4478, Content Loss: 0.2678, PSNR: 29.80 SSIM: 0.88
===> Epoch [97 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0948, D Fake Loss: -9.4213, D Real Loss: -7.9488, Wasserstein Loss: 9.4210, Content Loss: 0.2672, PSNR: 29.84 SSIM: 0.88
===> Epoch [97 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0953, D Fake Loss: -9.3733, D Real Loss: -7.8964, Wasserstein Loss: 9.3715, Content Loss: 0.2674, PSNR: 29.82 SSIM: 0.88
===> Epoch [97 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0949, D Fake Loss: -9.3768, D Real Loss: -7.9065, Wasserstein Loss: 9.3795, Content Loss: 0.2668, PSNR: 29.87 SSIM: 0.88
Epoch [97 / 100]: Gradient Penalty: 0.0953, D Fake Loss: -9.3493, D Real Loss: -7.8735, Wasserstein Loss: 9.3496, Content Loss: 0.2670, PSNR: 29.86 SSIM: 0.88
Epoch [98 / 100]
===> Epoch [98 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0960, D Fake Loss: -9.2892, D Real Loss: -7.8575, Wasserstein Loss: 9.3094, Content Loss: 0.2527, PSNR: 30.29 SSIM: 0.88
===> Epoch [98 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0994, D Fake Loss: -9.1202, D Real Loss: -7.6329, Wasserstein Loss: 9.1424, Content Loss: 0.2683, PSNR: 29.93 SSIM: 0.87
===> Epoch [98 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0989, D Fake Loss: -9.1382, D Real Loss: -7.6439, Wasserstein Loss: 9.1537, Content Loss: 0.2709, PSNR: 29.78 SSIM: 0.87
===> Epoch [98 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0984, D Fake Loss: -9.1969, D Real Loss: -7.7010, Wasserstein Loss: 9.2060, Content Loss: 0.2687, PSNR: 29.80 SSIM: 0.88
===> Epoch [98 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0998, D Fake Loss: -9.1508, D Real Loss: -7.6587, Wasserstein Loss: 9.1575, Content Loss: 0.2698, PSNR: 29.75 SSIM: 0.88
===> Epoch [98 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0986, D Fake Loss: -9.1459, D Real Loss: -7.6709, Wasserstein Loss: 9.1571, Content Loss: 0.2691, PSNR: 29.77 SSIM: 0.88
===> Epoch [98 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0971, D Fake Loss: -9.1645, D Real Loss: -7.7009, Wasserstein Loss: 9.1713, Content Loss: 0.2675, PSNR: 29.83 SSIM: 0.88
===> Epoch [98 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0965, D Fake Loss: -9.1457, D Real Loss: -7.6788, Wasserstein Loss: 9.1519, Content Loss: 0.2680, PSNR: 29.82 SSIM: 0.88
===> Epoch [98 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0957, D Fake Loss: -9.1677, D Real Loss: -7.7032, Wasserstein Loss: 9.1755, Content Loss: 0.2674, PSNR: 29.88 SSIM: 0.88
Epoch [98 / 100]: Gradient Penalty: 0.0954, D Fake Loss: -9.1619, D Real Loss: -7.7038, Wasserstein Loss: 9.1694, Content Loss: 0.2673, PSNR: 29.89 SSIM: 0.88
Epoch [99 / 100]
===> Epoch [99 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0927, D Fake Loss: -9.3490, D Real Loss: -7.9803, Wasserstein Loss: 9.3704, Content Loss: 0.2516, PSNR: 30.41 SSIM: 0.88
===> Epoch [99 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0969, D Fake Loss: -9.0312, D Real Loss: -7.5735, Wasserstein Loss: 9.0378, Content Loss: 0.2659, PSNR: 29.91 SSIM: 0.88
===> Epoch [99 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0948, D Fake Loss: -8.9082, D Real Loss: -7.4417, Wasserstein Loss: 8.9081, Content Loss: 0.2704, PSNR: 29.82 SSIM: 0.88
===> Epoch [99 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0943, D Fake Loss: -8.8945, D Real Loss: -7.4410, Wasserstein Loss: 8.9008, Content Loss: 0.2659, PSNR: 29.92 SSIM: 0.88
===> Epoch [99 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0931, D Fake Loss: -8.9857, D Real Loss: -7.5305, Wasserstein Loss: 8.9892, Content Loss: 0.2684, PSNR: 29.86 SSIM: 0.88
===> Epoch [99 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0948, D Fake Loss: -8.9910, D Real Loss: -7.5331, Wasserstein Loss: 8.9947, Content Loss: 0.2682, PSNR: 29.85 SSIM: 0.88
===> Epoch [99 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0945, D Fake Loss: -9.0015, D Real Loss: -7.5382, Wasserstein Loss: 9.0026, Content Loss: 0.2675, PSNR: 29.83 SSIM: 0.88
===> Epoch [99 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0957, D Fake Loss: -8.9712, D Real Loss: -7.5012, Wasserstein Loss: 8.9735, Content Loss: 0.2679, PSNR: 29.83 SSIM: 0.88
===> Epoch [99 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0966, D Fake Loss: -8.9908, D Real Loss: -7.5247, Wasserstein Loss: 8.9935, Content Loss: 0.2667, PSNR: 29.89 SSIM: 0.88
Epoch [99 / 100]: Gradient Penalty: 0.0974, D Fake Loss: -8.9485, D Real Loss: -7.4743, Wasserstein Loss: 8.9526, Content Loss: 0.2659, PSNR: 29.85 SSIM: 0.88
Epoch [100 / 100]
===> Epoch [100 / 100]: Batch [16 / 159]: Gradient Penalty: 0.0900, D Fake Loss: -8.8903, D Real Loss: -7.4848, Wasserstein Loss: 8.8712, Content Loss: 0.2539, PSNR: 30.14 SSIM: 0.87
===> Epoch [100 / 100]: Batch [32 / 159]: Gradient Penalty: 0.0946, D Fake Loss: -8.6688, D Real Loss: -7.1891, Wasserstein Loss: 8.6732, Content Loss: 0.2664, PSNR: 29.76 SSIM: 0.87
===> Epoch [100 / 100]: Batch [48 / 159]: Gradient Penalty: 0.0955, D Fake Loss: -8.5912, D Real Loss: -7.0853, Wasserstein Loss: 8.5984, Content Loss: 0.2691, PSNR: 29.67 SSIM: 0.87
===> Epoch [100 / 100]: Batch [64 / 159]: Gradient Penalty: 0.0942, D Fake Loss: -8.7362, D Real Loss: -7.2439, Wasserstein Loss: 8.7322, Content Loss: 0.2690, PSNR: 29.77 SSIM: 0.88
===> Epoch [100 / 100]: Batch [80 / 159]: Gradient Penalty: 0.0956, D Fake Loss: -8.7105, D Real Loss: -7.2379, Wasserstein Loss: 8.7171, Content Loss: 0.2684, PSNR: 29.78 SSIM: 0.88
===> Epoch [100 / 100]: Batch [96 / 159]: Gradient Penalty: 0.0953, D Fake Loss: -8.7407, D Real Loss: -7.2701, Wasserstein Loss: 8.7455, Content Loss: 0.2673, PSNR: 29.78 SSIM: 0.88
===> Epoch [100 / 100]: Batch [112 / 159]: Gradient Penalty: 0.0948, D Fake Loss: -8.7840, D Real Loss: -7.3181, Wasserstein Loss: 8.7862, Content Loss: 0.2664, PSNR: 29.81 SSIM: 0.88
===> Epoch [100 / 100]: Batch [128 / 159]: Gradient Penalty: 0.0942, D Fake Loss: -8.7936, D Real Loss: -7.3339, Wasserstein Loss: 8.7999, Content Loss: 0.2664, PSNR: 29.83 SSIM: 0.88
===> Epoch [100 / 100]: Batch [144 / 159]: Gradient Penalty: 0.0935, D Fake Loss: -8.8375, D Real Loss: -7.3848, Wasserstein Loss: 8.8435, Content Loss: 0.2656, PSNR: 29.90 SSIM: 0.88
Epoch [100 / 100]: Gradient Penalty: 0.0942, D Fake Loss: -8.8149, D Real Loss: -7.3560, Wasserstein Loss: 8.8198, Content Loss: 0.2669, PSNR: 29.87 SSIM: 0.88
